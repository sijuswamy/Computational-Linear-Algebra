---
title: "Linear Algebra for Advanced Applications"
execute: 
  enabled: true
jupyter: python3
---

## Singular Value Decomposition (SVD) â€“ An Intuitive and Mathematical Approach

Singular Value Decomposition (SVD) is one of the most powerful matrix factorization tools in linear algebra, extensively used in areas like data compression, signal processing, machine learning, and more. SVD generalizes the concept of diagonalization to non-square matrices, decomposing any $m \times n$ matrix $A$ into three matrices with well-defined geometric interpretations.

## The SVD Theorem

For any real or complex $m \times n$ matrix $A$, SVD states that:

$$
A = U \Sigma V^T
$$

Where:
- $U$ is an $m \times m$ orthogonal matrix (or unitary in the complex case),
- $\Sigma$ is an $m \times n$ diagonal matrix, with non-negative real numbers (the singular values of $A$) on the diagonal,
- $V^T$ is the transpose (or conjugate transpose in the complex case) of an $n \times n$ orthogonal matrix $V$.

These are range and null spaces for both the column and the row spaces.

\begin{align}
%
  \mathbf{C}^{n} = 
    \color{blue}{\mathcal{R} \left( \mathbf{A}^{*} \right)} \oplus
    \color{red}{\mathcal{N} \left( \mathbf{A} \right)} \\
%
  \mathbf{C}^{m} = 
    \color{blue}{\mathcal{R} \left( \mathbf{A} \right)} \oplus
    \color{red} {\mathcal{N} \left( \mathbf{A}^{*} \right)}
%
\end{align}

The singular value decomposition provides an orthonormal basis for the four fundamental subspaces.

:::{.callout-note}

### Singular Value Decomposition- A matrix visualization

\begin{align}
  \mathbf{A} &=
  \mathbf{U} \, \Sigma \, \mathbf{V}^{*} \\
%
 &=
% U 
  \left[ \begin{array}{cc}
     \color{blue}{\mathbf{U}_{\mathcal{R}}} & \color{red}{\mathbf{U}_{\mathcal{N}}}
  \end{array} \right]  
% Sigma
  \left[ \begin{array}{cccc|cc}
     \sigma_{1} & 0 & \dots &  &   & \dots &  0 \\
     0 & \sigma_{2}  \\
     \vdots && \ddots \\
       & & & \sigma_{\rho} \\\hline
       & & & & 0 & \\
     \vdots &&&&&\ddots \\
     0 & & &   &   &  & 0 \\
  \end{array} \right]
% V 
  \left[ \begin{array}{c}
     \color{blue}{\mathbf{V}_{\mathcal{R}}}^{*} \\ 
     \color{red}{\mathbf{V}_{\mathcal{N}}}^{*}
  \end{array} \right]  \\
%
  & =
% U
   \left[ \begin{array}{cccccccc}
    \color{blue}{u_{1}} & \dots & \color{blue}{u_{\rho}} & \color{red}{u_{\rho+1}} & \dots & \color{red}{u_{m}}
  \end{array} \right]
% Sigma
  \left[ \begin{array}{cc}
     \mathbf{S}_{\rho\times \rho} & \mathbf{0} \\
     \mathbf{0} & \mathbf{0} 
  \end{array} \right]
% V
   \left[ \begin{array}{c}
    \color{blue}{v_{1}^{*}} \\ 
    \vdots \\
    \color{blue}{v_{\rho}^{*}} \\
    \color{red}{v_{\rho+1}^{*}} \\
    \vdots \\ 
    \color{red}{v_{n}^{*}}
  \end{array} \right]
%
\end{align}

The column vectors of $U$  are an orthonormal span of $\mathbb{C}^{m}$ (column space), while the column vectors of $V$  are an orthonormal span of $\mathbb{C}^{n}$ (row space).

The $\rho$ singular values are real and ordered (descending):

$$\sigma_{1} \ge \sigma_{2} \ge \dots \ge \sigma_{\rho}>0.$$

These singular values for the diagonal matrix of singular values

$$\mathbf{S} = \text{diagonal} (\sigma_{1},\sigma_{1},\dots,\sigma_{\rho}) \in\mathbb{R}^{\rho\times\rho}.$$

The $S$  matrix is embedded in the sabot matrix $\Sigma\in\mathbb{R}^{m\times n}$ whose shape insures conformability.

Please note that the singular values _only_ correspond to _range space_ vectors.

The column vectors form spans for the subspaces:
\begin{align} 
% R A
\color{blue}{\mathcal{R} \left( \mathbf{A} \right)} &=
\text{span} \left\{
 \color{blue}{u_{1}}, \dots , \color{blue}{u_{\rho}}
\right\} \\
% R A*
\color{blue}{\mathcal{R} \left( \mathbf{A}^{*} \right)} &=
\text{span} \left\{
 \color{blue}{v_{1}}, \dots , \color{blue}{v_{\rho}}
\right\} \\
% N A*
\color{red}{\mathcal{N} \left( \mathbf{A}^{*} \right)} &=
\text{span} \left\{
\color{red}{u_{\rho+1}}, \dots , \color{red}{u_{m}}
\right\} \\
% N A
\color{red}{\mathcal{N} \left( \mathbf{A} \right)} &=
\text{span} \left\{
\color{red}{v_{\rho+1}}, \dots , \color{red}{v_{n}}
\right\} \\
%
\end{align}

:::

## Intuition Behind SVD

The SVD can be understood geometrically:
- The columns of $V$ form an orthonormal basis of the input space.
- The matrix $\Sigma$ scales and transforms this space along the principal axes.
- The columns of $U$ form an orthonormal basis of the output space, representing how the transformed vectors in the input space map to the output space.

SVD essentially performs three steps on any vector $x$:
1. **Rotation**: $V^T$ aligns $x$ with the principal axes.

2. **Scaling**: $\Sigma$ scales along these axes.

3. **Rotation**: $U$ maps the result back to the output space.

## Spectral Decomposition vs. SVD

- **Spectral Decomposition** (also known as **Eigendecomposition**) applies to **square** matrices and decomposes a matrix $A$ into $A = Q \Lambda Q^{-1}$, where $Q$ is an orthogonal matrix of eigenvectors, and $\Lambda$ is a diagonal matrix of eigenvalues.
- **SVD**, on the other hand, applies to **any** matrix (square or rectangular) and generalizes this idea by using singular values (which are always non-negative) instead of eigenvalues.

### Comparison:

- **Eigenvectors and Eigenvalues**: Spectral decomposition only works if $A$ is square and diagonalizable. It gives insight into the properties of a matrix (e.g., whether it is invertible).
- **Singular Vectors and Singular Values**: SVD works for any matrix and provides a more general and stable decomposition, useful even for non-square matrices.

## Steps to Find $U$, $\Sigma$, and $V^T$

Given a matrix $A$, the SVD factors $U$, $\Sigma$, and $V^T$ can be computed as follows:

1. **Compute $A^T A$ and find the eigenvalues and eigenvectors:**

   - The matrix $V$ is formed from the eigenvectors of $A^T A$.

   - The singular values $\sigma_i$ are the square roots of the eigenvalues of $A^T A$.
   
2. **Construct $\Sigma$:**

   - $\Sigma$ is a diagonal matrix where the non-zero entries are the singular values $\sigma_1, \sigma_2, \dots$, arranged in decreasing order.
   
3. **Compute $A A^T$ and find the eigenvectors:**

   - The matrix $U$ is formed from the eigenvectors of $A A^T$.
   
4. **Transpose $V$:**

   - The matrix $V^T$ is simply the transpose of $V$.

## Example

Let's consider a simple example where $A$ is a $2 \times 2$ matrix:

$$
A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}
$$

### Step 1: Compute $A^T A$

$$
A^T A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} = \begin{pmatrix} 10 & 6 \\ 6 & 10 \end{pmatrix}
$$

Find the eigenvalues of $A^T A$:

$$
\det(A^T A - \lambda I) = \det\begin{pmatrix} 10 - \lambda & 6 \\ 6 & 10 - \lambda \end{pmatrix} = 0
$$

$$
(10 - \lambda)^2 - 36 = 0 \quad \Rightarrow \quad \lambda = 16, \lambda = 4
$$

The eigenvalues of $A^T A$ are $16$ and $4$, so the singular values of $A$ are $\sigma_1 = 4$ and $\sigma_2 = 2$.

### Step 2: Find $V$ from the eigenvectors of $A^T A$

Solve $(A^T A - \lambda I)v = 0$ for each eigenvalue:

- For $\lambda = 16$, the eigenvector is $v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$,
- For $\lambda = 4$, the eigenvector is $v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}$.

Thus, 

$$
V = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}
$$

### Step 3: Construct $\Sigma$

The singular values $\sigma_1 = 4$ and $\sigma_2 = 2$, so:

$$
\Sigma = \begin{pmatrix} 4 & 0 \\ 0 & 2 \end{pmatrix}
$$

### Step 4: Find $U$ from the eigenvectors of $A A^T$

Similarly, compute $A A^T$:

$$
A A^T = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} = \begin{pmatrix} 10 & 6 \\ 6 & 10 \end{pmatrix}
$$

Solve for the eigenvectors of $A A^T$ (same as $A^T A$):

The eigenvectors are $u_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}$ and $u_2 = \begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}$.

Thus,

$$
U = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}
$$

### Step 5: Final SVD

We can now write the SVD of $A$ as:

$$
A = U \Sigma V^T
$$

Where:

$$
U = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 4 & 0 \\ 0 & 2 \end{pmatrix}, \quad V^T = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}
$$

`Python` code to find SVD of this example is given below.

```{python}
import numpy as np

# Define the matrix A
A = np.array([[3, 1],
              [1, 3]])

# Perform SVD decomposition
U, Sigma, VT = np.linalg.svd(A)

# Create Sigma matrix from singular values
Sigma_matrix = np.zeros((A.shape[0], A.shape[1]))
np.fill_diagonal(Sigma_matrix, Sigma)

# Print results
print("Matrix A:")
print(A)
print("\nU matrix:")
print(U)
print("\nSigma matrix:")
print(Sigma_matrix)
print("\nV^T matrix:")
print(VT)

# Verify the decomposition A = U * Sigma * V^T
A_reconstructed = U @ Sigma_matrix @ VT
print("\nReconstructed A (U * Sigma * V^T):")
print(A_reconstructed)
```

## Reconstructing Matrix $A$ Using SVD

Given the Singular Value Decomposition (SVD) of a matrix $A$, the matrix can be reconstructed as a linear combination of low-rank matrices using the left singular vectors $u_i$, singular values $\sigma_i$, and the right singular vectors $v_i^T$.

The formula to reconstruct the matrix $A$ is:

$$
A = \sum_{i=1}^{r} \sigma_i \, u_i \, v_i^T
$$

where:
- $r$ is the rank of the matrix $A$ (i.e., the number of non-zero singular values),
- $\sigma_i$ is the $i$-th singular value from the diagonal matrix $\Sigma$,
- $u_i$ is the $i$-th column of the matrix $U$ (left singular vectors),
- $v_i^T$ is the transpose of the $i$-th row of the matrix $V^T$ (right singular vectors).

### Breakdown of Terms:
- $u_i \in \mathbb{R}^m$ is a column vector from the matrix $U$ (size $m \times 1$),
- $v_i^T \in \mathbb{R}^n$ is a row vector from the matrix $V^T$ (size $1 \times n$),
- $\sigma_i \in \mathbb{R}$ is a scalar representing the $i$-th singular value.

Each term $\sigma_i u_i v_i^T$ represents a **rank-1 matrix** (the outer product of two vectors). The sum of these rank-1 matrices reconstructs the original matrix $A$.

### Example:

For a matrix $A$, its SVD is represented as:

$$
A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_i \, u_i \, v_i^T
$$

If the rank of $A$ is 2, then the reconstructed form of $A$ would be:

$$
A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T
$$

Each term $\sigma_i u_i v_i^T$ corresponds to a **low-rank approximation** that contributes to the final matrix. By summing these terms, the full matrix $A$ is obtained.

Python code demonstrating reconstruction is given below:

```{python}
import numpy as np

# Define the matrix A and convert it to float64
A = np.array([[3, 1], 
              [1, 3]], dtype=np.float64)

# Perform SVD
U, Sigma, VT = np.linalg.svd(A)

# Reconstruct A using the singular values and singular vectors
A_reconstructed = np.zeros_like(A)  # This will be float64 now
for i in range(len(Sigma)-1):
    A_reconstructed += Sigma[i] * np.outer(U[:, i], VT[i, :])

print("Original matrix A:")
print(A)

print("\nReconstructed A from rank-1 matrices:")
print(A_reconstructed)

```

## Takeaway

Singular Value Decomposition provides a general framework for decomposing any matrix into orthogonal components, revealing the underlying structure of the matrix. SVD has numerous applications in machine learning, signal processing, and more. The method to find the matrices $U$, $\Sigma$, and $V^T$ involves using the eigenvalues and eigenvectors of $A^T A$ and $A A^T$.

