[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Linear Algebra",
    "section": "",
    "text": "Preface\nWelcome to the course on Computational Linear Algebra. This course is designed to provide a practical perspective on linear algebra, bridging the gap between mathematical theory and real-world applications. As we delve into the intricacies of linear algebra, our focus will be on equipping you with the skills to effectively utilize these concepts in the design, development, and manipulation of data-driven processes applicable to Computer Science and Engineering.\nThroughout this course, you will explore linear algebra not just as a set of abstract mathematical principles, but as a powerful tool for solving complex problems and optimizing processes. The curriculum integrates robust mathematical theory with hands-on implementation, enabling you to apply linear algebra techniques in practical scenarios.\nFrom understanding fundamental operations to applying advanced concepts in data-driven contexts, this course aims to build a strong foundation that supports both theoretical knowledge and practical expertise. Whether you’re tackling computational challenges or developing innovative solutions, the skills and insights gained here will be invaluable in your academic and professional endeavors Knuth (1984).\nWe look forward to guiding you through this journey of blending theory with practice and helping you harness the full potential of linear algebra in your work.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction to Computational Linear Algebra\nWelcome to the Computational Linear Algebra course, a pivotal component of our Computational Mathematics for Engineering minor program. This course is meticulously designed to connect theoretical linear algebra concepts with their practical applications in Artificial Intelligence (AI) and Data Science.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-computational-linear-algebra",
    "href": "intro.html#introduction-to-computational-linear-algebra",
    "title": "Introduction",
    "section": "",
    "text": "Course Themes\n\nPractical Application Proficiency\n\nOur primary focus is on seamlessly translating theoretical concepts into practical solutions for real-world challenges.\nDevelop robust problem-solving skills applicable to AI, Data Science, and advanced engineering scenarios.\n\nMathematical Expertise for Data Insights\n\nGain in-depth proficiency in computational linear algebra, covering essential topics like matrix operations, eigendecomposition, and singular value decomposition.\nLeverage linear algebra techniques to derive meaningful insights and make informed decisions in data science applications.\n\nHands-On Learning\n\nEngage in immersive, project-based learning experiences with a strong emphasis on Python implementation.\nApply linear algebra principles to practical problems, including linear regression, principal component analysis (PCA), and neural networks.\n\n\n\n\nRelevance and Impact\nIn today’s technology-driven landscape, linear algebra forms the backbone of many critical algorithms and applications in AI and Data Science. This course will not only enhance your analytical and computational skills but also prepare you to address complex engineering problems with confidence.\nBy the end of this course, you will have acquired a comprehensive understanding of the role of linear algebra in computational mathematics and its practical applications. This knowledge will equip you with the tools necessary to excel in the rapidly evolving tech industry.\nLet us start this educational journey together, where theoretical knowledge meets practical application, and explore the fascinating and impactful world of Computational Linear Algebra.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "module_1.html",
    "href": "module_1.html",
    "title": "1  Python for Linear Algebra",
    "section": "",
    "text": "1.1 Pseudocode: the new language for algorithm design\nPseudocode is a way to describe algorithms in a structured but plain language. It helps in planning the logic without worrying about the syntax of a specific programming language. In this module, we’ll use Python-flavored pseudocode to describe various matrix operations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#pseudocode-the-new-language-for-algorithm-design",
    "href": "module_1.html#pseudocode-the-new-language-for-algorithm-design",
    "title": "1  Python for Linear Algebra",
    "section": "",
    "text": "Caution\n\n\n\nThere are varities of approaches in writing pseudocode. Students can adopt any of the standard approach to write pseudocode.\n\n\n\n1.1.1 Matrix Sum\nMathematical Procedure:\nTo add two matrices \\(A\\) and \\(B\\), both matrices must have the same dimensions. The sum \\(C\\) of two matrices \\(A\\) and \\(B\\) is calculated element-wise:\n\\[C[i][j] = A[i][j] + B[i][j]\\]\nExample:\nLet \\(A\\) and \\(B\\) be two \\(2 \\times 2\\) matrices:\n\\[ A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\]\nThe sum \\(C\\) is:\n\\[ C = A + B = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix} \\]\nPseudocode:\nFUNCTION matrix_sum(A, B):\n    Get the number of rows and columns in matrix A\n    Create an empty matrix C with the same dimensions\n    FOR each row i:\n        FOR each column j:\n            Set C[i][j] to the sum of A[i][j] and B[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrix \\(A\\).\nCreate a new matrix \\(C\\) with the same dimensions.\nLoop through each element of the matrices and add corresponding elements.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.2 Matrix Difference\nMathematical Procedure:\nTo subtract matrix \\(B\\) from matrix \\(A\\), both matrices must have the same dimensions. The difference \\(C\\) of two matrices \\(A\\) and \\(B\\) is calculated element-wise:\n\\[ C[i][j] = A[i][j] - B[i][j] \\]\nExample:\nLet \\(A\\) and \\(B\\) be two \\(2 \\times 2\\) matrices:\n\\[ A = \\begin{bmatrix} 9 & 8 \\\\ 7 & 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\]\nThe difference \\(C\\) is:\n\\[ C = A - B = \\begin{bmatrix} 9-1 & 8-2 \\\\ 7-3 & 6-4 \\end{bmatrix} = \\begin{bmatrix} 8 & 6 \\\\ 4 & 2 \\end{bmatrix} \\]\nPseudocode:\nFUNCTION matrix_difference(A, B):\n    # Determine the number of rows and columns in matrix A\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Create an empty matrix C with the same dimensions as A and B\n    C = create_matrix(rows, cols)\n    \n    # Iterate through each row\n    FOR i FROM 0 TO rows-1:\n        # Iterate through each column\n        FOR j FROM 0 TO cols-1:\n            # Calculate the difference for each element and store it in C\n            C[i][j] = A[i][j] - B[i][j]\n    \n    # Return the result matrix C\n    RETURN C\nEND FUNCTION\nIn more human readable format the above pseudocode can be written as:\nFUNCTION matrix_difference(A, B):\n    Get the number of rows and columns in matrix A\n    Create an empty matrix C with the same dimensions\n    FOR each row i:\n        FOR each column j:\n            Set C[i][j] to the difference of A[i][j] and B[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrix \\(A\\).\nCreate a new matrix \\(C\\) with the same dimensions.\nLoop through each element of the matrices and subtract corresponding elements.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.3 Matrix Product\nMathematical Procedure:\nTo find the product of two matrices \\(A\\) and \\(B\\), the number of columns in \\(A\\) must be equal to the number of rows in \\(B\\). The element \\(C[i][j]\\) in the product matrix \\(C\\) is computed as:\n\\[C[i][j] = \\sum_{k} A[i][k] \\cdot B[k][j]\\]\nExample:\nLet \\(A\\) be a \\(2 \\times 3\\) matrix and \\(B\\) be a \\(3 \\times 2\\) matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix}\\]\nThe product \\(C\\) is:\n\\[C = A \\cdot B = \\begin{bmatrix} 58 & 64 \\\\ 139 & 154 \\end{bmatrix}\\]\nPseudocode:\nFUNCTION matrix_product(A, B):\n    # Get the dimensions of A and B\n    rows_A = number_of_rows(A)\n    cols_A = number_of_columns(A)\n    rows_B = number_of_rows(B)\n    cols_B = number_of_columns(B)\n    \n    # Check if multiplication is possible\n    IF cols_A != rows_B:\n        RAISE Error(\"Incompatible matrix dimensions\")\n    \n    # Initialize result matrix C\n    C = create_matrix(rows_A, cols_B)\n    \n    # Calculate matrix product\n    FOR each row i FROM 0 TO rows_A-1:\n        FOR each column j FROM 0 TO cols_B-1:\n            # Compute the sum for C[i][j]\n            sum = 0\n            FOR each k FROM 0 TO cols_A-1:\n                sum = sum + A[i][k] * B[k][j]\n            C[i][j] = sum\n    \n    RETURN C\nEND FUNCTION\nA more human readable version of the pseudocode is shown below:\nFUNCTION matrix_product(A, B):\n    Get the number of rows and columns in matrix A\n    Get the number of columns in matrix B\n    Create an empty matrix C with dimensions rows_A x cols_B\n    FOR each row i in A:\n        FOR each column j in B:\n            Initialize C[i][j] to 0\n            FOR each element k in the common dimension:\n                Add the product of A[i][k] and B[k][j] to C[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrices \\(A\\) and \\(B\\).\nCreate a new matrix \\(C\\) with dimensions \\(\\text{rows}(A)\\times \\text{columns}(B)\\).\nLoop through each element of the resulting matrix \\(C[i][j]\\) and calculate the dot product of \\(i\\) the row of \\(A\\) to \\(j\\) th column of \\(B\\) for each element.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.4 Determinant\nMathematical Procedure:\nTo find the determinant of a square matrix \\(A\\), we can use the Laplace expansion, which involves breaking the matrix down into smaller submatrices. For a \\(2 \\times 2\\) matrix, the determinant is calculated as:\n\\[\\text{det}(A) = A[0][0] \\cdot A[1][1] - A[0][1] \\cdot A[1][0]\\]\nFor larger matrices, the determinant is calculated recursively.\nExample:\nLet \\(A\\) be a \\(2 \\times 2\\) matrix:\n\\[A = \\begin{bmatrix} 4 & 3 \\\\ 6 & 3 \\end{bmatrix}\\]\nThe determinant of \\(A\\) is:\n\\[\\text{det}(A) = (4 \\cdot 3) - (3 \\cdot 6) = 12 - 18 = -6\\]\nPseudocode:\nFUNCTION determinant(A):\n    # Step 1: Get the size of the matrix\n    n = number_of_rows(A)\n    \n    # Base case for a 2x2 matrix\n    IF n == 2:\n        RETURN A[0][0] * A[1][1] - A[0][1] * A[1][0]\n    \n    # Step 2: Initialize determinant to 0\n    det = 0\n    \n    # Step 3: Loop through each column of the first row\n    FOR each column j FROM 0 TO n-1:\n        # Get the submatrix excluding the first row and current column\n        submatrix = create_submatrix(A, 0, j)\n        # Recursive call to determinant\n        sub_det = determinant(submatrix)\n        # Alternating sign and adding to the determinant\n        det = det + ((-1) ^ j) * A[0][j] * sub_det\n    \n    RETURN det\nEND FUNCTION\n\nFUNCTION create_sub_matrix(A, row, col):\n    sub_matrix = create_matrix(number_of_rows(A)-1, number_of_columns(A)-1)\n    sub_i = 0\n    FOR i FROM 0 TO number_of_rows(A)-1:\n        IF i == row:\n            CONTINUE\n        sub_j = 0\n        FOR j FROM 0 TO number_of_columns(A)-1:\n            IF j == col:\n                CONTINUE\n            sub_matrix[sub_i][sub_j] = A[i][j]\n            sub_j = sub_j + 1\n        sub_i = sub_i + 1\n    RETURN sub_matrix\nEND FUNCTION\nA human readable version of the same pseudocode is shown below:\nFUNCTION determinant(A):\n    IF the size of A is 2x2:\n        RETURN the difference between the product of the diagonals\n    END IF\n    Initialize det to 0\n    FOR each column c in the first row:\n        Create a sub_matrix by removing the first row and column c\n        Add to det: the product of (-1)^c, the element A[0][c], and the determinant of the sub_matrix\n    RETURN det\nEND FUNCTION\n\nFUNCTION create_sub_matrix(A, row, col):\n    Create an empty sub_matrix with dimensions one less than A\n    Set sub_i to 0\n    FOR each row i in A:\n        IF i is the row to be removed:\n            CONTINUE to the next row\n        Set sub_j to 0\n        FOR each column j in A:\n            IF j is the column to be removed:\n                CONTINUE to the next column\n            Copy the element A[i][j] to sub_matrix[sub_i][sub_j]\n            Increment sub_j\n        Increment sub_i\n    RETURN sub_matrix\nEND FUNCTION\nExplanation:\n\nIf the matrix is \\(2×2\\), calculate the determinant directly.\nFor larger matrices, use the Laplace expansion to recursively calculate the determinant.\nCreate submatrices by removing the current row and column.\nSum the determinants of the submatrices, adjusted for the sign and the current element.\n\n\n\n1.1.5 Rank of a Matrix\nMathematical Procedure:\nThe rank of a matrix \\(A\\) is the maximum number of linearly independent rows or columns in \\(A\\). This can be found using Gaussian elimination to transform the matrix into its row echelon form (REF) and then counting the number of non-zero rows.\nExample:\nLet \\(A\\) be a \\(3 \\times 3\\) matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\nAfter performing Gaussian elimination, we obtain:\n\\[\\text{REF}(A) = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\nThe rank of \\(A\\) is the number of non-zero rows, which is 2.\nPseudocode:\nFUNCTION matrix_rank(A):\n    # Step 1: Get the dimensions of the matrix\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Step 2: Transform the matrix to row echelon form\n    row_echelon_form(A, rows, cols)\n    \n    # Step 3: Count non-zero rows\n    rank = 0\n    FOR each row i FROM 0 TO rows-1:\n        non_zero = FALSE\n        FOR each column j FROM 0 TO cols-1:\n            IF A[i][j] != 0:\n                non_zero = TRUE\n                BREAK\n        IF non_zero:\n            rank = rank + 1\n    \n    RETURN rank\nEND FUNCTION\n\nFUNCTION row_echelon_form(A, rows, cols):\n    # Perform Gaussian elimination\n    lead = 0\n    FOR r FROM 0 TO rows-1:\n        IF lead &gt;= cols:\n            RETURN\n        i = r\n        WHILE A[i][lead] == 0:\n            i = i + 1\n            IF i == rows:\n                i = r\n                lead = lead + 1\n                IF lead == cols:\n                    RETURN\n        # Swap rows i and r\n        swap_rows(A, i, r)\n        # Make A[r][lead] = 1\n        lv = A[r][lead]\n        A[r] = [m / float(lv) for m in A[r]]\n        # Make all rows below r have 0 in column lead\n        FOR i FROM r + 1 TO rows-1:\n            lv = A[i][lead]\n            A[i] = [iv - lv * rv for rv, iv in zip(A[r], A[i])]\n        lead = lead + 1\nEND FUNCTION\n\nFUNCTION swap_rows(A, row1, row2):\n    temp = A[row1]\n    A[row1] = A[row2]\n    A[row2] = temp\nEND FUNCTION\nA more human readable version of the above pseudocode is shown below:\nFUNCTION rank(A):\n    Get the number of rows and columns in matrix A\n    Initialize the rank to 0\n    FOR each row i in A:\n        IF the element in the current row and column is non-zero:\n            Increment the rank\n            FOR each row below the current row:\n                Calculate the multiplier to zero out the element below the diagonal\n                Subtract the appropriate multiple of the current row from each row below\n        ELSE:\n            Initialize a variable to track if a swap is needed\n            FOR each row below the current row:\n                IF a non-zero element is found in the current column:\n                    Swap the current row with the row having the non-zero element\n                    Set the swap variable to True\n                    BREAK the loop\n            IF no swap was made:\n                Decrement the rank\n    RETURN the rank\nEND FUNCTION\nExplanation:\n\nInitialize the rank to 0.\nLoop through each row of the matrix.\nIf the diagonal element is non-zero, increment the rank and perform row operations to zero out the elements below the diagonal.\nIf the diagonal element is zero, try to swap with a lower row that has a non-zero element in the same column.\nIf no such row is found, decrement the rank.\nReturn the resulting rank of the matrix.\n\n\n\n1.1.6 Practice Problems\nFind the rank of the following matrices.\n\n\\(\\begin{pmatrix} 1&1&3\\\\ 2&2&6\\\\ 2&5&3\\end{pmatrix}\\).\n\\(\\begin{pmatrix} 2&0&2\\\\ 1&2&3\\\\ 3&2&7\\end{pmatrix}\\)\n\n\n\n1.1.7 Solving a System of Equations\nMathematical Procedure:\nTo solve a system of linear equations represented as \\(A \\mathbf{x} = \\mathbf{b}\\), where\\(A\\) is the coefficient matrix, \\(\\mathbf{x}\\) is the vector of variables, and\\(\\mathbf{b}\\) is the constant vector, we can use Gaussian elimination to transform the augmented matrix \\([A | \\mathbf{b}]\\) into its row echelon form (REF) and then perform back substitution to find the solution vector \\(\\mathbf{x}\\) Strang (2022).\nExample:\nConsider the system of equations:\n\\[\\begin{cases}\nx + 2y + 3z &= 9 \\\\\n4x + 5y + 6z& = 24 \\\\\n7x + 8y + 9z& = 39\n\\end{cases}\\]\nThe augmented matrix is:\n\\[[A | \\mathbf{b}] = \\begin{bmatrix} 1 & 2 & 3 & | & 9 \\\\ 4 & 5 & 6 & | & 24 \\\\ 7 & 8 & 9 & | & 39 \\end{bmatrix}\\]\nAfter performing Gaussian elimination on the augmented matrix, we get:\n\\[\\text{REF}(A) = \\begin{bmatrix} 1 & 2 & 3 & | & 9 \\\\ 0 & -3 & -6 & | & -12 \\\\ 0 & 0 & 0 & | & 0 \\end{bmatrix}\\]\nPerforming back substitution, we solve for \\(z\\),\\(y\\), and \\(x\\):\n\\[\\begin{cases}\nz = 1 \\\\\ny = 0 \\\\\nx = 3\n\\end{cases}\\]\nTherefore, the solution vector is \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\nPseudocode:\nFUNCTION solve_system_of_equations(A, b):\n    # Step 1: Get the dimensions of the matrix\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Step 2: Create the augmented matrix\n    augmented_matrix = create_augmented_matrix(A, b)\n    \n    # Step 3: Transform the augmented matrix to row echelon form\n    row_echelon_form(augmented_matrix, rows, cols)\n    \n    # Step 4: Perform back substitution\n    solution = back_substitution(augmented_matrix, rows, cols)\n    \n    RETURN solution\nEND FUNCTION\n\nFUNCTION create_augmented_matrix(A, b):\n    # Combine A and b into an augmented matrix\n    augmented_matrix = []\n    FOR i FROM 0 TO number_of_rows(A)-1:\n        augmented_matrix.append(A[i] + [b[i]])\n    RETURN augmented_matrix\nEND FUNCTION\n\nFUNCTION row_echelon_form(augmented_matrix, rows, cols):\n    # Perform Gaussian elimination\n    lead = 0\n    FOR r FROM 0 TO rows-1:\n        IF lead &gt;= cols:\n            RETURN\n        i = r\n        WHILE augmented_matrix[i][lead] == 0:\n            i = i + 1\n            IF i == rows:\n                i = r\n                lead = lead + 1\n                IF lead == cols:\n                    RETURN\n        # Swap rows i and r\n        swap_rows(augmented_matrix, i, r)\n        # Make augmented_matrix[r][lead] = 1\n        lv = augmented_matrix[r][lead]\n        augmented_matrix[r] = [m / float(lv) for m in augmented_matrix[r]]\n        # Make all rows below r have 0 in column lead\n        FOR i FROM r + 1 TO rows-1:\n            lv = augmented_matrix[i][lead]\n            augmented_matrix[i] = [iv - lv * rv for rv, iv in zip(augmented_matrix[r], augmented_matrix[i])]\n        lead = lead + 1\nEND FUNCTION\n\nFUNCTION back_substitution(augmented_matrix, rows, cols):\n    # Initialize the solution vector\n    solution = [0 for _ in range(rows)]\n    # Perform back substitution\n    FOR i FROM rows-1 DOWNTO 0:\n        solution[i] = augmented_matrix[i][cols-1]\n        FOR j FROM i+1 TO cols-2:\n            solution[i] = solution[i] - augmented_matrix[i][j] * solution[j]\n    RETURN solution\nEND FUNCTION\n\nFUNCTION swap_rows(matrix, row1, row2):\n    temp = matrix[row1]\n    matrix[row1] = matrix[row2]\n    matrix[row2] = temp\nEND FUNCTION\nExplanation:\n\nAugment the coefficient matrix \\(A\\) with the constant matrix \\(B\\).\nPerform Gaussian elimination to reduce the augmented matrix to row echelon form.\nBack-substitute to find the solution vector \\(X\\).\nReturn the solution vector \\(X\\).\n\n\n\n1.1.8 Review Problems\nQ1: Fill in the missing parts of the pseudocode to yield a meaningful algebraic operation on of two matrices.\nPseudocode:\nFUNCTION matrix_op1(A, B):\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    result = create_matrix(rows, cols, 0)\n    \n    FOR i FROM 0 TO rows-1:\n        FOR j FROM 0 TO cols-1:\n            result[i][j] = A[i][j] + ---\n    \n    RETURN result\nEND FUNCTION\nQ2: Write the pseudocode to get useful derivable from a given a matrix by fill in the missing part.\nPseudocode:\nFUNCTION matrix_op2(A):\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    result = create_matrix(cols, rows, 0)\n    \n    FOR i FROM 0 TO rows-1:\n        FOR j FROM 0 TO cols-1:\n            result[j][i] = A[i][--]\n    \n    RETURN result\nEND FUNCTION",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#transition-from-pseudocode-to-python-programming",
    "href": "module_1.html#transition-from-pseudocode-to-python-programming",
    "title": "1  Python for Linear Algebra",
    "section": "1.2 Transition from Pseudocode to Python Programming",
    "text": "1.2 Transition from Pseudocode to Python Programming\nIn this course, our initial approach to understanding and solving linear algebra problems has been through pseudocode. Pseudocode allows us to focus on the logical steps and algorithms without getting bogged down by the syntax of a specific programming language. This method helps us build a strong foundation in the computational aspects of linear algebra.\nHowever, to fully leverage the power of computational tools and prepare for real-world applications, it is essential to implement these algorithms in a practical programming language. Python is a highly versatile and widely-used language in the fields of data science, artificial intelligence, and engineering. By transitioning from pseudocode to Python, we align with the following course objectives:\n\nPractical Implementation: Python provides numerous libraries and tools, such as NumPy and SciPy, which are specifically designed for numerical computations and linear algebra. Implementing our algorithms in Python allows us to perform complex calculations efficiently and accurately.\nHands-On Experience: Moving to Python programming gives students hands-on experience in coding, debugging, and optimizing algorithms. This practical experience is crucial for developing the skills required in modern computational tasks.\nIndustry Relevance: Python is extensively used in industry for data analysis, machine learning, and scientific research. Familiarity with Python and its libraries ensures that students are well-prepared for internships, research projects, and future careers in these fields.\nIntegration with Other Tools: Python’s compatibility with various tools and platforms allows for seamless integration into larger projects and workflows. This integration is vital for tackling real-world problems that often require multi-disciplinary approaches.\nEnhanced Learning: Implementing algorithms in Python helps reinforce theoretical concepts by providing immediate feedback through code execution and results visualization. This iterative learning process deepens understanding and retention of the material.\n\nBy transitioning to Python programming, we not only achieve our course objectives but also equip students with valuable skills that are directly applicable to their academic and professional pursuits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#python-fundamentals",
    "href": "module_1.html#python-fundamentals",
    "title": "1  Python for Linear Algebra",
    "section": "1.3 Python Fundamentals",
    "text": "1.3 Python Fundamentals\n\n1.3.1 Python Programming Overview\nPython is a high-level, interpreted programming language that was created by Guido van Rossum and first released in 1991. Its design philosophy emphasizes code readability and simplicity, making it an excellent choice for both beginners and experienced developers. Over the years, Python has undergone significant development and improvement, with major releases adding new features and optimizations. The language’s versatility and ease of use have made it popular in various domains, including web development, data science, artificial intelligence, scientific computing, automation, and more. Python’s extensive standard library and active community contribute to its widespread adoption, making it one of the most popular programming languages in the world today.\n\n\n1.3.2 Variables\nIn Python, variables are used to store data that can be used and manipulated throughout a program. Variables do not need explicit declaration to reserve memory space. The declaration happens automatically when a value is assigned to a variable.\nBasic Input/Output Functions\nPython provides built-in functions for basic input and output operations. The print() function is used to display output, while the input() function is used to take input from the user.\nOutput with print() function\n\nExample 1\n\n# Printing text\nprint(\"Hello, World!\")\n\n# Printing multiple values\nx = 5\ny = 10\nprint(\"The value of x is:\", x, \"and the value of y is:\", y)\n\nExample 2\n\n# Assigning values to variables\na = 10\nb = 20.5\nname = \"Alice\"\n\n# Printing the values\nprint(\"Values Stored in the Variables:\")\nprint(a)\nprint(b)\nprint(name)\nInput with input() Function:\n# Taking input from the user\nname = input(\"Enter usr name: \")\nprint(\"Hello, \" + name + \"!\")\n\n# Taking numerical input\nage = int(input(\"Enter usr age: \"))\nprint(\"us are\", age, \"years old.\")\n\n\n\n\n\n\nNote\n\n\n\nThe print() function in Python, defined in the built-in __builtin__ module, is used to display output on the screen, providing a simple way to output text and variable values to the console.\n\n\nCombining Variables and Input/Output\nus can combine variables and input/output functions to create interactive programs.\n\nExample:\n\n# Program to calculate the sum of two numbers\nnum1 = float(input(\"Enter first number: \"))\nnum2 = float(input(\"Enter second number: \"))\n\n# Calculate sum\nsum = num1 + num2\n\n# Display the result\nprint(\"The sum of\", num1, \"and\", num2, \"is\", sum)\n\n\n1.3.3 Python Programming Style\n\n1.3.3.1 Indentation\nPython uses indentation to define the blocks of code. Proper indentation is crucial as it affects the program’s flow. Use 4 spaces per indentation level.\nif a &gt; b:\n    print(\"a is greater than b\")\nelse:\n    print(\"b is greater than or equal to a\")\n\n\n1.3.3.2 Comments\nUse comments to explain user code. Comments begin with the # symbol and extend to the end of the line. Write comments that are clear and concise. See the example:\n# This is a comment\na = 10  # This is an inline comment\n\n\n1.3.3.3 Variable Naming\nUse meaningful variable names to make usr code more understandable. Variable names should be in lowercase with words separated by underscores.\nstudent_name = \"John\"\ntotal_score = 95\n\n\n1.3.3.4 Consistent Style\nFollow the PEP 8 style guide for Python code to maintain consistency and readability. Use blank lines to separate different sections of usr code. See the following example of function definition:\n\ndef calculate_sum(x, y):\n    return x + y\n\nresult = calculate_sum(5, 3)\nprint(result)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#basic-datatypes-in-python",
    "href": "module_1.html#basic-datatypes-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.4 Basic Datatypes in Python",
    "text": "1.4 Basic Datatypes in Python\nIn Python, a datatype is a classification that specifies which type of value a variable can hold. Understanding datatypes is essential as it helps in performing appropriate operations on variables. Python supports various built-in datatypes, which can be categorized into several groups.\n\n1.4.1 Numeric Types\nNumeric types represent data that consists of numbers. Python has three distinct numeric types:\n\nIntegers (int):\n\nWhole numbers, positive or negative, without decimals.\nExample: a = 10, b = -5.\n\nFloating Point Numbers (float):\n\nNumbers that contain a decimal point.\nExample: pi = 3.14, temperature = -7.5.\n\nComplex Numbers (complex):\n\nNumbers with a real and an imaginary part.\nExample: z = 3 + 4j.\n\n\n# Examples of numeric types\na = 10         # Integer\npi = 3.14      # Float\nz = 3 + 4j     # Complex\n\n\n1.4.2 Sequence Types\nSequence types are used to store multiple items in a single variable. Python has several sequence types, including:\n\n1.4.2.1 String Type\nStrings in Python are sequences of characters enclosed in quotes. They are used to handle and manipulate textual data.\nCharacteristics of Strings\n\nOrdered: Characters in a string have a defined order.\nImmutable: Strings cannot be modified after they are created.\nHeterogeneous: Strings can include any combination of letters, numbers, and symbols.\n\nCreating Strings\nStrings can be created using single quotes, double quotes, or triple quotes for multiline strings.\n\nExample:\n\n# Creating strings with different types of quotes\nsingle_quoted = 'Hello, World!'\ndouble_quoted = \"Hello, World!\"\nmultiline_string = \"\"\"This is a\nmultiline string\"\"\"\nAccessing String Characters\nCharacters in a string are accessed using their index, with the first character having an index of 0. Negative indexing can be used to access characters from the end.\n\nExample:\n\n# Accessing characters in a string\nfirst_char = single_quoted[0]  # Output: 'H'\nlast_char = single_quoted[-1]  # Output: '!'\nCommon String Methods\nPython provides various methods for string manipulation:\n\nupper(): Converts all characters to uppercase.\nlower(): Converts all characters to lowercase.\nstrip(): Removes leading and trailing whitespace.\nreplace(old, new): Replaces occurrences of a substring with another substring.\nsplit(separator): Splits the string into a list based on a separator.\n\n\nExample:\n\n# Using string methods\ntext = \"   hello, world!   \"\nuppercase_text = text.upper()       # Result: \"   HELLO, WORLD!   \"\nstripped_text = text.strip()        # Result: \"hello, world!\"\nreplaced_text = text.replace(\"world\", \"Python\")  # Result: \"   hello, Python!   \"\nwords = text.split(\",\")             # Result: ['hello', ' world!   ']\n\n\n1.4.2.2 List Type\nLists are one of the most versatile and commonly used sequence types in Python. They allow for the storage and manipulation of ordered collections of items.\n\nCharacteristics of Lists\n\n\nOrdered: The items in a list have a defined order, which will not change unless explicitly modified.\nMutable: The content of a list can be changed after its creation (i.e., items can be added, removed, or modified).\nDynamic: Lists can grow or shrink in size as items are added or removed.\nHeterogeneous: Items in a list can be of different data types (e.g., integers, strings, floats).\n\nCreating Lists\nLists are created by placing comma-separated values inside square brackets.\n\nExample:\n\n# Creating a list of fruits\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Creating a mixed list\nmixed_list = [1, \"Hello\", 3.14]\nAccessing List Items\nList items are accessed using their index, with the first item having an index of 0.\n\nExample:\n\n\n# Accessing the first item\nfirst_fruit = fruits[0]  # Output: \"apple\"\n\n# Accessing the last item\nlast_fruit = fruits[-1]  # Output: \"cherry\"\nModifying Lists\nLists can be modified by changing the value of specific items, adding new items, or removing existing items.\n\nExample:\n\n# Changing the value of an item\nfruits[1] = \"blueberry\"  # fruits is now [\"apple\", \"blueberry\", \"cherry\"]\n\n# Adding a new item\nfruits.append(\"orange\")  # fruits is now [\"apple\", \"blueberry\", \"cherry\", \"orange\"]\n\n# Removing an item\nfruits.remove(\"blueberry\")  # fruits is now [\"apple\", \"cherry\", \"orange\"]\nList Methods\nPython provides several built-in methods to work with lists:\n\nappend(item): Adds an item to the end of the list.\ninsert(index, item): Inserts an item at a specified index.\nremove(item): Removes the first occurrence of an item.\npop(index): Removes and returns the item at the specified index.\nsort(): Sorts the list in ascending order.\nreverse(): Reverses the order of the list.\n\n\nExample:\n\n# Using list methods\nnumbers = [5, 2, 9, 1]\n\nnumbers.append(4)     # numbers is now [5, 2, 9, 1, 4]\nnumbers.sort()        # numbers is now [1, 2, 4, 5, 9]\nnumbers.reverse()     # numbers is now [9, 5, 4, 2, 1]\nfirst_number = numbers.pop(0)  # first_number is 9, numbers is now [5, 4, 2, 1]\n\n\n1.4.2.3 Tuple Type\nTuples are a built-in sequence type in Python that is used to store an ordered collection of items. Unlike lists, tuples are immutable, which means their contents cannot be changed after creation.\nCharacteristics of Tuples\n\nOrdered: Tuples maintain the order of items, which is consistent throughout their lifetime.\nImmutable: Once a tuple is created, its contents cannot be modified. This includes adding, removing, or changing items.\nFixed Size: The size of a tuple is fixed; it cannot grow or shrink after creation.\nHeterogeneous: Tuples can contain items of different data types, such as integers, strings, and floats.\n\nCreating Tuples\nTuples are created by placing comma-separated values inside parentheses. Single-element tuples require a trailing comma.\n\nExample:\n\n# Creating a tuple with multiple items\ncoordinates = (10, 20, 30)\n\n# Creating a single-element tuple\nsingle_element_tuple = (5,)\n\n# Creating a tuple with mixed data types\nmixed_tuple = (1, \"Hello\", 3.14)\nAccessing Tuple Items\nTuple items are accessed using their index, with the first item having an index of 0. Negative indexing can be used to access items from the end.\n\nExample:\n\n# Accessing the first item\nx = coordinates[0]  # Output: 10\n\n# Accessing the last item\nz = coordinates[-1]  # Output: 30\nModifying Tuples\nSince tuples are immutable, their contents cannot be modified. However, us can create new tuples by combining or slicing existing ones.\n\nExample:\n\n# Combining tuples\nnew_coordinates = coordinates + (40, 50)  # Result: (10, 20, 30, 40, 50)\n\n# Slicing tuples\nsub_tuple = coordinates[1:3]  # Result: (20, 30)\nTuple Methods\nTuples have a limited set of built-in methods compared to lists:\n\ncount(item): Returns the number of occurrences of the specified item.\nindex(item): Returns the index of the first occurrence of the specified item.\n\n\nExample:\n\n# Using tuple methods\nnumbers = (1, 2, 3, 1, 2, 1)\n\n# Counting occurrences of an item\ncount_1 = numbers.count(1)  # Result: 3\n\n# Finding the index of an item\nindex_2 = numbers.index(2)  # Result: 1\n\n\n\n1.4.3 Mapping Types\nMapping types in Python are used to store data in key-value pairs. Unlike sequences, mappings do not maintain an order and are designed for quick lookups of data.\n\n1.4.3.1 Dictionary (dict)\nThe primary mapping type in Python is the dict. Dictionaries store data as key-value pairs, where each key must be unique, and keys are used to access their corresponding values.\nCharacteristics of Dictionaries\n\nUnordered: The order of items is not guaranteed and may vary.\nMutable: us can add, remove, and change items after creation.\nKeys: Must be unique and immutable (e.g., strings, numbers, tuples).\nValues: Can be of any data type and can be duplicated.\n\nCreating Dictionaries\nDictionaries are created using curly braces {} with key-value pairs separated by colons :.\n\nExample:\n\n# Creating a dictionary\nstudent = {\n    \"name\": \"Alice\",\n    \"age\": 21,\n    \"major\": \"Computer Science\"\n}\nAccessing and Modifying Dictionary Items\nItems in a dictionary are accessed using their keys. us can also modify, add, or remove items.\n\nExample:\n\n# Accessing a value\nname = student[\"name\"]  # Output: \"Alice\"\n\n# Modifying a value\nstudent[\"age\"] = 22  # Updates the age to 22\n\n# Adding a new key-value pair\nstudent[\"graduation_year\"] = 2024\n\n# Removing a key-value pair\ndel student[\"major\"]\nDictionary Methods\nPython provides several built-in methods to work with dictionaries:\n\nkeys(): Returns a view object of all keys.\nvalues(): Returns a view object of all values.\nitems(): Returns a view object of all key-value pairs.\nget(key, default): Returns the value for the specified key, or a default value if the key is not found.\npop(key, default): Removes and returns the value for the specified key, or a default value if the key is not found.\n\n\nExample:\n\n# Using dictionary methods\nkeys = student.keys()        # Result: dict_keys(['name', 'age', 'graduation_year'])\nvalues = student.values()    # Result: dict_values(['Alice', 22, 2024])\nitems = student.items()      # Result: dict_items([('name', 'Alice'), ('age', 22), ('graduation_year', 2024)])\nname = student.get(\"name\")  # Result: \"Alice\"\nage = student.pop(\"age\")    # Result: 22\n\n\n\n1.4.4 Set Types\nSets are a built-in data type in Python used to store unique, unordered collections of items. They are particularly useful for operations involving membership tests, set operations, and removing duplicates.\nCharacteristics of Sets\n\nUnordered : The items in a set do not have a specific order and may change.\nMutable : us can add or remove items from a set after its creation.\nUnique : Sets do not allow duplicate items; all items must be unique.\nUnindexed : Sets do not support indexing or slicing.\n\nCreating Sets\nSets are created using curly braces {} with comma-separated values, or using the set() function.\n\nExample:\n\n# Creating a set using curly braces\nfruits = {\"apple\", \"banana\", \"cherry\"}\n\n# Creating a set using the set() function\nnumbers = set([1, 2, 3, 4, 5])\nAccessing and Modifying Set Items\nWhile us cannot access individual items by index, us can check for membership and perform operations like adding or removing items.\n\nExample:\n\n# Checking membership\nhas_apple = \"apple\" in fruits  # Output: True\n\n# Adding an item\nfruits.add(\"orange\")\n\n# Removing an item\nfruits.remove(\"banana\")  # Raises KeyError if item is not present\nSet Operations Sets support various mathematical set operations, such as union, intersection, and difference.\n\nExample:\n\n# Union of two sets\nset1 = {1, 2, 3}\nset2 = {3, 4, 5}\nunion = set1 | set2  # Result: {1, 2, 3, 4, 5}\n\n# Intersection of two sets\nintersection = set1 & set2  # Result: {3}\n\n# Difference between two sets\ndifference = set1 - set2  # Result: {1, 2}\n\n# Symmetric difference (items in either set, but not in both)\nsymmetric_difference = set1 ^ set2  # Result: {1, 2, 4, 5}\nSet Methods\nPython provides several built-in methods for set operations:\n\nadd(item): Adds an item to the set.\nremove(item): Removes an item from the set; raises KeyError if item is not present.\ndiscard(item): Removes an item from the set if present; does not raise an error if item is not found.\npop(): Removes and returns an arbitrary item from the set.\nclear(): Removes all items from the set.\n\n\nExample:\n\n# Using set methods\nset1 = {1, 2, 3}\n\nset1.add(4)        # set1 is now {1, 2, 3, 4}\nset1.remove(2)     # set1 is now {1, 3, 4}\nset1.discard(5)    # No error, set1 remains {1, 3, 4}\nitem = set1.pop()  # Removes and returns an arbitrary item, e.g., 1\nset1.clear()      # set1 is now an empty set {}\n\n1.4.4.1 ## Frozen Sets\nFrozen sets are a built-in data type in Python that are similar to sets but are immutable. Once created, a frozen set cannot be modified, making it suitable for use as a key in dictionaries or as elements of other sets.\nCharacteristics of Frozen Sets\n\nUnordered : The items in a frozen set do not have a specific order and may change.\nImmutable : Unlike regular sets, frozen sets cannot be altered after creation. No items can be added or removed.\nUnique : Like sets, frozen sets do not allow duplicate items; all items must be unique.\nUnindexed : Frozen sets do not support indexing or slicing.\n\nCreating Frozen Sets\nFrozen sets are created using the frozenset() function, which takes an iterable as an argument.\n\nExample:\n\n# Creating a frozen set\nnumbers = frozenset([1, 2, 3, 4, 5])\n\n# Creating a frozen set from a set\nfruits = frozenset({\"apple\", \"banana\", \"cherry\"})\nAccessing and Modifying Frozen Set Items\nFrozen sets do not support modification operations such as adding or removing items. However, us can perform membership tests and other set operations.\n\nExample:\n\n# Checking membership\nhas_apple = \"apple\" in fruits  # Output: True\n\n# Since frozenset is immutable, us cannot use add() or remove() methods\nSet Operations with Frozen Sets\nFrozen sets support various mathematical set operations similar to regular sets, such as union, intersection, and difference. These operations return new frozen sets and do not modify the original ones.\n\nExample:\n\n# Union of two frozen sets\nset1 = frozenset([1, 2, 3])\nset2 = frozenset([3, 4, 5])\nunion = set1 | set2  # Result: frozenset({1, 2, 3, 4, 5})\n\n# Intersection of two frozen sets\nintersection = set1 & set2  # Result: frozenset({3})\n\n# Difference between two frozen sets\ndifference = set1 - set2  # Result: frozenset({1, 2})\n\n# Symmetric difference (items in either set, but not in both)\nsymmetric_difference = set1 ^ set2  # Result: frozenset({1, 2, 4, 5})\nFrozen Set Methods\nFrozen sets have a subset of the methods available to regular sets. The available methods include:\n\ncopy() : Returns a shallow copy of the frozen set.\ndifference(other) : Returns a new frozen set with elements in the original frozen set but not in other.\nintersection(other) : Returns a new frozen set with elements common to both frozen sets.\nunion(other) : Returns a new frozen set with elements from both frozen sets.\nsymmetric_difference(other) : Returns a new frozen set with elements in either frozen set but not in both.\n\n\nExample:\n\n# Using frozen set methods\nset1 = frozenset([1, 2, 3])\nset2 = frozenset([3, 4, 5])\n\n# Getting the difference\ndifference = set1.difference(set2)  # Result: frozenset({1, 2})\n\n# Getting the intersection\nintersection = set1.intersection(set2)  # Result: frozenset({3})\n\n# Getting the union\nunion = set1.union(set2)  # Result: frozenset({1, 2, 3, 4, 5})\n\n# Getting the symmetric difference\nsymmetric_difference = set1.symmetric_difference(set2)  # Result: frozenset({1, 2, 4, 5})",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#control-structures-in-python",
    "href": "module_1.html#control-structures-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.5 Control Structures in Python",
    "text": "1.5 Control Structures in Python\nControl structures in Python allow us to control the flow of execution in our programs. They help manage decision-making, looping, and the execution of code blocks based on certain conditions. Python provides several key control structures: if statements, for loops, while loops, and control flow statements like break, continue, and pass.\n\n1.5.1 Conditional Statements\nConditional statements are used to execute code based on certain conditions. The primary conditional statement in Python is the if statement, which can be combined with elif and else to handle multiple conditions.\n\nSyntax:\n\nif condition:\n    # Code block to execute if condition is True\nelif another_condition:\n    # Code block to execute if another_condition is True\nelse:\n    # Code block to execute if none of the above conditions are True\n\nExample: Program to classify a person based on his/her age.\n\nage = 20\n\nif age &lt; 18:\n    print(\"us are a minor.\")\nelif age &lt; 65:\n    print(\"us are an adult.\")\nelse:\n    print(\"us are a senior citizen.\")\n\n\n1.5.2 Looping Statements\nLooping statements are used to repeat a block of code multiple times. Python supports for loops and while loops.\n\n1.5.2.1 For Loop\nThe for loop iterates over a sequence (like a list, tuple, or string) and executes a block of code for each item in the sequence.\n\nSyntax:\n\nfor item in sequence:\n    # Code block to execute for each item\n\nExample: Program to print names of fruits saved in a list.\n\n# Iterating over a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\n\n1.5.2.2 While Loop\nThe while loop repeatedly executes a block of code as long as a specified condition is True.\n\nSyntax:\n\nwhile condition:\n    # Code block to execute while condition is True\n\nExample: Print all counting numbers less than 5.\n\n# Counting from 0 to 4\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n\n\n\n1.5.3 Control Flow Statements\nControl flow statements alter the flow of execution within loops and conditionals.\n\n1.5.3.1 Break Statement\nThe break statement exits the current loop, regardless of the loop’s condition.\n\nExample: Program to exit from the printing of whole numbers less than 10, while trigger 5.\n\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n# Output: 0 1 2 3 4\n\n\n1.5.3.2 Continue Statement\nThe continue statement skips the rest of the code inside the current loop iteration and proceeds to the next iteration.\n\nExample: Program to print all the whole numbers in the range 5 except 2.\n\nfor i in range(5):\n    if i == 2:\n        continue\n    print(i) \n# Output: 0 1 3 4\n\n\n1.5.3.3 Pass Statement\nThe pass statement is a placeholder that does nothing and is used when a statement is syntactically required but no action is needed.\n\nExample: Program to print all the whole numbers in the range 5 except 3.\n\nfor i in range(5):\n    if i == 3:\n        pass  # Placeholder for future code\n    else:\n        print(i)\n# Output: 0 1 2 4\n\n\n\n\n\n\nCautions When Using Control Flow Structures\n\n\n\n\n\nControl flow structures are essential in Python programming for directing the flow of execution. However, improper use of these structures can lead to errors, inefficiencies, and unintended behaviors. Here are some cautions to keep in mind:\nInfinite Loops\n\nIssue: A while loop with a condition that never becomes False can lead to an infinite loop, which will cause the program to hang or become unresponsive.\nCaution: Always ensure that the condition in a while loop will eventually become False, and include logic within the loop to modify the condition.\n\nExample:\n# Infinite loop example\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    # Missing count increment, causing an infinite loop",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#functions-in-python-programming",
    "href": "module_1.html#functions-in-python-programming",
    "title": "1  Python for Linear Algebra",
    "section": "1.6 Functions in Python Programming",
    "text": "1.6 Functions in Python Programming\nFunctions are a fundamental concept in Python programming that enable code reuse, modularity, and organization. They allow us to encapsulate a block of code that performs a specific task, which can be executed whenever needed. Functions are essential for writing clean, maintainable, and scalable code, making them a cornerstone of effective programming practices.\nWhat is a Function?\nA function is a named block of code designed to perform a specific task. Functions can take inputs, called parameters or arguments, and can return outputs, which are the results of the computation or task performed by the function. By defining functions, we can write code once and reuse it multiple times, which enhances both efficiency and readability.\nDefining a Function\nIn Python, functions are defined using the def keyword, followed by the function name, parentheses containing any parameters, and a colon. The function body, which contains the code to be executed, is indented below the function definition.\n\nSyntax:\n\ndef function_name(parameters):\n    # Code block\n    return result\n\nExample:\n\ndef greet(name):\n    \"\"\"\n    Returns a greeting message for the given name.\n    \"\"\"\n    return f\"Hello, {name}!\"\n\n1.6.0.1 Relevance of functions in Programming\n\nCode Reusability : Functions allow us to define a piece of code once and reuse it in multiple places. This reduces redundancy and helps maintain consistency across our codebase.\nModularity : Functions break down complex problems into smaller, manageable pieces. Each function can be focused on a specific task, making it easier to understand and maintain the code.\nAbstraction : Functions enable us to abstract away the implementation details. We can use a function without needing to know its internal workings, which simplifies the code we write and enhances readability.\nTesting and Debugging : Functions allow us to test individual components of our code separately. This isolation helps in identifying and fixing bugs more efficiently.\nLibrary Creation : Functions are the building blocks of libraries and modules. By organizing related functions into libraries, we can create reusable components that can be shared and utilized across different projects.\n\n\nExample: Creating a Simple Library\n\nStage 1: Define Functions in a Module\n# my_library.py\n\ndef add(a, b):\n    \"\"\"\n    Returns the sum of two numbers.\n    \"\"\"\n    return a + b\n\ndef multiply(a, b):\n    \"\"\"\n    Returns the product of two numbers.\n    \"\"\"\n    return a * b\nStage 2: Use the Library in Another Program\n# main.py\n\nimport my_library\n\nresult_sum = my_library.add(5, 3)\nresult_product = my_library.multiply(5, 3)\n\nprint(f\"Sum: {result_sum}\")\nprint(f\"Product: {result_product}\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#object-oriented-programming-oop-in-python",
    "href": "module_1.html#object-oriented-programming-oop-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.7 Object-Oriented Programming (OOP) in Python",
    "text": "1.7 Object-Oriented Programming (OOP) in Python\nObject-Oriented Programming (OOP) is a programming paradigm that uses “objects” to design and implement software. It emphasizes the organization of code into classes and objects, allowing for the encapsulation of data and functionality. OOP promotes code reusability, scalability, and maintainability through key principles such as encapsulation, inheritance, and polymorphism.\n\n1.7.1 Key Concepts of OOP\n\nClasses and Objects\n\n\nClass: A class is a blueprint for creating objects. It defines a set of attributes and methods that the created objects will have. A class can be thought of as a template or prototype for objects.\nObject: An object is an instance of a class. It is a specific realization of the class with actual values for its attributes.\n\n\n1.7.1.1 Example\n# Defining a class\nclass Dog:\n    def __init__(self, name, age):\n        self.name = name  # Attribute\n        self.age = age    # Attribute\n    \n    def bark(self):\n        return \"Woof!\"   # Method\n\n# Creating an object of the class\nmy_dog = Dog(name=\"Buddy\", age=3)\n\n# Accessing attributes and methods\nprint(my_dog.name)  # Output: Buddy\nprint(my_dog.age)   # Output: 3\nprint(my_dog.bark())  # Output: Woof!\n\nEncapsulation\n\nEncapsulation is the concept of bundling data (attributes) and methods (functions) that operate on the data into a single unit, or class. It restricts direct access to some of the object’s components and can help protect the internal state of the object from unintended modifications.\n\nExample: Controll the access to member variables using encapsulation.\n\nclass Account:\n    def __init__(self, balance):\n        self.__balance = balance  # Private attribute\n    \n    def deposit(self, amount):\n        if amount &gt; 0:\n            self.__balance += amount\n    \n    def get_balance(self):\n        return self.__balance\n\n# Creating an object of the class\nmy_account = Account(balance=1000)\nmy_account.deposit(500)\n\nprint(my_account.get_balance())  # Output: 1500\n# print(my_account.__balance)  # This will raise an AttributeError\n\nInheritance\n\nInheritance is a mechanism in which a new class (child or derived class) inherits attributes and methods from an existing class (parent or base class). It allows for code reuse and the creation of a hierarchy of classes.\n\nExample: Demonstrating usage of attributes of base class in the derived classes.\n\n# Base class\nclass Animal:\n    def __init__(self, name):\n        self.name = name\n    \n    def speak(self):\n        return \"Some sound\"\n\n# Derived class\nclass Dog(Animal):\n    def __init__(self, name, breed):\n        super().__init__(name)  # Calling the constructor of the base class\n        self.breed = breed\n    \n    def speak(self):\n        return \"Woof!\"\n\n# Another derived class\nclass Cat(Animal):\n    def __init__(self, name, color):\n        super().__init__(name)  # Calling the constructor of the base class\n        self.color = color\n    \n    def speak(self):\n        return \"Meow!\"\n\n# Creating objects of the derived classes\ndog = Dog(name=\"Buddy\", breed=\"Golden Retriever\")\ncat = Cat(name=\"Whiskers\", color=\"Gray\")\n\nprint(f\"{dog.name} is a {dog.breed} and says {dog.speak()}\")  # Output: Buddy is a Golden Retriever and says Woof!\nprint(f\"{cat.name} is a {cat.color} cat and says {cat.speak()}\")  # Output: Whiskers is a Gray cat and says Meow!\n\nPolymorphism\n\nPolymorphism allows objects of different classes to be treated as objects of a common superclass. It enables a single interface to be used for different data types. In Python, polymorphism is often achieved through method overriding, where a method in a derived class has the same name as a method in the base class but implements different functionality.\n\nExample:\n\nclass Bird:\n    def fly(self):\n        return \"Flies in the sky\"\n\nclass Penguin(Bird):\n    def fly(self):\n        return \"Cannot fly, swims instead\"\n\n# Creating objects of different classes\nbird = Bird()\npenguin = Penguin()\n\nprint(bird.fly())      # Output: Flies in the sky\nprint(penguin.fly())  # Output: Cannot fly, swims instead",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#working-with-files-in-python",
    "href": "module_1.html#working-with-files-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.8 Working with Files in Python",
    "text": "1.8 Working with Files in Python\nFile handling is an essential part of programming that allows us to work with data stored in files. Python provides built-in functions and methods to create, read, write, and manage files efficiently. This section will cover basic file operations, including opening, reading, writing, and closing files.\nOpening a File\nIn Python, we use the open() function to open a file. This function returns a file object, which provides methods and attributes to interact with the file. The open() function requires at least one argument: the path to the file. we can also specify the mode in which the file should be opened.\n\nSyntax:\n\nfile_object = open(file_path, mode)\nWhere,\n\nfile_path : Path to the file (can be a relative or absolute path).\nmode : Specifies the file access mode (e.g., ‘r’ for reading, ‘w’ for writing, ‘a’ for appending).\n\n\nExample:\n\n# Opening a file in read mode\nfile = open('example.txt', 'r')\nReading from a File\nOnce a file is opened, we can read its contents using various methods. Common methods include read(), readline(), and readlines().\n\nread() : Reads the entire file content.\nreadline() : Reads a single line from the file.\nreadlines() : Reads all the lines into a list.\n\n\nExample:\n\n# Reading the entire file\nfile_content = file.read()\nprint(file_content)\n\n# Reading a single line\nfile.seek(0)  # Move cursor to the start of the file\nline = file.readline()\nprint(line)\n\n# Reading all lines\nfile.seek(0)\nlines = file.readlines()\nprint(lines)\nWriting to a File\nTo write data to a file, we need to open the file in write (‘w’) or append (‘a’) mode. When opened in write mode, the file is truncated (i.e., existing content is deleted). When opened in append mode, new data is added to the end of the file.\n\nExample:\n\n# Opening a file in write mode\nfile = open('example.txt', 'w')\n\n# Writing data to the file\nfile.write(\"Hello, World!\\n\")\nfile.write(\"Python file handling example.\")\n\n# Closing the file\nfile.close()\nClosing a File\nIt is important to close a file after performing operations to ensure that all changes are saved and resources are released. We can close a file using the close() method of the file object.\n\nExample:\n\nf_1 = open('example.txt', 'w') # open the file example.txt to f_1\nf_1.close() # close the file with handler 'f_1'\nUsing Context Managers\nContext managers provide a convenient way to handle file operations, automatically managing file opening and closing. We can use the with statement to ensure that a file is properly closed after its block of code is executed.\n\nExample:\n\n# Using context manager to open and write to a file\nwith open('example.txt', 'w') as file:\n    file.write(\"This is written using a context manager.\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#from-theory-to-practice",
    "href": "module_1.html#from-theory-to-practice",
    "title": "1  Python for Linear Algebra",
    "section": "1.9 From Theory to Practice",
    "text": "1.9 From Theory to Practice\nIn this section, we transition from theoretical concepts to practical applications by exploring how fundamental matrix operations can be used in the field of image processing. By leveraging the knowledge gained from understanding matrix addition, subtraction, multiplication, and other operations, we can tackle real-world problems such as image blending, sharpening, filtering, and transformations. This hands-on approach not only reinforces the theoretical principles but also demonstrates their utility in processing and enhancing digital images. Through practical examples and coding exercises, you’ll see how these mathematical operations are essential tools in modern image manipulation and analysis.\n\n1.9.1 Applications of Matrix Operations in Digital Image Processing\nMatrix operations play a pivotal role in digital image processing, enabling a wide range of techniques for manipulating and enhancing images. By leveraging fundamental matrix operations such as addition, subtraction, multiplication, and transformations, we can perform essential tasks like image blending, filtering, edge detection, and geometric transformations. These operations provide the mathematical foundation for various algorithms used in image analysis, compression, and reconstruction. Understanding and applying matrix operations is crucial for developing efficient and effective image processing solutions, making it an indispensable skill in fields like computer vision, graphics, and multimedia applications.\n\n1.9.1.1 Matrix Addition in Image Blending\nMatrix addition is a fundamental operation in image processing, particularly useful in the technique of image blending. Image blending involves combining two images to produce a single image that integrates the features of both original images. This technique is commonly used in applications such as image overlay, transition effects in videos, and creating composite images.\nConcept\nWhen working with grayscale images, each image can be represented as a matrix where each element corresponds to the intensity of a pixel. By adding corresponding elements (pixels) of two matrices (images), we can blend the images together. The resultant matrix represents the blended image, where each pixel is the sum of the corresponding pixels in the original images.\n\nExample:\n\nConsider two 2x2 grayscale images represented as matrices:\nimage1= [[100, 150],[200, 250]]\nimage2=[[50, 100],[100, 150]]\nTo blend these images, we add the corresponding pixel values as:\nblended_image[i][j] = image1[i][j] + image2[i][j]\nEnsure that the resulting pixel values do not exceed the maximum value allowed (255 for 8-bit images).\nPython Implementation of image blending\nBelow is the Python code for blending two images using matrix addition:\n\ndef matrix_addition(image1, image2):\n    rows = len(image1)\n    cols = len(image1[0])\n    blended_image = [[0] * cols for _ in range(rows)]\n\n    for i in range(rows):\n        for j in range(cols):\n            blended_pixel = image1[i][j] + image2[i][j]\n            blended_image[i][j] = min(blended_pixel, 255)  # Clip to 255\n\n    return blended_image\n\n# Example matrices (images)\nimage1 = [[100, 150], [200, 250]]\nimage2 = [[50, 100], [100, 150]]\n\nblended_image = matrix_addition(image1, image2)\nprint(\"Blended Image:\")\nfor row in blended_image:\n    print(row)\n\nBlended Image:\n[150, 250]\n[255, 255]\n\n\nImage blending is a powerful technique with numerous real-time applications. It is widely used in creating smooth transitions in video editing, overlaying images in augmented reality, and producing composite images in photography and graphic design. By understanding and applying matrix operations, we can develop efficient algorithms that seamlessly integrate multiple images, enhancing the overall visual experience. The practical implementation of matrix addition in image blending underscores the importance of mathematical foundations in achieving sophisticated image processing tasks in real-world applications.\n\n\n\n\n\n\nImage Blending as Basic Arithmetic with Libraries\n\n\n\nIn upcoming chapters, we will explore how specific libraries for image handling simplify the process of image blending to a basic arithmetic operation—adding two objects. Using these libraries, such as PIL (Python Imaging Library) or OpenCV, allows us to leverage efficient built-in functions that streamline tasks like resizing, matrix operations, and pixel manipulation.\n\n\nLet’s summarize a few more matrix operations and its uses in digital image processing tasks in the following sections.\n\n\n1.9.1.2 Matrix Subtraction in Image Sharpening\nMatrix subtraction is a fundamental operation in image processing, essential for techniques like image sharpening. Image sharpening enhances the clarity and detail of an image by increasing the contrast along edges and boundaries.\nConcept\nIn grayscale images, each pixel value represents the intensity of light at that point. Image sharpening involves subtracting a smoothed version of the image from the original. This process accentuates edges and fine details, making them more prominent.\n\nExample:\n\nConsider a grayscale image represented as a matrix:\noriginal_image [[100, 150, 200],[150, 200, 250],[200, 250, 100]]\nTo sharpen the image, we subtract a blurred version (smoothed image) from the original. This enhances edges and fine details:\nsharpened_image[i][j] = original_image[i][j] - blurred_image[i][j]\nPython Implementation\nBelow is a simplified Python example of image sharpening using matrix subtraction:\n# Original image matrix (grayscale values)\noriginal_image = [\n    [100, 150, 200],\n    [150, 200, 250],\n    [200, 250, 100]\n]\n\n# Function to apply Gaussian blur (for demonstration, simplified as average smoothing)\ndef apply_blur(image):\n    blurred_image = []\n    for i in range(len(image)):\n        row = []\n        for j in range(len(image[0])):\n            neighbors = []\n            for dx in [-1, 0, 1]:\n                for dy in [-1, 0, 1]:\n                    ni, nj = i + dx, j + dy\n                    if 0 &lt;= ni &lt; len(image) and 0 &lt;= nj &lt; len(image[0]):\n                        neighbors.append(image[ni][nj])\n            blurred_value = sum(neighbors) // len(neighbors)\n            row.append(blurred_value)\n        blurred_image.append(row)\n    return blurred_image\n\n# Function for matrix subtraction (image sharpening)\ndef image_sharpening(original_image, blurred_image):\n    sharpened_image = []\n    for i in range(len(original_image)):\n        row = []\n        for j in range(len(original_image[0])):\n            sharpened_value = original_image[i][j] - blurred_image[i][j]\n            row.append(sharpened_value)\n        sharpened_image.append(row)\n    return sharpened_image\n\n# Apply blur to simulate smoothed image\nblurred_image = apply_blur(original_image)\n\n# Perform matrix subtraction for image sharpening\nsharpened_image = image_sharpening(original_image, blurred_image)\n\n# Print the sharpened image\nprint(\"Sharpened Image:\")\nfor row in sharpened_image:\n    print(row)\n\n\n1.9.1.3 Matrix Multiplication in Image Filtering (Convolution)\nMatrix multiplication, specifically convolution in the context of image processing, is a fundamental operation used for various tasks such as smoothing, sharpening, edge detection, and more. Convolution involves applying a small matrix, known as a kernel or filter, to an image matrix. This process modifies the pixel values of the image based on the values in the kernel, effectively filtering the image.\nConcept In grayscale images, each pixel value represents the intensity of light at that point. Convolution applies a kernel matrix over the image matrix to compute a weighted sum of neighborhood pixels. This weighted sum determines the new value of each pixel in the resulting filtered image.\n\nExample:\n\nConsider a grayscale image represented as a matrix:\noriginal_image= [[100, 150, 200, 250],\n [150, 200, 250, 300],\n [200, 250, 300, 350],\n [250, 300, 350, 400]]\nTo perform smoothing (averaging) using a simple kernel:\n[[1/9, 1/9, 1/9],\n [1/9, 1/9, 1/9],\n [1/9, 1/9, 1/9]]\nThe kernel is applied over the image using convolution:\nsmoothed_image[i][j] = sum(original_image[ii][jj] * kernel[k][l] for all (ii, jj) in neighborhood around (i, j))\nPython Implementation\nHere’s a simplified Python example demonstrating convolution for image smoothing without external libraries:\n# Original image matrix (grayscale values)\noriginal_image = [\n    [100, 150, 200, 250],\n    [150, 200, 250, 300],\n    [200, 250, 300, 350],\n    [250, 300, 350, 400]\n]\n\n# Define a simple kernel/filter for smoothing (averaging)\nkernel = [\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n]\n\n# Function for applying convolution (image filtering)\ndef apply_convolution(image, kernel):\n    height = len(image)\n    width = len(image[0])\n    ksize = len(kernel)\n    kcenter = ksize // 2  # Center of the kernel\n\n    # Initialize result image\n    filtered_image = [[0]*width for _ in range(height)]\n\n    # Perform convolution\n    for i in range(height):\n        for j in range(width):\n            sum = 0.0\n            for k in range(ksize):\n                for l in range(ksize):\n                    ii = i + k - kcenter\n                    jj = j + l - kcenter\n                    if ii &gt;= 0 and ii &lt; height and jj &gt;= 0 and jj &lt; width:\n                        sum += image[ii][jj] * kernel[k][l]\n            filtered_image[i][j] = int(sum)\n    \n    return filtered_image\n\n# Apply convolution to simulate smoothed image (averaging filter)\nsmoothed_image = apply_convolution(original_image, kernel)\n\n# Print the smoothed image\nprint(\"Smoothed Image:\")\nfor row in smoothed_image:\n    print(row)\n\n\n1.9.1.4 Determinant: Image Transformation\nConcept The determinant of a transformation matrix helps understand how transformations like scaling affect an image. A transformation matrix determines how an image is scaled, rotated, or sheared.\n\nExample:\n\nHere, we compute the determinant of a scaling matrix to understand how the scaling affects the image area.\ndef calculate_determinant(matrix):\n    a, b = matrix[0]\n    c, d = matrix[1]\n    return a * d - b * c\n\n# Example transformation matrix (scaling)\ntransformation_matrix = [[2, 0], [0, 2]]\ndeterminant = calculate_determinant(transformation_matrix)\nprint(f\"Determinant of the transformation matrix: {determinant}\")\nThis value indicates how the transformation scales the image area.\n\n\n1.9.1.5 Rank: Image Rank and Data Compression\nConcept The rank of a matrix indicates the number of linearly independent rows or columns. In image compression, matrix rank helps approximate an image with fewer data.\n\nExample:\n\nHere, we compute the rank of a matrix representing an image. A lower rank might indicate that the image can be approximated with fewer data.\ndef matrix_rank(matrix):\n    def is_zero_row(row):\n        return all(value == 0 for value in row)\n\n    def row_echelon_form(matrix):\n        A = [row[:] for row in matrix]\n        m = len(A)\n        n = len(A[0])\n        rank = 0\n        for i in range(min(m, n)):\n            if A[i][i] != 0:\n                for j in range(i + 1, m):\n                    factor = A[j][i] / A[i][i]\n                    for k in range(i, n):\n                        A[j][k] -= factor * A[i][k]\n                rank += 1\n        return rank\n\n    return row_echelon_form(matrix)\n\n# Example matrix (image)\nimage_matrix = [[1, 2], [3, 4]]\nrank = matrix_rank(image_matrix)\nprint(f\"Rank of the image matrix: {rank}\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#matrix-operations-using-python-libraries",
    "href": "module_1.html#matrix-operations-using-python-libraries",
    "title": "1  Python for Linear Algebra",
    "section": "1.10 Matrix Operations Using Python Libraries",
    "text": "1.10 Matrix Operations Using Python Libraries\n\n1.10.1 Introduction\nIn this section, we will explore the computational aspects of basic matrix algebra using Python. We will utilize the SymPy library for symbolic mathematics, which allows us to perform matrix operations and convert results into LaTeX format. Additionally, the Pillow (PIL) library will be used for image manipulation to demonstrate practical applications of these matrix operations in digital image processing. By the end of this section, you’ll understand how to implement matrix operations and apply them to real-world problems such as image blending, sharpening, filtering, and solving systems of equations.\n\n\n1.10.2 Introduction to SymPy\nSymPy is a powerful Python library designed for symbolic mathematics. It provides tools for algebraic operations, equation solving, and matrix handling in a symbolic form. This makes it ideal for educational purposes and theoretical work where exact results are needed.\n\n1.10.2.1 Key Matrix Functions in SymPy\n\nMatrix Addition: Adds two matrices element-wise.\nMatrix Subtraction: Subtracts one matrix from another element-wise.\nMatrix Multiplication: Multiplies two matrices using the dot product.\nMatrix Power: Raises a matrix to a given power using matrix multiplication.\n\n\nExample 1: Matrix Addition\n\nPseudocode\nFUNCTION matrix_add():\n    # Define matrices A and B\n    A = [[1, 2], [3, 4]]\n    B = [[5, 6], [7, 8]]\n\n    # Check if matrices A and B have the same dimensions\n    if dimensions_of(A) != dimensions_of(B):\n         raise ValueError(\"Matrices must have the same dimensions\")\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(len(A[0]))] for _ in range(len(A))]\n\n    # Add corresponding elements from A and B\n    for i in range(len(A)):\n        for j in range(len(A[0])):\n            result[i][j] = A[i][j] + B[i][j]\n\n# Return the result matrix\n    return result\nENDFUNCTION\nPython implementation of the above pseudocode is given below:\n\nimport sympy as sy\nsy.init_printing()\n# Define matrices A and B\nA = sy.Matrix([[1, 2], [3, 4]])\nB = sy.Matrix([[5, 6], [7, 8]])\n\n# Add matrices\nC = A + B\n\n# Print the result in symbolic form\nprint(\"Matrix Addition Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\n#latex_code = sy.latex(C)\n#print(\"LaTeX Code for Addition Result:\")\n#print(latex_code)\n\nMatrix Addition Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}6 & 8\\\\10 & 12\\end{matrix}\\right]\\)\n\n\n\nExample 2: Matrix Subtraction\n\nPseudocode:\n# Define matrices A and B\nA = [[5, 6], [7, 8]]\nB = [[1, 2], [3, 4]]\n\n# Check if matrices A and B have the same dimensions\nif dimensions_of(A) != dimensions_of(B):\n    raise ValueError(\"Matrices must have the same dimensions\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(A[0]))] for _ in range(len(A))]\n\n# Subtract corresponding elements from A and B\nfor i in range(len(A)):\n    for j in range(len(A[0])):\n        result[i][j] = A[i][j] - B[i][j]\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is given below:\n\nfrom sympy import Matrix\n# Define matrices A and B\nA = Matrix([[5, 6], [7, 8]])\nB = Matrix([[1, 2], [3, 4]])\n\n# Subtract matrices\nC = A - B\n\n# Print the result in symbolic form\nprint(\"Matrix Subtraction Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(C)\nprint(\"LaTeX Code for Subtraction Result:\")\nprint(latex_code)\n\nMatrix Subtraction Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}4 & 4\\\\4 & 4\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Subtraction Result:\n\\left[\\begin{matrix}4 & 4\\\\4 & 4\\end{matrix}\\right]\n\n\n\nExample 3: Matrix Multiplication\n\nPseudocode:\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Check if the number of columns in A equals the number of rows in B\nif len(A[0]) != len(B):\n    raise ValueError(\"Number of columns in A must equal number of rows in B\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(B[0]))] for _ in range(len(A))]\n\n# Multiply matrices A and B\nfor i in range(len(A)):\n    for j in range(len(B[0])):\n        for k in range(len(B)):\n            result[i][j] += A[i][k] * B[k][j]\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is given below:\n\n# Define matrices A and B\nA = Matrix([[1, 2], [3, 4]])\nB = Matrix([[5, 6], [7, 8]])\n\n# Multiply matrices\nM = A * B\n\n# Print the result in symbolic form\nprint(\"Matrix Multiplication Result:\")\ndisplay(M)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(M)\nprint(\"LaTeX Code for Multiplication Result:\")\nprint(latex_code)\n\nMatrix Multiplication Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Multiplication Result:\n\\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\n\n\n\nExample 3: Matrix Multiplication\n\nPseudocode:\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Check if the number of columns in A equals the number of rows in B\nif len(A[0]) != len(B):\n    raise ValueError(\"Number of columns in A must equal number of rows in B\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(B[0]))] for _ in range(len(A))]\n\n# Multiply matrices A and B\nfor i in range(len(A)):\n    for j in range(len(B[0])):\n        for k in range(len(B)):\n            result[i][j] += A[i][k] * B[k][j]\n\n# Return the result matrix\nreturn result\nPython code for implementing the above pseudocode is shown below:\n\n# Define matrices A and B\nA = Matrix([[1, 2], [3, 4]])\nB = Matrix([[5, 6], [7, 8]])\n\n# Multiply matrices\nM = A * B\n\n# Print the result in symbolic form\nprint(\"Matrix Multiplication Result:\")\ndisplay(M)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(M)\nprint(\"LaTeX Code for Multiplication Result:\")\nprint(latex_code)\n\nMatrix Multiplication Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Multiplication Result:\n\\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\n\n\n\nExample 4: Matrix Power\n\nPseudocode:\n# Define matrix A and power n\nA = [[1, 2], [3, 4]]\nn = 2\n\n# Initialize result matrix as identity matrix\nresult = identity_matrix_of(len(A))\n\n# Compute A raised to the power of n\nfor _ in range(n):\n    result = matrix_multiply(result, A)\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is shown below:\n\n# Define matrix A\nA = Matrix([[1, 2], [3, 4]])\n\n# Compute matrix A raised to the power of 2\nn = 2\nC = A**n\n\n# Print the result in symbolic form\nprint(\"Matrix Power Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(C)\nprint(\"LaTeX Code for Power Result:\")\nprint(latex_code)\n\nMatrix Power Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}7 & 10\\\\15 & 22\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Power Result:\n\\left[\\begin{matrix}7 & 10\\\\15 & 22\\end{matrix}\\right]\n\n\n\n\n1.10.2.2 Introduction to PIL for Image Manipulation\nThe PIL (Python Imaging Library), now known as Pillow, provides essential tools for opening, manipulating, and saving various image file formats. In this session, we will use Pillow to perform image operations such as resizing and blending to demonstrate the practical applications of these matrix operations in digital image processing.\nMatrix operations have significant applications in digital image processing. These operations can manipulate images in various ways, from blending to filtering. Below we will discuss how matrix addition, subtraction, and multiplication are used in real-time image processing tasks.\n\nMatrix Addition: Image Blending\n\nMatrix addition can be used to blend two images by adding their pixel values. This process can be straightforward or involve weighted blending.\nExample 1: Simple Image Blending\n\nimport numpy as np\nfrom PIL import Image\nimport urllib.request\nurllib.request.urlretrieve('http://lenna.org/len_top.jpg',\"input.jpg\")\nimg1 = Image.open(\"input.jpg\") #loading first image\n\nurllib.request.urlretrieve('https://www.keralatourism.org/images/destination/large/thekkekudi_cave_temple_in_pathanamthitta20131205062431_315_1.jpg',\"input2.jpg\")\n\nimg2 = Image.open(\"input2.jpg\")# loading second image\n\n# Resize second image to match the size of the first image\nimg2 = img2.resize(img1.size)\n# Convert images to numpy arrays\narr1 = np.array(img1)\narr2 = np.array(img2)\n\n# Add the images\nblended_arr = arr1 + arr2\n\n# Clip the values to be in the valid range [0, 255]\nblended_arr = np.clip(blended_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nblended_img = Image.fromarray(blended_arr)\n\n# Save or display the blended image\n#blended_img.save('blended_image.jpg')\n#blended_img.show()\n#blended_img #display the blended image\n\nThe input and output images are shown below:\n\nimg1\n\n\n\n\n\n\n\n\n\nimg2\n\n\n\n\n\n\n\n\n\nblended_img\n\n\n\n\n\n\n\n\n\nExample 2: Weighted Image Blending\n\n\n# Blend with weights\nalpha = 0.7\nblended_arr = alpha * arr1 + (1 - alpha) * arr2\n\n# Clip the values to be in the valid range [0, 255]\nblended_arr = np.clip(blended_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nblended_img = Image.fromarray(blended_arr)\n\n# Save or display the weighted blended image\n#blended_img.save('weighted_blended_image.jpg')\n#blended_img.show()\nblended_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.3 Matrix Subtraction: Image Sharpening\nMatrix subtraction can be used to sharpen images by subtracting a blurred version of the image from the original.\n\nExample 1: Sharpening by Subtracting Blurred Image\n\n\nfrom PIL import Image, ImageFilter\n# Convert image to grayscale for simplicity\nimg_gray = img1.convert('L')\narr = np.array(img_gray)\n\n# Apply Gaussian blur\nblurred_img = img_gray.filter(ImageFilter.GaussianBlur(radius=5))\nblurred_arr = np.array(blurred_img)\n\n# Sharpen the image by subtracting blurred image\nsharpened_arr = arr - blurred_arr\n\n# Clip the values to be in the valid range [0, 255]\nsharpened_arr = np.clip(sharpened_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nsharpened_img = Image.fromarray(sharpened_arr)\n\n# Save or display the sharpened image\n#sharpened_img.save('sharpened_image.jpg')\n#sharpened_img.show()\nsharpened_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.4 Matrix Multiplication: Image Filtering (Convolution)\nMatrix multiplication is used in image filtering to apply convolution kernels for various effects.\n\nExample 1: Applying a Convolution Filter\n\n\n# Define a simple convolution kernel (e.g., edge detection)\nkernel = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n# Convert the image to grayscale for simplicity\nimg_gray = img1.convert('L')\narr = np.array(img_gray)\n\n# Apply convolution\nfiltered_arr = np.zeros_like(arr)\nfor i in range(1, arr.shape[0] - 1):\n    for j in range(1, arr.shape[1] - 1):\n        region = arr[i-1:i+2, j-1:j+2]\n        filtered_arr[i, j] = np.sum(region * kernel)\n\n# Convert back to image\nfiltered_img = Image.fromarray(np.clip(filtered_arr, 0, 255).astype(np.uint8))\n\n# Save or display the filtered image\n#filtered_img.save('filtered_image.jpg')\n#filtered_img.show()\nfiltered_img\n\n\n\n\n\n\n\n\n\nExample 2: Applying a Gaussian Blur Filter\n\n\n# Define a Gaussian blur filter\nblurred_img = img1.filter(ImageFilter.GaussianBlur(radius=5))\n\n# Save or display the blurred image\n#blurred_img.save('blurred_image.jpg')\n#blurred_img.show()\nblurred_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.5 Solving Systems of Equations and Applications\nIntroduction\nSolving systems of linear equations is crucial in various image processing tasks, such as image transformation, camera calibration, and object detection. In this section, we will demonstrate how to solve systems of linear equations using Python and explore practical applications in image processing.\n\nExample 1: Solving a System of Equations\n\nConsider the system:\n\\[\\begin{cases}\n2x + 3y& = 13 \\\\\n4x - y &= 7\n\\end{cases}\\]\n\nPython Implementation\n\n\nfrom sympy import Matrix\n# Define the coefficient matrix and constant matrix\nA = Matrix([[2, 3], [4, -1]])\nB = Matrix([13, 7])\n# Solve the system of equations\nsolution = A.solve_least_squares(B)\n# Print the solution\nprint(\"Solution to the System of Equations:\")\nprint(solution)\n\nSolution to the System of Equations:\nMatrix([[17/7], [19/7]])",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#conclusion",
    "href": "module_1.html#conclusion",
    "title": "1  Python for Linear Algebra",
    "section": "1.11 Conclusion",
    "text": "1.11 Conclusion\nIn this chapter, we transitioned from understanding fundamental matrix operations to applying them in practical scenarios, specifically in the realm of image processing. We began by covering essential matrix operations such as addition, subtraction, multiplication, and determinant calculations, providing both pseudocode and detailed explanations. This foundational knowledge was then translated into Python code, demonstrating how to perform these operations computationally.\nWe further explored the application of these matrix operations to real-world image processing tasks. By applying techniques such as image blending, sharpening, filtering, and transformation, we illustrated how theoretical concepts can be used to manipulate and enhance digital images effectively. These practical examples highlighted the significance of matrix operations in solving complex image processing challenges.\nBy integrating theoretical understanding with practical implementation, this chapter reinforced how matrix operations form the backbone of many image processing techniques. This blend of theory and practice equips you with essential skills for tackling advanced problems and developing innovative solutions in the field of image processing and beyond.\n\n\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_2.html",
    "href": "module_2.html",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "",
    "text": "2.1 Introduction\nIn the first module, we established a solid foundation in matrix algebra by exploring pseudocode and implementing fundamental matrix operations using Python. We practiced key concepts such as matrix addition, subtraction, multiplication, and determinants through practical examples in image processing, leveraging the SymPy library for symbolic computation.\nAs we begin the second module, “Transforming Linear Algebra to Computational Language,” our focus will shift towards applying these concepts with greater depth and actionable insight. This module is designed to bridge the theoretical knowledge from matrix algebra with practical computational applications. You will learn to interpret and utilize matrix operations, solve systems of equations, and analyze the rank of matrices within a variety of real-world contexts.\nA new concept we will introduce is the Rank-Nullity Theorem, which provides a fundamental relationship between the rank of a matrix and the dimensions of its null space. This theorem is crucial for understanding the solution spaces of linear systems and the properties of linear transformations. By applying this theorem, you will be able to gain deeper insights into the structure of solutions and the behavior of matrix transformations.\nThis transition will not only reinforce your understanding of linear algebra but also enhance your ability to apply these concepts effectively in computational settings. Through engaging examples and practical exercises, you will gain valuable experience in transforming abstract mathematical principles into tangible solutions, setting a strong groundwork for advanced computational techniques.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_2.html#relearning-of-terms-and-operations-in-linear-algebra",
    "href": "module_2.html#relearning-of-terms-and-operations-in-linear-algebra",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "2.2 Relearning of Terms and Operations in Linear Algebra",
    "text": "2.2 Relearning of Terms and Operations in Linear Algebra\nIn this section, we will revisit fundamental matrix operations such as addition, subtraction, scaling, and more through practical examples. Our goal is to transform theoretical linear algebra into modern computational applications. We will demonstrate these concepts using Python, focusing on practical and industrial applications.\n\n2.2.1 Matrix Addition and Subtraction in Data Analysis\nMatrix addition and subtraction are fundamental operations that help in combining datasets and analyzing differences.\nSimple Example: Combining Quarterly Sales Data\nWe begin with quarterly sales data from different regions and combine them to get the total sales. The sales data is given in Table 2.1. A ar plot of the total sales is shown in Fig 2.1.\n\n\n\nTable 2.1: Quarterly Sales Data\n\n\n\n\n\nRegion\nQ1\nQ2\nQ3\nQ4\n\n\n\n\nA\n2500\n2800\n3100\n2900\n\n\nB\n1500\n1600\n1700\n1800\n\n\n\n\n\n\nFrom Scratch Python Implementation:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Quarterly sales data\nsales_region_a = np.array([2500, 2800, 3100, 2900])\nsales_region_b = np.array([1500, 1600, 1700, 1800])\n\n# Combine sales data\ntotal_sales = sales_region_a + sales_region_b\n\n# Visualization\nquarters = ['Q1', 'Q2', 'Q3', 'Q4']\nplt.bar(quarters, total_sales, color='skyblue')\nplt.xlabel('Quarter')\nplt.ylabel('Total Sales')\nplt.title('Combined Quarterly Sales Data for Regions A and B')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.1: Computing Total Sales using Numpy aggregation method\n\n\n\n\n\nIn the above Python code, we have performed the aggregation operation with the NumPy method. Same can be done in a more data analysis style using pandas inorder to handle tabular data meaningfully. In this approach, quarterly sales data of each region is stored as DataFrames(like an excel sheet). The we combine these two DataFrames into one. After that create a new row with index ‘Total’ and populate this row with sum of quarterly sales in Region A and Region B. Finally a bar plot is created using this ‘Total’ sales. Advantage of this approach is that we don’t need the matplotlib library to create visualizations!. The EDA using this approach is shown in Fig 2.2.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# DataFrames for quarterly sales data\ndf_a = pd.DataFrame({'Q1': [2500], 'Q2': [2800], 'Q3': [3100], 'Q4': [2900]}, index=['Region A'])\ndf_b = pd.DataFrame({'Q1': [1500], 'Q2': [1600], 'Q3': [1700], 'Q4': [1800]}, index=['Region B'])\n\n# Combine data\ndf_combined = df_a.add(df_b, fill_value=0)\ndf_combined.loc[\"Total\"] = df_combined.sum(axis=0)\n# Visualization\ndf_combined.loc[\"Total\"].plot(kind='bar', color=['green'])\nplt.xlabel('Quarter')\nplt.ylabel('Total Sales')\nplt.title('Combined Quarterly Sales Data for Regions A and B')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.2: Computation of Total Sales using Pandas method\n\n\n\n\n\nWe can extend this in to more advanced examples. Irrespective to the size of the data, for representation and aggregation tasks matrix models are best options and are used in industry as a standard. Let us consider an advanced example to analyse difference in stock prices. For this example we are using a simulated data. The python code for this simulation process is shown in Fig 2.3.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated observed and predicted stock prices\nobserved_prices = np.random.uniform(100, 200, size=(100, 5))\npredicted_prices = np.random.uniform(95, 210, size=(100, 5))\n\n# Calculate the difference matrix\nprice_differences = observed_prices - predicted_prices\n\n# Visualization\nplt.imshow(price_differences, cmap='coolwarm', aspect='auto')\nplt.colorbar()\nplt.title('Stock Price Differences')\nplt.xlabel('Stock Index')\nplt.ylabel('Day Index')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.3: Demonstration of Stock Price simulated from a Uniform Distribution\n\n\n\n\n\nAnother important matrix operation relevant to data analytics and Machine Learning application is scaling. This is considered as a statistical tool to make various features (attributes) in to same scale so as to avoid unnecessary misleading impact in data analysis and its intepretation. In Machine Learning context, this pre-processing stage is inevitable so as to make the model relevant and usable.\nSimple Example: Normalizing Employee Performance Data\n\n\n\nTable 2.2: Employee Performance Data\n\n\n\n\n\nEmployee\nMetric A\nMetric B\n\n\n\n\nX\n80\n700\n\n\nY\n90\n800\n\n\nZ\n100\n900\n\n\nA\n110\n1000\n\n\nB\n120\n1100\n\n\n\n\n\n\nUsing simple python code we can simulate the model for min-max scaling. The formula for min-max scaling is: \\[min_max(X)=\\dfrac{X-min(X)}{max(X)-min(X)}\\]\nFor example, while applying the min-max scaling in the first value of Metric A, the scaled value is \\[min_max(80)\\dfrac{80-80}{120-80}=0\\]\nSimilarly\n\\[min_max(100)\\dfrac{100-80}{120-80}=0.5\\]\nWhen we apply this formula to Metric A and Metric B, the scaled output from Table 2.2 will be as follows:\n\n\n\nTable 2.3: Employee Performance Data\n\n\n\n\n\nEmployee\nMetric A\nMetric B\n\n\n\n\nX\n0.00\n0.00\n\n\nY\n0.25\n0.25\n\n\nZ\n0.50\n0.50\n\n\nA\n0.75\n0.75\n\n\nB\n1.00\n1.00\n\n\n\n\n\n\nIt is interesting to look into the scaled data! In the orginal table (Table 2.2) it is looked like Metric B is superior. But from the scaled table (Table 2.3), it is clear that both the Metrics are representing same relative information. This will help us to identify the redundency in measure and so skip any one of the Metric before analysis!.\nThe same can be achieved through a matrix operation. The Python implementation of this scaling process is shown in Fig 2.4.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Employee performance data with varying scales\ndata = np.array([[80, 700], [90, 800], [100, 900], [110, 1000], [120, 1100]])\n\n# Manual scaling\nmin_vals = np.min(data, axis=0)\nmax_vals = np.max(data, axis=0)\nscaled_data = (data - min_vals) / (max_vals - min_vals)\n\n# Visualization\nplt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(data, cmap='viridis')\nplt.title('Original Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(scaled_data, cmap='viridis')\nplt.title('Scaled Data')\nplt.colorbar()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2.4: Total sales using pandas method\n\n\n\n\n\nFrom the first sub plot, it is clear that there is a significant difference in the distributions (Metric A and Metric B values). But the second sub plot shows that both the distributions have same pattern and the values ranges between 0 and 1. In short the visualization is more appealing and self explanatory in this case.\n\n\n\n\n\n\nNote\n\n\n\nThe min-max scaling method will confine the feature values (attributes) into the range \\([0,1]\\). So in effect all the features are scaled proportionally to the data spectrum.\n\n\nSimilarly, we can use the standard scaling (transformation to normal distribution) using the transformation \\(\\dfrac{x-\\bar{x}}{\\sigma}\\). Scaling table is given as a practice task to the reader. The python code for this operation is shown in Fig 2.5.\n\n# Standard scaling from scratch\ndef standard_scaling(data):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    scaled_data = (data - mean) / std\n    return scaled_data\n\n# Apply standard scaling\nscaled_data_scratch = standard_scaling(data)\n\nprint(\"Standard Scaled Data (from scratch):\\n\", scaled_data_scratch)\n\n# Visualization\nplt.figure(figsize=(6, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(data, cmap='viridis')\nplt.title('Original Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(scaled_data_scratch, cmap='viridis')\nplt.title('Scaled Data')\nplt.colorbar()\n\nplt.show()\n\nStandard Scaled Data (from scratch):\n [[-1.41421356 -1.41421356]\n [-0.70710678 -0.70710678]\n [ 0.          0.        ]\n [ 0.70710678  0.70710678]\n [ 1.41421356  1.41421356]]\n\n\n\n\n\n\n\n\nFigure 2.5: Min-max scaling using basic python\n\n\n\n\n\nTo understand the effect of standard scaling, let us consider Fig 2.6. This plot create the frequency distribution of the data as a histogram along with the density function. From the first sub-plot, it is clear that the distribution has multiple modes (peaks). When we apply the standard scaling, the distribution become un-modal(only one peek). This is demonstrated in the second sub-plot.\n\n# Standard scaling from scratch\nimport seaborn as sns\n# Create plots\nplt.figure(figsize=(6, 5))\n\n# Plot for original data\nplt.subplot(1, 2, 1)\nsns.histplot(data, kde=True, bins=10, palette=\"viridis\")\nplt.title('Original Data Distribution')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Plot for standard scaled data\nplt.subplot(1, 2, 2)\nsns.histplot(scaled_data_scratch, kde=True, bins=10, palette=\"viridis\")\nplt.title('Standard Scaled Data Distribution')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.6: Impact of standard scaling on the distribution\n\n\n\n\n\nA scatter plot showing the compare the impact of scaling on the given distribution is shown in Fig 2.7.\n\n# Plot original and scaled data\nplt.figure(figsize=(6, 5))\n\n# Original Data\nplt.subplot(1, 3, 1)\nplt.scatter(data[:, 0], data[:, 1], color='blue')\nplt.title('Original Data')\nplt.xlabel('Metric A')\nplt.ylabel('Metric B')\n\n# Standard Scaled Data\nplt.subplot(1, 3, 2)\nplt.scatter(scaled_data_scratch[:, 0], scaled_data_scratch[:, 1], color='green')\nplt.title('Standard Scaled Data')\nplt.xlabel('Metric A (Standard Scaled)')\nplt.ylabel('Metric B (Standard Scaled)')\n\n# Min-Max Scaled Data\nplt.subplot(1, 3, 3)\nplt.scatter(scaled_data[:, 0], scaled_data[:, 1], color='red')\nplt.title('Min-Max Scaled Data')\nplt.xlabel('Metric A (Min-Max Scaled)')\nplt.ylabel('Metric B (Min-Max Scaled)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.7: Comparison of impact of scaling on the distribution\n\n\n\n\n\nFrom the Fig 2.7, it is clear that the scaling does not affect the pattern of the data, instead it just scale the distribution proportionally!\nWe can use the scikit-learn library for do the same thing in a very simple handy approach. The python code for this job is shown below.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Min-max scaling using sklearn\nscaler = MinMaxScaler()\nmin_max_scaled_data_sklearn = scaler.fit_transform(data)\n\nprint(\"Min-Max Scaled Data (using sklearn):\\n\", min_max_scaled_data_sklearn)\n\nMin-Max Scaled Data (using sklearn):\n [[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [0.75 0.75]\n [1.   1.  ]]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standard scaling using sklearn\nscaler = StandardScaler()\nscaled_data_sklearn = scaler.fit_transform(data)\n\nprint(\"Standard Scaled Data (using sklearn):\\n\", scaled_data_sklearn)\n\nStandard Scaled Data (using sklearn):\n [[-1.41421356 -1.41421356]\n [-0.70710678 -0.70710678]\n [ 0.          0.        ]\n [ 0.70710678  0.70710678]\n [ 1.41421356  1.41421356]]\n\n\nA scatter plot showing the impact on scaling is shown in Fig 2.8. This plot compare the mmin-max and standard-scaling.\n\n# Plot original and scaled data\nplt.figure(figsize=(6, 5))\n\n# Original Data\nplt.subplot(1, 3, 1)\nplt.scatter(data[:, 0], data[:, 1], color='blue')\nplt.title('Original Data')\nplt.xlabel('Metric A')\nplt.ylabel('Metric B')\n\n# Standard Scaled Data\nplt.subplot(1, 3, 2)\nplt.scatter(scaled_data_sklearn[:, 0], scaled_data_sklearn[:, 1], color='green')\nplt.title('Standard Scaled Data')\nplt.xlabel('Metric A (Standard Scaled)')\nplt.ylabel('Metric B (Standard Scaled)')\n\n# Min-Max Scaled Data\nplt.subplot(1, 3, 3)\nplt.scatter(min_max_scaled_data_sklearn[:, 0], min_max_scaled_data_sklearn[:, 1], color='red')\nplt.title('Min-Max Scaled Data')\nplt.xlabel('Metric A (Min-Max Scaled)')\nplt.ylabel('Metric B (Min-Max Scaled)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.8: Camparison of Min-max and standard scalings with original data\n\n\n\n\n\n\n\n2.2.2 More on Matrix Product and its Applications\nIn the first module of our course, we introduced matrix products as scalar projections, focusing on how matrices interact through basic operations. In this section, we will expand on this by exploring different types of matrix products that have practical importance in various fields. One such product is the Hadamard product, which is particularly useful in applications ranging from image processing to neural networks and statistical analysis. We will cover the definition, properties, and examples of the Hadamard product, and then delve into practical applications with simulated data.\n\n2.2.2.1 Hadamard Product\nThe Hadamard product (or element-wise product) of two matrices is a binary operation that combines two matrices of the same dimensions to produce another matrix of the same dimensions, where each element is the product of corresponding elements in the original matrices.\n\n\n\n\n\n\nDefinition (Hadamard Product):\n\n\n\nFor two matrices \\(A\\) and \\(B\\) of the same dimension \\(m \\times n\\), the Hadamard product \\(A \\circ B\\) is defined as:\n\\[(A \\circ B)_{ij} = A_{ij} \\cdot B_{ij}\\]\nwhere \\(\\cdot\\) denotes element-wise multiplication.\n\n\n\n\n\n\n\n\nProperties of Hadamard Product\n\n\n\n\nCommutativity: \\[A \\circ B = B \\circ A\\]\nAssociativity: \\[(A \\circ B) \\circ C = A \\circ (B \\circ C)\\]\nDistributivity: \\[A \\circ (B + C) = (A \\circ B) + (A \\circ C)\\]\n\n\n\nSome simple examples to demonstrate the Hadamard product is given below.\nExample 1: Basic Hadamard Product\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 & 6 \\\\7 & 8\\end{pmatrix}\\]\nThe Hadamard product \\(A \\circ B\\) is:\n\\[A \\circ B = \\begin{pmatrix}1 \\cdot 5 & 2 \\cdot 6 \\\\3 \\cdot 7 & 4 \\cdot 8\\end{pmatrix} = \\begin{pmatrix}5 & 12 \\\\21 & 32\\end{pmatrix}\\]\nExample 2: Hadamard Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{pmatrix}, \\quad B = \\begin{pmatrix}9 & 8 & 7 \\\\6 & 5 & 4 \\\\3 & 2 & 1\\end{pmatrix}\\]\nThe Hadamard product \\(A \\circ B\\) is:\n\\[A \\circ B = \\begin{pmatrix}1 \\cdot 9 & 2 \\cdot 8 & 3 \\cdot 7 \\\\4 \\cdot 6 & 5 \\cdot 5 & 6 \\cdot 4 \\\\7 \\cdot 3 & 8 \\cdot  & 9 \\cdot 1\\end{pmatrix} = \\begin{pmatrix}9 & 16 & 21 \\\\24 & 25 & 24 \\\\21 & 16 & 9\\end{pmatrix}\\]\nIn the following code chunks the computational process of Hadamard product is implemented in Python. Here both the from the scratch and use of external module versions are included.\n1. Compute Hadamard Product from Scratch (without Libraries)\nHere’s how you can compute the Hadamard product manually:\n\n# Define matrices A and B\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8, 9], [10, 11, 12]]\n\n# Function to compute Hadamard product\ndef hadamard_product(A, B):\n    # Get the number of rows and columns\n    num_rows = len(A)\n    num_cols = len(A[0])\n    \n    # Initialize the result matrix\n    result = [[0]*num_cols for _ in range(num_rows)]\n    \n    # Compute the Hadamard product\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result[i][j] = A[i][j] * B[i][j]\n    \n    return result\n\n# Compute Hadamard product\nhadamard_product_result = hadamard_product(A, B)\n\n# Display result\nprint(\"Hadamard Product (From Scratch):\")\nfor row in hadamard_product_result:\n    print(row)\n\nHadamard Product (From Scratch):\n[7, 16, 27]\n[40, 55, 72]\n\n\n2. Compute Hadamard Product Using SymPy\nHere’s how to compute the Hadamard product using SymPy:\n\nimport sympy as sp\n\n# Define matrices A and B\nA = sp.Matrix([[1, 2, 3], [4, 5, 6]])\nB = sp.Matrix([[7, 8, 9], [10, 11, 12]])\n\n# Compute Hadamard product using SymPy\nHadamard_product_sympy = A.multiply_elementwise(B)\n\n# Display result\nprint(\"Hadamard Product (Using SymPy):\")\nprint(Hadamard_product_sympy)\n\nHadamard Product (Using SymPy):\nMatrix([[7, 16, 27], [40, 55, 72]])\n\n\nPractical Applications\nApplication 1: Image Masking\nThe Hadamard product can be used for image masking. Here’s how you can apply a mask to an image and visualize it as shown in Fig 2.9.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulated large image (2D array) using NumPy\nimage = np.random.rand(100, 100)\n\n# Simulated mask (binary matrix) using NumPy\nmask = np.random.randint(0, 2, size=(100, 100))\n\n# Compute Hadamard product\nmasked_image = image * mask\n\n# Plot original image and masked image\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].imshow(image, cmap='gray')\nax[0].set_title('Original Image')\nax[1].imshow(masked_image, cmap='gray')\nax[1].set_title('Masked Image')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.9: Demonstration of Masking in DIP using Hadamard Product\n\n\n\n\n\nApplication 2: Element-wise Scaling in Neural Networks\nThe Hadamard product can be used for dropout1 in neural networks. A simple simulated example is given below.\n\n# Simulated large activations (2D array) using NumPy\nactivations = np.random.rand(100, 100)\n\n# Simulated dropout mask (binary matrix) using NumPy\ndropout_mask = np.random.randint(0, 2, size=(100, 100))\n\n# Apply dropout\ndropped_activations = activations * dropout_mask\n\n# Display results\nprint(\"Original Activations:\")\nprint(activations)\nprint(\"\\nDropout Mask:\")\nprint(dropout_mask)\nprint(\"\\nDropped Activations:\")\nprint(dropped_activations)\n\nOriginal Activations:\n[[5.43153791e-01 9.62383964e-01 7.63874101e-02 ... 5.72163623e-01\n  8.17080964e-01 2.41271188e-01]\n [1.90551263e-01 1.50580931e-01 1.59197846e-01 ... 9.75857625e-01\n  5.16425633e-01 5.53577614e-01]\n [4.25325757e-01 5.71067701e-01 4.62505715e-01 ... 5.02880266e-01\n  9.38140071e-01 8.42002871e-01]\n ...\n [9.24897201e-02 4.98215557e-01 4.09217556e-04 ... 8.48142327e-01\n  5.66011719e-01 4.86086429e-01]\n [9.46405520e-01 7.43941659e-01 1.53349416e-01 ... 2.50571287e-01\n  7.89539960e-01 5.22188824e-02]\n [9.61309788e-01 2.84436542e-01 2.73405480e-01 ... 2.21103655e-01\n  2.76761437e-01 4.52046877e-01]]\n\nDropout Mask:\n[[0 0 1 ... 1 1 0]\n [0 0 0 ... 1 0 0]\n [0 1 0 ... 1 0 1]\n ...\n [0 1 0 ... 0 1 1]\n [1 1 1 ... 0 0 0]\n [1 0 0 ... 1 1 0]]\n\nDropped Activations:\n[[0.         0.         0.07638741 ... 0.57216362 0.81708096 0.        ]\n [0.         0.         0.         ... 0.97585762 0.         0.        ]\n [0.         0.5710677  0.         ... 0.50288027 0.         0.84200287]\n ...\n [0.         0.49821556 0.         ... 0.         0.56601172 0.48608643]\n [0.94640552 0.74394166 0.15334942 ... 0.         0.         0.        ]\n [0.96130979 0.         0.         ... 0.22110366 0.27676144 0.        ]]\n\n\nApplication 3: Statistical Data Analysis\nIn statistics, the Hadamard product can be applied to scale covariance matrices. Here’s how we can compute the covariance matrix using matrix operations and apply scaling. Following Python code demonstrate this.\n\nimport sympy as sp\nimport numpy as np\n\n# Simulated large dataset (2D array) using NumPy\ndata = np.random.rand(100, 10)\n\n# Compute the mean of each column\nmean = np.mean(data, axis=0)\n\n# Center the data\ncentered_data = data - mean\n\n# Compute the covariance matrix using matrix product operation\ncov_matrix = (centered_data.T @ centered_data) / (centered_data.shape[0] - 1)\ncov_matrix_sympy = sp.Matrix(cov_matrix)\n\n# Simulated scaling factors (2D array) using SymPy Matrix\nscaling_factors = sp.Matrix(np.random.rand(10, 10))\n\n# Compute Hadamard product\nscaled_cov_matrix = cov_matrix_sympy.multiply(scaling_factors)\n\n# Display results\nprint(\"Covariance Matrix:\")\nprint(cov_matrix_sympy)\nprint(\"\\nScaling Factors:\")\nprint(scaling_factors)\nprint(\"\\nScaled Covariance Matrix:\")\nprint(scaled_cov_matrix)\n\nCovariance Matrix:\nMatrix([[0.0802459794439421, 0.00316761216653116, 0.0110596011583817, 0.0181185450249288, -0.0146187770811494, 0.00470203144890464, 0.000425539790425243, -0.00682996216767847, 0.00432892181262246, -0.0111626692062763], [0.00316761216653116, 0.0745005750030244, 0.00524419589043266, 0.00720987639239865, -0.00214479928877799, -0.000220439358784978, -0.0102570543719054, -0.0147308214138342, 0.00639961512827329, 0.00958246307803259], [0.0110596011583817, 0.00524419589043266, 0.0704021687491803, 0.00596842075737487, -0.00400834340262805, 0.00182434039581439, -0.0139877051742245, -0.00450767198982659, -0.00301144791237551, 0.000139060420059220], [0.0181185450249288, 0.00720987639239865, 0.00596842075737487, 0.0783009082326542, 0.00325602535871958, 0.0116055698713151, 0.0132610421987366, 0.00298857519067976, -0.00281420626581457, -0.00133239232046959], [-0.0146187770811494, -0.00214479928877799, -0.00400834340262805, 0.00325602535871958, 0.0727473731772742, -0.0127412403909042, 0.00472847848989389, 0.0121102743509024, 0.000310582343592597, 0.00516836382861972], [0.00470203144890464, -0.000220439358784978, 0.00182434039581439, 0.0116055698713151, -0.0127412403909042, 0.0929647752655750, -0.00141813503123604, 0.0196769287336856, -0.00682430109702627, 0.00174090816500395], [0.000425539790425243, -0.0102570543719054, -0.0139877051742245, 0.0132610421987366, 0.00472847848989389, -0.00141813503123604, 0.0745304020857527, 0.00717198443264252, -0.00538332738142420, -0.00787800512857588], [-0.00682996216767847, -0.0147308214138342, -0.00450767198982659, 0.00298857519067976, 0.0121102743509024, 0.0196769287336856, 0.00717198443264252, 0.0957979365007534, -0.00520156596447741, -0.00368255126811522], [0.00432892181262246, 0.00639961512827329, -0.00301144791237551, -0.00281420626581457, 0.000310582343592597, -0.00682430109702627, -0.00538332738142420, -0.00520156596447741, 0.0789477997576823, 0.0104327769822155], [-0.0111626692062763, 0.00958246307803259, 0.000139060420059220, -0.00133239232046959, 0.00516836382861972, 0.00174090816500395, -0.00787800512857588, -0.00368255126811522, 0.0104327769822155, 0.0897201492187388]])\n\nScaling Factors:\nMatrix([[0.241035282335252, 0.404192193819690, 0.133254061257886, 0.505591541662115, 0.487701366439416, 0.0791559087784863, 0.719556604535888, 0.489746372762736, 0.671953056528303, 0.126489964692729], [0.992976536796623, 0.0699792944604534, 0.989483630305834, 0.0824570820035997, 0.665170148674908, 0.271965822087616, 0.454295947259546, 0.540247444224669, 0.297877708982769, 0.500772834392218], [0.0136020925452112, 0.823951194543269, 0.988685790421244, 0.184821744252162, 0.423560801915781, 0.598007518345911, 0.255268172435621, 0.727980396494851, 0.191826642045881, 0.393538682111660], [0.769839456312620, 0.839773766959744, 0.499615672135859, 0.915612650509459, 0.476679580998412, 0.708730239040374, 0.102037519530376, 0.272001963203813, 0.108320687293893, 0.297802485544629], [0.177418422534319, 0.876792189156311, 0.222851126130205, 0.584211339717469, 0.0721576910841829, 0.738077300144945, 0.805696845380612, 0.501439795677470, 0.843849745491406, 0.634158790615285], [0.189258052046017, 0.0749899727448611, 0.287571333553747, 0.386311710573009, 0.685417879020709, 0.300000172698498, 0.374938130918473, 0.893710287007465, 0.637420539212613, 0.0562557742320328], [0.747814973491226, 0.477907322542136, 0.607382167619181, 0.0986463822743091, 0.648562082504072, 0.265765426450678, 0.773001006732102, 0.685376621805596, 0.959543300028570, 0.673851819958669], [0.188286653606758, 0.737218530002754, 0.835263657008141, 0.747935895473488, 0.672597995519899, 0.136258767713227, 0.659004812344661, 0.649922380266153, 0.191184387045208, 0.877786561727445], [0.818839667958074, 0.596712930907451, 0.0830814985533730, 0.119001686624797, 0.252206421603737, 0.959882055745458, 0.344768082666072, 0.448792206733051, 0.764622892005308, 0.755741520318363], [0.855812493333970, 0.570720998851224, 0.944134126916719, 0.470006703960890, 0.550991541058076, 0.192817883664161, 0.788360313162527, 0.353536976256503, 0.510250783243687, 0.942773351066496]])\n\nScaled Covariance Matrix:\nMatrix([[0.0279063138240351, 0.0359000396005339, 0.0162827835983265, 0.0429447225213087, 0.0473556437709649, 0.0184744774437033, 0.0423573951707934, 0.0447118214851341, 0.0463271254555213, -0.000482235085611052], [0.0829374162121477, 0.00849826440558878, 0.0734296111654679, 0.00721299450382308, 0.0467862097958942, 0.0303664837952448, 0.0285126516829013, 0.0359623641303012, 0.0212814622769161, 0.0345751978567652], [-0.000596255899401174, 0.0527540155999975, 0.0665018061260251, 0.0178194252074258, 0.0297212595061582, 0.0390260567357367, 0.0116636578200684, 0.0469260383617797, 0.00441033778800723, 0.0155419818692450], [0.0816957377940240, 0.0883268962067135, 0.0676917194520543, 0.0915193469103477, 0.0708411003241261, 0.0693166140680595, 0.0430002853876089, 0.0597133273532864, 0.0445584606047654, 0.0424634033771081], [0.0177877578846246, 0.0705243369564535, 0.0240340303486761, 0.0442404766901140, 0.00195237187614962, 0.0522424324786434, 0.0574682347682366, 0.0278190064229376, 0.0521152073238987, 0.0608094918793593], [0.0237537892847350, 0.0196842081994396, 0.0485558234672088, 0.0563756417351098, 0.0828047626723341, 0.0242032743714420, 0.0404142271899969, 0.0927324459080017, 0.0512783680513968, 0.0146087265152096], [0.0464418553443247, 0.0363023552199630, 0.0267232151660160, 0.0195137159284515, 0.0406136865774241, 0.0154408766212477, 0.0509787190051162, 0.0397364258444630, 0.0638203483505311, 0.0413030098360834], [0.00782835871617469, 0.0759439285441924, 0.0703714438291838, 0.0819202042342318, 0.0664906575366727, 0.0189733833714133, 0.0686611666348342, 0.0734266038502033, 0.0325840044017515, 0.0811822682514756], [0.0725231633898075, 0.0437694917668591, 0.00942713000067131, 0.00700510700594439, 0.0177654900730687, 0.0721224380679933, 0.0305121809764064, 0.0287258038839247, 0.0593733910586548, 0.0628486165663054], [0.0857889018500152, 0.0507672575619655, 0.0868321977305464, 0.0375232275881484, 0.0463998870964131, 0.0299166918638927, 0.0668496813166301, 0.0322051014450369, 0.0462005585938090, 0.0903492533289207]])\n\n\n\n\n2.2.2.2 Practice Problems\nProblem 1: Basic Hadamard Product\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&6\\\\7&8\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot 5&2\\cdot 6\\\\3\\cdot7&4\\cdot 8 \\end{bmatrix}=\\begin{bmatrix}5&12\\\\21&32\\end{bmatrix}\\]\nProblem 2: Hadamard Product with Identity Matrix\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\] \\[I=\\begin{bmatrix}1&0&0\\\\0&1&0\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ I\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot1&2\\cdot 0&3\\cdot 0\\\\4\\cdot 0&5\\cdot 1&6\\cdot 0 \\end{bmatrix}= \\begin{bmatrix} 1&0&0\\\\0&5&0\\end{bmatrix}\\]\nProblem 3: Hadamard Product with Zero Matrix\nGiven matrices: \\[A=\\begin{bmatrix}3&4\\\\5&6\\end{bmatrix}\\] \\[Z=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ Z\\).\nSolution:\n\\[C=\\begin{bmatrix}3\\cdot 0&4\\cdot 0\\\\ 5\\cdot 0&6\\cdot 0 \\end{bmatrix}=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}\\]\nProblem 4: Hadamard Product of Two Identity Matrices\nGiven identity matrices: \\[I_2=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[I_3=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}\\]\nFind the Hadamard product \\(C=I_2\\circ I_3\\) (extend \\(I_2\\) to match dimensions of \\(I_3\\)).\nSolution:\nExtend \\(I_2\\) to \\(I_3\\): \\[I_2=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}\\]\n\\[C=\\begin{bmatrix}1\\cdot 1&0\\cdot 0&0\\cdot 0\\\\0\\cdot 0&1\\cdot 1&0\\cdot 0\\\\0\\cdot 0&0\\cdot 0&0\\cdot 1\\end{bmatrix}=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}\\]\nProblem 5: Hadamard Product with Random Matrices\nGiven random matrices: \\[A=\\begin{bmatrix}2&3\\\\1&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&5\\\\6&2\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}2\\cdot 0&3\\cdot 5\\\\1\\cdot 6&4\\cdot 2\\end{bmatrix}=\\begin{bmatrix}0&15\\\\6&8\\end{bmatrix}\\]\nProblem 6: Hadamard Product of 3x3 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\\\7&8&9\\end{bmatrix}\\] \\[B=\\begin{bmatrix}9&8&7\\\\6&5&4\\\\3&2&1\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot 9&2\\cdot 8&3\\cdot 7\\\\4\\cdot 6&5\\cdot 5&6\\cdot 4\\\\7\\cdot 3&8\\cdot 2&9\\cdot 1\\end{bmatrix}=\\begin{bmatrix}9&16&21\\\\24&25&24\\\\21&16&9\\end{bmatrix}\\]\nProblem 7: Hadamard Product of Column Vectors\nGiven column vectors: \\[u=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[v=\\begin{bmatrix}5\\\\6\\end{bmatrix}\\]\nFind the Hadamard product \\(w=u\\circ v\\).\nSolution:\n\\[w=\\begin{bmatrix}2\\cdot 5\\\\3\\cdot 6\\end{bmatrix}=\\begin{bmatrix}10\\\\18\\end{bmatrix}\\]\nProblem 8: Hadamard Product with Non-Square Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\\\5&6\\end{bmatrix}\\] \\[B=\\begin{bmatrix}7&8\\\\9&10\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\) (extend \\(B\\) to match dimensions of \\(A\\)).\nSolution:\nExtend \\(B\\) to match dimensions of \\(A\\): \\[B=\\begin{bmatrix}7&8\\\\9&10\\\\7&8\\end{bmatrix}\\]\n\\[C=\\begin{bmatrix}1\\cdot 7&2\\cdot 8\\\\3\\cdot 9&4\\cdot 10\\\\5\\cdot 7&6\\cdot 8\\end{bmatrix}=\\begin{bmatrix}7&16\\\\27&40\\\\35&48\\end{bmatrix}\\]\nProblem 9: Hadamard Product in Image Processing\nGiven matrices representing image pixel values: \\[A=\\begin{bmatrix}10&20\\\\30&40\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0.5&1.5\\\\2.0&0.5\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}10\\cdot 0.5&20\\cdot 1.5\\\\30\\cdot 2.0&40\\cdot 0.5\\end{bmatrix}=\\begin{bmatrix}5&30\\\\60&20\\end{bmatrix}\\]\nProblem 10: Hadamard Product in Statistical Data\nGiven matrices representing two sets of statistical data:\n\\[A=\\begin{bmatrix}5&6&7\\\\8&9&10\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}5\\cdot 1&6\\cdot 2&7\\cdot 3\\\\8\\cdot 4&9\\cdot 5&10\\cdot 6\\end{bmatrix}=\\begin{bmatrix}5&12&21\\\\32&45&60\\end{bmatrix}\\]\n\n\n2.2.2.3 Inner Product of Matrices\nThe inner product of two matrices is a generalized extension of the dot product, where each matrix is treated as a vector in a high-dimensional space. For two matrices \\(A\\) and \\(B\\) of the same dimension \\(m \\times n\\), the inner product is defined as the sum of the element-wise products of the matrices.\n\n\n\n\n\n\nDefinition (Inner product)\n\n\n\nFor two matrices \\(A\\) and \\(B\\) of dimension \\(m \\times n\\), the inner product \\(\\langle A, B \\rangle\\) is given by:\n\\[\\langle A, B \\rangle = \\sum_{i=1}^{m} \\sum_{j=1}^{n} A_{ij} \\cdot B_{ij}\\]\nwhere \\(\\cdot\\) denotes element-wise multiplication.\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nCommutativity: \\[\\langle A, B \\rangle = \\langle B, A \\rangle\\]\nLinearity: \\[\\langle A + C, B \\rangle = \\langle A, B \\rangle + \\langle C, B \\rangle\\]\nPositive Definiteness: \\[\\langle A, A \\rangle \\geq 0\\] with equality if and only if \\(A\\) is a zero matrix.\n\n\n\nSome simple examples showing the mathematical process of calculating the inner product is given bellow.\nExample 1: Basic Inner Product\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 & 6 \\\\7 & 8\\end{pmatrix}\\]\nThe inner product \\(\\langle A, B \\rangle\\) is:\n\\[\\langle A, B \\rangle = 1 \\cdot 5 + 2 \\cdot 6 + 3 \\cdot 7 + 4 \\cdot 8 = 5 + 12 + 21 + 32 = 70\\]\nExample 2: Inner Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{pmatrix}, \\quad B = \\begin{pmatrix}9 & 8 & 7 \\\\6 & 5 & 4 \\\\3 & 2 & 1\\end{pmatrix}\\]\nThe inner product \\(\\langle A, B \\rangle\\) is calculated as: \\[\\begin{align*}\n\\langle A, B \\rangle &= 1 \\cdot 9 + 2 \\cdot 8 + 3 \\cdot 7 + 4 \\cdot 6 + 5 \\cdot 5 + 6 \\cdot 4 + 7 \\cdot 3 + 8 \\cdot 2 + 9 \\cdot 1\\\\\n&= 9 + 16 + 21 + 24 + 25 + 24 + 21 + 16 + 9\\\\\n&= 175\n\\end{align*}\\]\n\n\n2.2.2.4 Practice Problems\nProblem 1: Inner Product of 2x2 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&6\\\\7&8\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot5 + 2\\cdot6 + 3\\cdot7 + 4\\cdot8 \\\\\n&= 5 + 12 + 21 + 32 \\\\\n&= 70\n\\end{align*}\\]\n\nProblem 2: Inner Product of 3x3 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&0&2\\\\3&4&5\\\\6&7&8\\end{bmatrix}\\] \\[B=\\begin{bmatrix}8&7&6\\\\5&4&3\\\\2&1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot8 + 0\\cdot7 + 2\\cdot6 + \\\\\n&\\quad 3\\cdot5 + 4\\cdot4 + 5\\cdot3 + \\\\\n&\\quad 6\\cdot2 + 7\\cdot1 + 8\\cdot0 \\\\\n&= 8 + 0 + 12 + 15 + 16 + 15 + 12 + 7 + 0 \\\\\n&= 85\n\\end{align*}\\]\n\nProblem 3: Inner Product of Diagonal Matrices\nGiven diagonal matrices: \\[A=\\begin{bmatrix}2&0&0\\\\0&3&0\\\\0&0&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&0&0\\\\0&6&0\\\\0&0&7\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 2\\cdot5 + 0\\cdot0 + 0\\cdot0 + \\\\\n&\\quad 0\\cdot0 + 3\\cdot6 + 0\\cdot0 + \\\\\n&\\quad 0\\cdot0 + 0\\cdot0 + 4\\cdot7 \\\\\n&= 10 + 0 + 0 + 0 + 18 + 0 + 0 + 0 + 28 \\\\\n&= 56\n\\end{align*}\\]\n\nProblem 4: Inner Product of Column Vectors\nGiven column vectors: \\[u=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\] \\[v=\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle u,v \\rangle &= \\sum_{i} u_i v_i \\\\\n&= 1\\cdot4 + 2\\cdot5 + 3\\cdot6 \\\\\n&= 4 + 10 + 18 \\\\\n&= 32\n\\end{align*}\\]\n\nProblem 5: Inner Product with Random Matrices\nGiven matrices: \\[A=\\begin{bmatrix}3&2\\\\1&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&7\\\\8&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 3\\cdot5 + 2\\cdot7 + \\\\\n&\\quad 1\\cdot8 + 4\\cdot6 \\\\\n&= 15 + 14 + 8 + 24 \\\\\n&= 61\n\\end{align*}\\]\n\nProblem 6: Inner Product of 2x3 and 3x2 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\] \\[B=\\begin{bmatrix}7&8\\\\9&10\\\\11&12\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot7 + 2\\cdot8 + 3\\cdot11 + \\\\\n&\\quad 4\\cdot9 + 5\\cdot10 + 6\\cdot12 \\\\\n&= 7 + 16 + 33 + 36 + 50 + 72 \\\\\n&= 214\n\\end{align*}\\]\n\nProblem 7: Inner Product with Transpose Operation\nGiven matrices: \\[A=\\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\\] \\[B=\\begin{bmatrix}6&7\\\\8&9\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 2\\cdot6 + 3\\cdot7 + \\\\\n&\\quad 4\\cdot8 + 5\\cdot9 \\\\\n&= 12 + 21 + 32 + 45 \\\\\n&= 110\n\\end{align*}\\]\n\nProblem 8: Inner Product of Symmetric Matrices\nGiven symmetric matrices: \\[A=\\begin{bmatrix}1&2\\\\2&3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&5\\\\5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot4 + 2\\cdot5 + \\\\\n&\\quad 2\\cdot5 + 3\\cdot6 \\\\\n&= 4 + 10 + 10 + 18 \\\\\n&= 42\n\\end{align*}\\]\n\nProblem 9: Inner Product with Complex Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1+i&2-i\\\\3+i&4-i\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5-i&6+i\\\\7-i&8+i\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} \\text{Re}(A_{ij} \\overline{B_{ij}}) \\\\\n&= (1+i)\\cdot(5+i) + (2-i)\\cdot(6-i) + \\\\\n&\\quad (3+i)\\cdot(7+i) + (4-i)\\cdot(8+i) \\\\\n&= (5+i+5i-i^2) + (12-i-6i+i^2) + \\\\\n&\\quad (21+i+7i-i^2) + (32+i-8i-i^2) \\\\\n&= 5+5 + 12 - 6 + 21 + 32 - 2 \\\\\n&= 62\n\\end{align*}\\]\n\nProblem 10: Inner Product of 4x4 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3&4\\\\5&6&7&8\\\\9&10&11&12\\\\13&14&15&16\\end{bmatrix}\\] \\[B=\\begin{bmatrix}16&15&14&13\\\\12&11&10&9\\\\8&7&6&5\\\\4&3&2&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot16 + 2\\cdot15 + 3\\cdot14 + 4\\cdot13 + \\\\\n&\\quad 5\\cdot12 + 6\\cdot11 + 7\\cdot10 + 8\\cdot9 + \\\\\n&\\quad 9\\cdot8 + 10\\cdot7 + 11\\cdot6 + 12\\cdot5 + \\\\\n&\\quad 13\\cdot4 + 14\\cdot3 + 15\\cdot2 + 16\\cdot1 \\\\\n&= 16 + 30 + 42 + 52 + 60 + 66 + 70 + 72 + \\\\\n&\\quad 72 + 70 + 66 + 60 + 52 + 42 + 30 + 16 \\\\\n&= 696\n\\end{align*}\\]\n\nNow let’s look into the computational part of inner product.\n\nCompute Inner Product from Scratch (without Libraries)\n\nHere’s how you can compute the inner product from the scratch:\n\n# Define matrices A and B\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8, 9], [10, 11, 12]]\n\n# Function to compute inner product\ndef inner_product(A, B):\n    # Get the number of rows and columns\n    num_rows = len(A)\n    num_cols = len(A[0])\n    \n    # Initialize the result\n    result = 0\n    \n    # Compute the inner product\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result += A[i][j] * B[i][j]\n    \n    return result\n\n# Compute inner product\ninner_product_result = inner_product(A, B)\n\n# Display result\nprint(\"Inner Product (From Scratch):\")\nprint(inner_product_result)\n\nInner Product (From Scratch):\n217\n\n\n\nCompute Inner Product Using NumPy\n\nHere’s how to compute the inner product using Numpy:\n\nimport numpy as np\n# Define matrices A and B\nA = np.array([[1, 2, 3], [4, 5, 6]])\nB = np.array([[7, 8, 9], [10, 11, 12]])\n# calculating innerproduct\ninner_product = (A*B).sum() # calculate element-wise product, then column sum\n\nprint(\"Inner Product (Using numpy):\")\nprint(inner_product)\n\nInner Product (Using numpy):\n217\n\n\nThe same operation can be done using SymPy functions as follows.\n\nimport sympy as sp\nimport numpy as np  \n# Define matrices A and B\nA = sp.Matrix([[1, 2, 3], [4, 5, 6]])\nB = sp.Matrix([[7, 8, 9], [10, 11, 12]])\n\n# Compute element-wise product\nelementwise_product = A.multiply_elementwise(B)\n\n# Calculate sum of each column\ninner_product_sympy = np.sum(elementwise_product)\n\n# Display result\nprint(\"Inner Product (Using SymPy):\")\nprint(inner_product_sympy)\n\nInner Product (Using SymPy):\n217\n\n\nA vector dot product (in Physics) can be calculated using SymPy .dot() function as shown below.\nLet \\(A=\\begin{pmatrix}1&2&3\\end{pmatrix}\\) and \\(B=\\begin{pmatrix}4&5&6\\end{pmatrix}\\), then the dot product, \\(A\\cdot B\\) is computed as:\n\nimport sympy as sp\nA=sp.Matrix([1,2,3])\nB=sp.Matrix([4,5,6])\ndisplay(A.dot(B)) # calculate fot product of A and B\n\n\\(\\displaystyle 32\\)\n\n\n\n\n\n\n\n\nA word of caution\n\n\n\nIn SymPy , sp.Matrix([1,2,3]) create a column vector. But np.array([1,2,3]) creates a row vector. So be careful while applying matrix/ dot product operations on these objects.\n\n\nThe same dot product using numpy object can be done as follows:\n\nimport numpy as np\nA=np.array([1,2,3])\nB=np.array([4,5,6])\ndisplay(A.dot(B.T))# dot() stands for dot product B.T represents the transpose of B\n\nnp.int64(32)\n\n\nPractical Applications\nApplication 1: Signal Processing\nIn signal processing, the inner product can be used to measure the similarity between two signals. Here the most popular measure of similarity is the cosine similarity. This measure is defined as:\n\\[\\cos \\theta=\\dfrac{A\\cdot B}{||A|| ||B||}\\]\nNow consider two digital signals are given. It’s cosine similarity measure can be calculated with a simulated data as shown below.\n\nimport numpy as np\n\n# Simulated large signals (1D array) using NumPy\nsignal1 = np.sin(np.random.rand(1000))\nsignal2 = np.cos(np.random.rand(1000))\n\n# Compute inner product\ninner_product_signal = np.dot(signal1, signal2)\n#cosine_sim=np.dot(signal1,signal2)/(np.linalg.norm(signal1)*np.linalg.norm(signal2))\n# Display result\ncosine_sim=inner_product_signal/(np.sqrt(np.dot(signal1,signal1))*np.sqrt(np.dot(signal2,signal2)))\nprint(\"Inner Product (Using numpy):\")\nprint(inner_product_signal)\nprint(\"Similarity of signals:\")\nprint(cosine_sim)\n\nInner Product (Using numpy):\n392.9911591500542\nSimilarity of signals:\n0.8735481544132466\n\n\nApplication 2: Machine Learning - Feature Similarity\nIn machine learning, the inner product is used to calculate the similarity between feature vectors.\n\nimport numpy as np\n\n# Simulated feature vectors (2D array) using NumPy\nfeatures1 = np.random.rand(100, 10)\nfeatures2 = np.random.rand(100, 10)\n\n# Compute inner product for each feature vector\ninner_products = np.einsum('ij,ij-&gt;i', features1, features2) # use Einstien's sum\n\n# Display results\nprint(\"Inner Products of Feature Vectors:\")\ndisplay(inner_products)\n\nInner Products of Feature Vectors:\n\n\narray([2.27776338, 2.57830182, 3.56620444, 2.28510313, 1.67407687,\n       2.47188401, 1.72664512, 3.91522369, 2.10032041, 1.70002827,\n       2.38714324, 1.84215204, 2.10608211, 3.12923283, 1.73967449,\n       2.13960411, 1.85324405, 1.75200565, 2.75859845, 4.18041781,\n       1.85052924, 2.88347543, 2.01870765, 3.16795097, 3.9781969 ,\n       2.71951008, 2.17112348, 2.7086382 , 1.64524117, 2.547652  ,\n       2.75616501, 2.247287  , 2.99201123, 2.04336543, 1.01660769,\n       2.29907718, 1.94169097, 3.64161484, 2.07950876, 3.60447454,\n       2.23580851, 1.64450625, 2.66620899, 1.45201891, 2.01650098,\n       3.60276287, 2.27194098, 2.96454252, 1.84983969, 2.74966643,\n       2.82843907, 1.53229135, 3.10024438, 1.38240307, 1.96905283,\n       2.98762882, 2.54365791, 1.71545605, 2.20413625, 2.16090386,\n       3.32828193, 0.92770208, 2.28994122, 1.31516311, 1.68024212,\n       2.39307024, 2.48810287, 3.16377163, 1.6978666 , 2.2110908 ,\n       3.19338647, 1.69394993, 3.47873547, 3.2988957 , 3.01375011,\n       2.37060402, 2.92471302, 3.19503524, 2.1312127 , 1.7758921 ,\n       3.63795215, 3.40406252, 1.54290523, 2.94255488, 2.14374412,\n       2.09313318, 1.71143727, 3.25915497, 2.25868428, 2.15088071,\n       3.24478827, 3.32634904, 1.94834626, 2.94910827, 1.45521725,\n       2.00705719, 1.54859632, 2.31874324, 1.93656216, 3.60781744])\n\n\nApplication 3: Covariance Matrix in Statistics\nThe inner product can be used to compute covariance matrices for statistical data analysis. If \\(X\\) is a given distribution and \\(x=X-\\bar{X}\\). Then the covariance of \\(X\\) can be calculated as \\(cov(X)=\\dfrac{1}{n-1}(x\\cdot x^T)\\) 2. The python code a simulated data is shown below.\n\nimport sympy as sp\nimport numpy as np\n\n# Simulated large dataset (2D array) using NumPy\ndata = np.random.rand(100, 10)\n\n# Compute the mean of each column\nmean = np.mean(data, axis=0)\n\n# Center the data\ncentered_data = data - mean\n\n# Compute the covariance matrix using matrix product operation\ncov_matrix = (centered_data.T @ centered_data) / (centered_data.shape[0] - 1)\ncov_matrix_sympy = sp.Matrix(cov_matrix)\n\n# Display results\nprint(\"Covariance Matrix:\")\ndisplay(cov_matrix_sympy)\n\nCovariance Matrix:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0947852673688543 & -0.000166472457905025 & -0.00112811488426859 & -0.0122543338865731 & 0.0106384249605843 & -0.00255309692096336 & -0.00824067447853588 & 0.00344533768551651 & 0.00463937103799542 & -0.0158284767892333\\\\-0.000166472457905025 & 0.0961263342346109 & 0.00406373591701362 & 0.0119647355673112 & -0.0105969313955045 & 0.00955345523842901 & 0.00273636864492482 & 0.00523898988042679 & -0.00456691599196887 & 0.00474458545155822\\\\-0.00112811488426859 & 0.00406373591701362 & 0.0739773693308285 & 0.021797232192434 & -0.00360342297335149 & 0.0098027101365592 & 0.00328367380408949 & -0.00139193275488433 & 0.00941053049308096 & 0.00154835558394245\\\\-0.0122543338865731 & 0.0119647355673112 & 0.021797232192434 & 0.0816862205542537 & 0.00346605879567529 & 0.0198030750114026 & 0.00417294340303088 & -0.00200769792850672 & 0.0135564357828594 & 0.00855990415346672\\\\0.0106384249605843 & -0.0105969313955045 & -0.00360342297335149 & 0.00346605879567529 & 0.0912909962755886 & 0.00344594267553297 & -0.00778601458419096 & 0.00156920376986948 & 0.00932932913056715 & 0.0126955457218353\\\\-0.00255309692096336 & 0.00955345523842901 & 0.0098027101365592 & 0.0198030750114026 & 0.00344594267553297 & 0.0940710289534623 & -0.00570188645499322 & -0.0141534369945574 & 0.00614181740153054 & 0.00107581174112897\\\\-0.00824067447853588 & 0.00273636864492482 & 0.00328367380408949 & 0.00417294340303088 & -0.00778601458419096 & -0.00570188645499322 & 0.0753138264855782 & -0.00100369807633172 & -0.0101680951101373 & 0.00395849720049042\\\\0.00344533768551651 & 0.00523898988042679 & -0.00139193275488433 & -0.00200769792850672 & 0.00156920376986948 & -0.0141534369945574 & -0.00100369807633172 & 0.0768965893734704 & -0.0168835820260702 & 0.0105713732690888\\\\0.00463937103799542 & -0.00456691599196887 & 0.00941053049308096 & 0.0135564357828594 & 0.00932932913056715 & 0.00614181740153054 & -0.0101680951101373 & -0.0168835820260702 & 0.0868569883813788 & -0.00796838791169777\\\\-0.0158284767892333 & 0.00474458545155822 & 0.00154835558394245 & 0.00855990415346672 & 0.0126955457218353 & 0.00107581174112897 & 0.00395849720049042 & 0.0105713732690888 & -0.00796838791169777 & 0.0862732903801952\\end{matrix}\\right]\\)\n\n\nThese examples demonstrate the use of inner product and dot product in various applications.\n\n\n2.2.2.5 Outer Product\nThe outer product of two vectors results in a matrix, and it is a way to combine these vectors into a higher-dimensional representation.\n\n\n\n\n\n\nDefinition (Outer Product)\n\n\n\nFor two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) of dimensions \\(m\\) and \\(n\\) respectively, the outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is an \\(m \\times n\\) matrix defined as:\n\\[(\\mathbf{u} \\otimes \\mathbf{v})_{ij} = u_i \\cdot v_j\\]\nwhere \\(\\cdot\\) denotes the outer product operation. In matrix notation, for two column vectors \\(u,v\\), \\[u\\otimes v=uv^T\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nLinearity: \\[(\\mathbf{u} + \\mathbf{w}) \\otimes \\mathbf{v} = (\\mathbf{u} \\otimes \\mathbf{v}) + (\\mathbf{w} \\otimes \\mathbf{v})\\]\nDistributivity: \\[\\mathbf{u} \\otimes (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} \\otimes \\mathbf{v}) + (\\mathbf{u} \\otimes \\mathbf{w})\\]\nAssociativity: \\[(\\mathbf{u} \\otimes \\mathbf{v}) \\otimes \\mathbf{w} = \\mathbf{u} \\otimes (\\mathbf{v} \\otimes \\mathbf{w})\\]\n\n\n\nSome simple examples of outer product is given below.\nExample 1: Basic Outer Product\nGiven vectors:\n\\[\\mathbf{u} = \\begin{pmatrix}1 \\\\2\\end{pmatrix}, \\quad\\mathbf{v} = \\begin{pmatrix}3 \\\\4 \\\\5\\end{pmatrix}\\]\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix}1 \\cdot 3 & 1 \\cdot 4 & 1 \\cdot 5 \\\\2 \\cdot 3 & 2 \\cdot 4 & 2 \\cdot 5\\end{pmatrix} = \\begin{pmatrix}3 & 4 & 5 \\\\6 & 8 & 10\\end{pmatrix}\\]\nExample 2: Outer Product with Larger Vectors\nGiven vectors: \\[\\mathbf{u} = \\begin{pmatrix}1 \\\\2 \\\\3\\end{pmatrix}, \\quad\\mathbf{v} = \\begin{pmatrix}4 \\\\5\\end{pmatrix}\\]\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix}1 \\cdot 4 & 1 \\cdot 5 \\\\2 \\cdot 4 & 2 \\cdot 5 \\\\3 \\cdot 4 & 3 \\cdot 5\\end{pmatrix} = \\begin{pmatrix}4 & 5 \\\\8 & 10 \\\\12 & 15\\end{pmatrix}\\]\n\n\n2.2.2.6 Practice Problems\nFind the outer product of A and B where A and B are given as follows:\nProblem 1:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\2\\end{bmatrix} \\otimes \\begin{bmatrix}3&4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 3 & 1 \\cdot 4 \\\\\n2 \\cdot 3 & 2 \\cdot 4\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 4 \\\\\n6 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 2:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}4&5&6\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 4 & 1 \\cdot 5 & 1 \\cdot 6 \\\\\n2 \\cdot 4 & 2 \\cdot 5 & 2 \\cdot 6 \\\\\n3 \\cdot 4 & 3 \\cdot 5 & 3 \\cdot 6\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n4 & 5 & 6 \\\\\n8 & 10 & 12 \\\\\n12 & 15 & 18\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 3:\nFind the outer product of: \\[A=\\begin{bmatrix}1&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\end{bmatrix} \\otimes \\begin{bmatrix}3\\\\4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 3 & 1 \\cdot 4 \\\\\n2 \\cdot 3 & 2 \\cdot 4\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 4 \\\\\n6 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 4:\nFind the outer product of: \\[A=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0\\\\1\\end{bmatrix} \\otimes \\begin{bmatrix}1&-1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot 1 & 0 \\cdot -1 \\\\\n1 \\cdot 1 & 1 \\cdot -1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n1 & -1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 5:\nFind the outer product of: \\[A=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}5&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot 5 & 2 \\cdot -2 \\\\\n3 \\cdot 5 & 3 \\cdot -2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n10 & -4 \\\\\n15 & -6\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 6:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&-1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} \\otimes \\begin{bmatrix}2&-1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 2 & 1 \\cdot -1 & 1 \\cdot 0 \\\\\n0 \\cdot 2 & 0 \\cdot -1 & 0 \\cdot 0 \\\\\n1 \\cdot 2 & 1 \\cdot -1 & 1 \\cdot 0\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 & -1 & 0 \\\\\n0 & 0 & 0 \\\\\n2 & -1 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 7:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&0\\\\3&-1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\otimes \\begin{bmatrix}2&0\\\\3&-1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2 & 3&0&-1 \\\\\n-2&-3&0&1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 8:\nFind the outer product of: \\[A=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-2&3\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}3\\\\4\\end{bmatrix} \\otimes \\begin{bmatrix}1&-2&3\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 \\cdot 1 & 3 \\cdot -2 & 3 \\cdot 3 \\\\\n4 \\cdot 1 & 4 \\cdot -2 & 4 \\cdot 3\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & -6 & 9 \\\\\n4 & -8 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 9:\nFind the outer product of: \\[A=\\begin{bmatrix}2\\\\3\\\\-1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\\\-1\\end{bmatrix} \\otimes \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot 4 & 2 \\cdot -2 \\\\\n3 \\cdot 4 & 3 \\cdot -2 \\\\\n-1 \\cdot 4 & -1 \\cdot -2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n8 & -4 \\\\\n12 & -6 \\\\\n-4 & 2\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 10:\nFind the outer product of: \\[A=\\begin{bmatrix}0\\\\5\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0\\\\5\\end{bmatrix} \\otimes \\begin{bmatrix}3&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot 3 & 0 \\cdot 1 \\\\\n5 \\cdot 3 & 5 \\cdot 1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n15 & 5\n\\end{bmatrix}\n\\end{align*}\\]\n\n1. Compute Outer Product of Vectors from Scratch (without Libraries)\nHere’s how you can compute the outer product manually:\n\n# Define vectors u and v\nu = [1, 2]\nv = [3, 4, 5]\n\n# Function to compute outer product\ndef outer_product(u, v):\n    # Initialize the result\n    result = [[a * b for b in v] for a in u]\n    return result\n\n# Compute outer product\nouter_product_result = outer_product(u, v)\n\n# Display result\nprint(\"Outer Product of Vectors (From Scratch):\")\nfor row in outer_product_result:\n    print(row)\n\nOuter Product of Vectors (From Scratch):\n[3, 4, 5]\n[6, 8, 10]\n\n\n2. Compute Outer Product of Vectors Using SymPy\nHere’s how to compute the outer product using SymPy:\n\nimport sympy as sp\n\n# Define vectors u and v\nu = sp.Matrix([1, 2])\nv = sp.Matrix([3, 4, 5])\n\n# Compute outer product using SymPy\nouter_product_sympy = u * v.T\n\n# Display result\nprint(\"Outer Product of Vectors (Using SymPy):\")\ndisplay(outer_product_sympy)\n\nOuter Product of Vectors (Using SymPy):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}3 & 4 & 5\\\\6 & 8 & 10\\end{matrix}\\right]\\)\n\n\nOuter Product of Matrices\nThe outer product of two matrices extends the concept from vectors to higher-dimensional tensors. For two matrices \\(A\\) and \\(B\\), the outer product results in a higher-dimensional tensor and is generally expressed as block matrices.\n\n\n\n\n\n\nDefinition (Outer Product of Matrices)\n\n\n\nFor two matrices \\(A\\) of dimension \\(m \\times p\\) and \\(B\\) of dimension \\(q \\times n\\), the outer product \\(A \\otimes B\\) results in a tensor of dimension \\(m \\times q \\times p \\times n\\). The entries of the tensor are given by:\n\\[(A \\otimes B)_{ijkl} = A_{ij} \\cdot B_{kl}\\]\nwhere \\(\\cdot\\) denotes the outer product operation.\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nLinearity: \\[(A + C) \\otimes B = (A \\otimes B) + (C \\otimes B)\\]\nDistributivity: \\[A \\otimes (B + D) = (A \\otimes B) + (A \\otimes D)\\]\nAssociativity:\n\n\\[(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)\\]\n\n\nHere are some simple examples to demonstrate the mathematical procedure to find outer product of matrices.\nExample 1: Basic Outer Product of Matrices\nGiven matrices: \\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 \\\\6\\end{pmatrix}\\]\nThe outer product \\(A \\otimes B\\) is:\n\\[A \\otimes B = \\begin{pmatrix}1 \\cdot 5 & 1 \\cdot 6 \\\\2 \\cdot 5 & 2 \\cdot 6 \\\\3 \\cdot 5 & 3 \\cdot 6 \\\\4 \\cdot 5 & 4 \\cdot 6\\end{pmatrix} = \\begin{pmatrix}5 & 6 \\\\10 & 12 \\\\15 & 18 \\\\20 & 24\\end{pmatrix}\\]\nExample 2: Outer Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6\\end{pmatrix}, \\quad B = \\begin{pmatrix}7 \\\\8\\end{pmatrix}\\]\nThe outer product \\(A \\otimes B\\) is:\n\\[A \\otimes B = \\begin{pmatrix}1 \\cdot 7 & 1 \\cdot 8 \\\\2 \\cdot 7 & 2 \\cdot 8 \\\\3 \\cdot 7 & 3 \\cdot 8 \\\\4 \\cdot 7 & 4 \\cdot 8 \\\\5 \\cdot 7 & 5 \\cdot 8 \\\\6 \\cdot 7 & 6 \\cdot 8\\end{pmatrix} = \\begin{pmatrix}7 & 8 \\\\14 & 16 \\\\21 & 24 \\\\28 & 32 \\\\35 & 40 \\\\42 & 48\\end{pmatrix}\\]\nExample 3: Compute the outer product of the following vectors \\(\\mathbf{u} = [0, 1, 2]\\) and \\(\\mathbf{v} = [2, 3, 4]\\).\nTo find the outer product, we calculate each element \\((i, j)\\) as the product of the \\((i)\\)-th element of \\(\\mathbf{u}\\) and the \\((j)\\)-th element of \\(\\mathbf{v}\\). Mathematically:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{bmatrix}0 \\cdot 2 & 0 \\cdot 3 & 0 \\cdot 4 \\\\1 \\cdot 2 & 1 \\cdot 3 & 1 \\cdot 4 \\\\2 \\cdot 2 & 2 \\cdot 3 & 2 \\cdot 4\\end{bmatrix}= \\begin{bmatrix}0 & 0 & 0 \\\\2 & 3 & 4 \\\\4 & 6 & 8\\end{bmatrix}\\]\n1. Compute Outer Product of Matrices from Scratch (without Libraries)\nHere’s how you can compute the outer product manually:\n\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5], [6]]\n\n# Function to compute outer product\ndef outer_product_matrices(A, B):\n    m = len(A)\n    p = len(A[0])\n    q = len(B)\n    n = len(B[0])\n    result = [[0] * (n * p) for _ in range(m * q)]\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(q):\n                for l in range(n):\n                    result[i*q + k][j*n + l] = A[i][j] * B[k][l]\n\n    return result\n\n# Compute outer product\nouter_product_result_matrices = outer_product_matrices(A, B)\n\n# Display result\nprint(\"Outer Product of Matrices (From Scratch):\")\nfor row in outer_product_result_matrices:\n    print(row)\n\nOuter Product of Matrices (From Scratch):\n[5, 10]\n[6, 12]\n[15, 20]\n[18, 24]\n\n\nHere is the Python code to compute the outer product of these vectors using the NumPy function .outer():\n\nimport numpy as np\n\n# Define vectors\nu = np.array([[1,2],[3,4]])\nv = np.array([[5],[4]])\n\n# Compute outer product\nouter_product = np.outer(u, v)\n\nprint(\"Outer Product of u and v:\")\ndisplay(outer_product)\n\nOuter Product of u and v:\n\n\narray([[ 5,  4],\n       [10,  8],\n       [15, 12],\n       [20, 16]])\n\n\nExample 3: Real-world Application in Recommendation Systems\nIn recommendation systems, the outer product can represent user-item interactions. A simple context is here. Let the user preferences of items is given as \\(u=[4, 3, 5]\\) and the item scores is given by \\(v=[2, 5, 4]\\). Now the recommendation score can be calculated as the outer product of these two vectors. Calculation of this score is shown below. The outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is calculated as follows:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{bmatrix}4 \\cdot 2 & 4 \\cdot 5 & 4 \\cdot 4 \\\\3 \\cdot 2 & 3 \\cdot 5 & 3 \\cdot 4 \\\\5 \\cdot 2 & 5 \\cdot 5 & 5 \\cdot 4\\end{bmatrix}= \\begin{bmatrix}8 & 20 & 16 \\\\6 & 15 & 12 \\\\10 & 25 & 20\\end{bmatrix}\\]\nThe python code for this task is given below.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the user and product ratings vectors\nuser_ratings = np.array([4, 3, 5])\nproduct_ratings = np.array([2, 5, 4])\n\n# Compute the outer product\npredicted_ratings = np.outer(user_ratings, product_ratings)\n\n# Print the predicted ratings matrix\nprint(\"Predicted Ratings Matrix:\")\ndisplay(predicted_ratings)\n\n# Plot the result\nplt.imshow(predicted_ratings, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.title('Predicted Ratings Matrix (Recommendation System)')\nplt.xlabel('Product Ratings')\nplt.ylabel('User Ratings')\nplt.xticks(ticks=np.arange(len(product_ratings)), labels=product_ratings)\nplt.yticks(ticks=np.arange(len(user_ratings)), labels=user_ratings)\nplt.show()\n\nPredicted Ratings Matrix:\n\n\narray([[ 8, 20, 16],\n       [ 6, 15, 12],\n       [10, 25, 20]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Properties & Definitions\n\n\n\n\nDefinition and Properties\nGiven two vectors:\n\n\\(\\mathbf{u} \\in \\mathbb{R}^m\\)\n\\(\\mathbf{v} \\in \\mathbb{R}^n\\)\n\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) results in an \\(m \\times n\\) matrix where each element \\((i, j)\\) of the matrix is calculated as: \\[(\\mathbf{u} \\otimes \\mathbf{v})_{ij} = u_i \\cdot v_j\\]\nNon-Symmetry\nThe outer product is generally not symmetric. For vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the matrix \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is not necessarily equal to \\(\\mathbf{v} \\otimes \\mathbf{u}\\): \\[\\mathbf{u} \\otimes \\mathbf{v} \\neq \\mathbf{v} \\otimes \\mathbf{u}\\]\nRank of the Outer Product\nThe rank of the outer product matrix \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is always 1, provided neither \\(\\mathbf{u}\\) nor \\(\\mathbf{v}\\) is a zero vector. This is because the matrix can be expressed as a single rank-1 matrix.\nDistributive Property\nThe outer product is distributive over vector addition. For vectors \\(\\mathbf{u}_1, \\mathbf{u}_2 \\in \\mathbb{R}^m\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\): \\[(\\mathbf{u}_1 + \\mathbf{u}_2) \\otimes \\mathbf{v} = (\\mathbf{u}_1 \\otimes \\mathbf{v}) + (\\mathbf{u}_2 \\otimes \\mathbf{v})\\]\nAssociativity with Scalar Multiplication\nThe outer product is associative with scalar multiplication. For a scalar \\(\\alpha\\) and vectors \\(\\mathbf{u} \\in \\mathbb{R}^m\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\): \\[\\alpha (\\mathbf{u} \\otimes \\mathbf{v}) = (\\alpha \\mathbf{u}) \\otimes \\mathbf{v} = \\mathbf{u} \\otimes (\\alpha \\mathbf{v})\\]\nMatrix Trace\nThe trace of the outer product of two vectors is given by: \\[\\text{tr}(\\mathbf{u} \\otimes \\mathbf{v}) = (\\mathbf{u}^T \\mathbf{v})= (\\mathbf{v}^T \\mathbf{u})\\] Here, \\(\\text{tr}\\) denotes the trace of a matrix, which is the sum of its diagonal elements.\nMatrix Norm\nThe Frobenius norm of the outer product matrix can be expressed in terms of the norms of the original vectors: \\[\\| \\mathbf{u} \\otimes \\mathbf{v} \\|_F = \\| \\mathbf{u} \\|_2 \\cdot \\| \\mathbf{v} \\|_2\\] where \\(\\| \\cdot \\|_2\\) denotes the Euclidean norm.\n\n\n\nExample Calculation in Python\nHere’s how to compute and visualize the outer product properties using Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define vectors\nu = np.array([1, 2, 3])\nv = np.array([4, 5])\n\n# Compute outer product\nouter_product = np.outer(u, v)\n\n# Display results\nprint(\"Outer Product Matrix:\")\nprint(outer_product)\n\n# Compute and display rank\nrank = np.linalg.matrix_rank(outer_product)\nprint(f\"Rank of Outer Product Matrix: {rank}\")\n\n# Compute Frobenius norm\nfrobenius_norm = np.linalg.norm(outer_product, 'fro')\nprint(f\"Frobenius Norm: {frobenius_norm}\")\n\n# Plot the result\nplt.imshow(outer_product, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.title('Outer Product Matrix')\nplt.xlabel('Vector v')\nplt.ylabel('Vector u')\nplt.xticks(ticks=np.arange(len(v)), labels=v)\nplt.yticks(ticks=np.arange(len(u)), labels=u)\nplt.show()\n\nOuter Product Matrix:\n[[ 4  5]\n [ 8 10]\n [12 15]]\nRank of Outer Product Matrix: 1\nFrobenius Norm: 23.958297101421877\n\n\n\n\n\n\n\n\nFigure 2.10: Demonstration of Outer Product and its Properties\n\n\n\n\n\n\n\n2.2.2.7 Kronecker Product\nIn mathematics, the Kronecker product, sometimes denoted by \\(\\otimes\\), is an operation on two matrices of arbitrary size resulting in a block matrix. It is a specialization of the tensor product (which is denoted by the same symbol) from vectors to matrices and gives the matrix of the tensor product linear map with respect to a standard choice of basis. The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is a \\(p \\times q\\) matrix, then the Kronecker product \\(A\\otimes B\\) is the \\(pm \\times qn\\) block matrix defined as: Each \\(a_{ij}\\) of \\(A\\) is replaced by the matrix \\(a_{ij}B\\). Symbolically this will result in a block matrix defined by:\n\\[A\\otimes B=A \\otimes B = \\begin{bmatrix}a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\\vdots & \\vdots & \\ddots & vdots \\\\a_{m1}B & a_{m2}B & \\cdots & a_{mn}B\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nProperties of the Kronecker Product\n\n\n\n\nAssociativity\nThe Kronecker product is associative. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), and \\(C \\in \\mathbb{R}^{r \\times s}\\): \\[(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)\\]\nDistributivity Over Addition\nThe Kronecker product distributes over matrix addition. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), and \\(C \\in \\mathbb{R}^{p \\times q}\\): \\[A \\otimes (B + C) = (A \\otimes B) + (A \\otimes C)\\]\nMixed Product Property\nThe Kronecker product satisfies the mixed product property with the matrix product. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), \\(C \\in \\mathbb{R}^{r \\times s}\\), and \\(D \\in \\mathbb{R}^{r \\times s}\\): \\[(A \\otimes B) (C \\otimes D) = (A C) \\otimes (B D)\\]\nTranspose\nThe transpose of the Kronecker product is given by: \\[(A \\otimes B)^T = A^T \\otimes B^T\\]\nNorm\nThe Frobenius norm of the Kronecker product can be computed as: \\[\\| A \\otimes B \\|_F = \\| A \\|_F \\cdot \\| B \\|_F\\] where \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.\n\n\n\n\n\n\n\n\n\n\nFrobenius Norm\n\n\n\nThe Frobenius norm, also known as the Euclidean norm for matrices, is a measure of a matrix’s magnitude. It is defined as the square root of the sum of the absolute squares of its elements. Mathematically, for a matrix \\(A\\) with elements \\(a_{ij}\\), the Frobenius norm is given by:\n\\[\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\\]\n\n\nExample 1: Calculation of Frobenius Norm\nConsider the matrix \\(A\\):\n\\[A = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\|A\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 4^2}= \\sqrt{1 + 4 + 9 + 16}= \\sqrt{30}\\approx 5.48\\]\nExample 2: Frobenius Norm of a Sparse Matrix\nConsider the sparse matrix \\(B\\):\n\\[B = \\begin{bmatrix}0 & 0 & 0 \\\\0 & 5 & 0 \\\\0 & 0 & 0\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\|B\\|_F = \\sqrt{0^2 + 0^2 + 0^2 + 5^2 + 0^2 + 0^2}= \\sqrt{25}= 5\\]\nExample 3: Frobenius Norm in a Large Matrix\nConsider the matrix \\(C\\) of size $3 $:\n\\[C = \\begin{bmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\begin{align*}\n\\|C\\|_F &= \\sqrt{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 + 7^2 + 8^2 + 9^2}\\\\\n&= \\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81}\\\\\n&= \\sqrt{285}\\\\\n&\\approx 16.88\n\\end{align*}\\]\nApplications of the Frobenius Norm\n\nApplication 1: Image Compression: In image processing, the Frobenius norm can measure the difference between the original and compressed images, indicating how well the compression has preserved the original image quality.\nApplication 2: Matrix Factorization: In numerical analysis, Frobenius norm is used to evaluate the error in matrix approximations, such as in Singular Value Decomposition (SVD). A lower Frobenius norm of the error indicates a better approximation.\nApplication 3: Error Measurement in Numerical Solutions: In solving systems of linear equations, the Frobenius norm can be used to measure the error between the true solution and the computed solution, providing insight into the accuracy of numerical methods.\n\nThe linalg sub module of NumPy library can be used to calculate various norms. Basically norm is the generalized form of Euclidean distance.\n\nimport numpy as np\n\n# Example 1: Simple Matrix\nA = np.array([[1, 2], [3, 4]])\nfrobenius_norm_A = np.linalg.norm(A, 'fro')\nprint(f\"Frobenius Norm of A: {frobenius_norm_A:.2f}\")\n\n# Example 2: Sparse Matrix\nB = np.array([[0, 0, 0], [0, 5, 0], [0, 0, 0]])\nfrobenius_norm_B = np.linalg.norm(B, 'fro')\nprint(f\"Frobenius Norm of B: {frobenius_norm_B:.2f}\")\n\n# Example 3: Large Matrix\nC = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nfrobenius_norm_C = np.linalg.norm(C, 'fro')\nprint(f\"Frobenius Norm of C: {frobenius_norm_C:.2f}\")\n\nFrobenius Norm of A: 5.48\nFrobenius Norm of B: 5.00\nFrobenius Norm of C: 16.88\n\n\nFrobenius norm of Kronecker product\nLet us consider two matrices,\n\\[A = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nand\n\\[B = \\begin{bmatrix}0 & 5 \\\\6 & 7\\end{bmatrix}\\]\nThe Kronecker product \\(C = A \\otimes B\\) is:\n\\[C = \\begin{bmatrix}1 \\cdot B & 2 \\cdot B \\\\3 \\cdot B & 4 \\cdot B\\end{bmatrix}= \\begin{bmatrix}\\begin{bmatrix}0 & 5 \\\\6 & 7\\end{bmatrix} & \\begin{bmatrix}0 \\cdot 2 & 5 \\cdot 2 \\\\6 \\cdot 2 & 7 \\cdot 2\\end{bmatrix} \\\\\\begin{bmatrix}0 \\cdot 3 & 5 \\cdot 3 \\\\6 \\cdot 3 & 7 \\cdot 3\\end{bmatrix} & \\begin{bmatrix}0 \\cdot 4 & 5 \\cdot 4 \\\\6 \\cdot 4 & 7 \\cdot 4\\end{bmatrix}\\end{bmatrix}\\]\nThis expands to:\n\\[C = \\begin{bmatrix}0 & 5 & 0 & 10 \\\\6 & 7 & 12 & 14 \\\\0 & 15 & 0 & 20 \\\\18 & 21 & 24 & 28\\end{bmatrix}\\]\nComputing the Frobenius Norm\nTo compute the Frobenius norm of \\(C\\):\n\\[\\|C\\|_F = \\sqrt{\\sum_{i=1}^{4} \\sum_{j=1}^{4} |c_{ij}|^2}\\]\n\\[\\|C\\|_F = \\sqrt{0^2 + 5^2 + 0^2 + 10^2 + 6^2 + 7^2 + 12^2 + 14^2 + 0^2 + 15^2 + 0^2 + 20^2 + 18^2 + 21^2 + 24^2 + 28^2}\\]\n\\[\\|C\\|_F = \\sqrt{0 + 25 + 0 + 100 + 36 + 49 + 144 + 196 + 0 + 225 + 0 + 400 + 324 + 441 + 576 + 784}\\]\n\\[\\|C\\|_F = \\sqrt{2896}\\] \\[\\|C\\|_F \\approx 53.87\\]\n\n\n\n2.2.2.8 Practice Problems\nFind the Kronecker product of A and B where A and B are given as follows:\nProblem 1:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\\\3&4\\end{bmatrix} \\otimes \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 4 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 1 & 0 & 2 \\\\\n1 & 0 & 2& 0\\\\\n0 & 3 & 0 & 4 \\\\\n3 & 0 & 4 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 2:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\otimes \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} & 0 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 & 3 & 0 & 0 \\\\\n4 & 5 & 0 & 0 \\\\\n0 & 0 & 2 & 3 \\\\\n0 & 0 & 4 & 5\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 3:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\end{bmatrix} \\otimes \\begin{bmatrix}3\\\\4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}3\\\\4\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}3\\\\4\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 6 \\\\\n4 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 4:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 &1&-1\\\\\n0 & 0&2&0 \\\\\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 5:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}4&-2\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n8 & -4 \\\\\n12 & -6\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 6:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&-1\\\\0&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&-1\\\\0&2\\end{bmatrix} \\otimes \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & -1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 1 & 0 & -1 \\\\\n1 & 0 & -1 & 0 \\\\\n0 & 0 & 0 & 2 \\\\\n0 & 0 & 2 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 7:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&4\\\\5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\end{bmatrix} \\otimes \\begin{bmatrix}3&4\\\\5&6\\end{bmatrix} \\\\\n&= 2 \\cdot \\begin{bmatrix}3&4\\\\5&6\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 8:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 9:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} & 0 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 10:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2&-1\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2&-1\\\\3&4\\end{bmatrix} \\otimes \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} & -1 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} & 4 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 10 & 0 & -5 \\\\\n-4 & 6 & 2 & -3 \\\\\n0 & 15 & 0 & 20 \\\\\n-6 & 9 & -8 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.2.2.9 Connection Between Outer Product and Kronecker Product\n\nConceptual Connection:\n\nThe outer product is a special case of the Kronecker product. Specifically, if \\(\\mathbf{A}\\) is a column vector and \\(\\mathbf{B}\\) is a row vector, then \\(\\mathbf{A}\\) is a \\(m \\times 1\\) matrix and \\(\\mathbf{B}\\) is a \\(1 \\times n\\) matrix. The Kronecker product of these two matrices will yield the same result as the outer product of these vectors.\nFor matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), the Kronecker product involves taking the outer product of each element of \\(\\mathbf{A}\\) with the entire matrix \\(\\mathbf{B}\\).\n\nMathematical Formulation:\n\nLet \\(\\mathbf{A} = \\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{bmatrix}\\) and \\(\\mathbf{B} = \\begin{bmatrix}b_{11} & b_{12}\\\\ b_{21} & b_{22}\\end{bmatrix}\\). Then:\n\n\\[\\mathbf{A} \\otimes \\mathbf{B} = \\begin{bmatrix} a_{11} \\mathbf{B} & a_{12} \\mathbf{B} \\\\ a_{21} \\mathbf{B} & a_{22} \\mathbf{B} \\end{bmatrix}\\]\n\nIf \\(\\mathbf{A} = \\mathbf{u} \\mathbf{v}^T\\) where \\(\\mathbf{u}\\) is a column vector and \\(\\mathbf{v}^T\\) is a row vector, then the Kronecker product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}^T\\) yields the same result as the outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummary\n\nThe outer product is a specific case of the Kronecker product where one of the matrices is a vector (either row or column).\nThe Kronecker product generalizes the outer product to matrices and is more versatile in applications involving tensor products and higher-dimensional constructs.\n\n\n\n\n\n2.2.2.10 Matrix Multiplication as Kronecker Product\nGiven matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), where: - \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix - \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix\nThe product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) can be expressed using Kronecker products as:\n\\[\\mathbf{C} = \\sum_{k=1}^n (\\mathbf{A}_{:,k} \\otimes \\mathbf{B}_{k,:})\\]\nwhere: - \\(\\mathbf{A}_{:,k}\\) denotes the \\(k\\)-th column of matrix \\(\\mathbf{A}\\) - \\(\\mathbf{B}_{k,:}\\) denotes the \\(k\\)-th row of matrix \\(\\mathbf{B}\\)\nExample:\nLet:\n\\[\\mathbf{A} = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nand:\n\\[\\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\1 & 0\\end{bmatrix}\\]\nTo find \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) using Kronecker products:\n\nCompute the Kronecker Product of Columns of \\(\\mathbf{A}\\) and Rows of \\(\\mathbf{B}\\):\n\nFor column \\(\\mathbf{A}_{:,1} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\\) and row \\(\\mathbf{B}_{1,:} = \\begin{bmatrix} 0 & 1 \\end{bmatrix}\\): \\[\\mathbf{A}_{:,1} \\otimes \\mathbf{B}_{1,:} = \\begin{bmatrix}     0 & 1 \\\\     0 & 3     \\end{bmatrix}\\]\nFor column \\(\\mathbf{A}_{:,2} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) and row \\(\\mathbf{B}_{2,:} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\\): \\[\\mathbf{A}_{:,2} \\otimes \\mathbf{B}_{2,:} = \\begin{bmatrix}2 & 0 \\\\ 4 & 0\\end{bmatrix}\\]\n\nSum the Kronecker Products:\n\\[\\mathbf{C} = \\begin{bmatrix}0 & 1 \\\\ 0 & 3\\end{bmatrix} +\\begin{bmatrix} 2 & 0 \\\\ 4 & 0 \\end{bmatrix}  = \\begin{bmatrix} 2 & 1 \\\\ 4 & 3\\end{bmatrix}\\]\n\n\nIn the previous block we have discussed the Frobenius norm and its applications. Now came back to the discussions on the Kronecker product. The Kronecker product is particularly useful in scenarios where interactions between different types of data need to be modeled comprehensively. In recommendation systems, it allows us to integrate user preferences with item relationships to improve recommendation accuracy.\nIn addition to recommendation systems, Kronecker products are used in various fields such as:\n\nSignal Processing: For modeling multi-dimensional signals.\nMachine Learning: In building features for complex models.\nCommunication Systems: For modeling network interactions.\n\nBy understanding the Kronecker product and its applications, we can extend it to solve complex problems and enhance systems across different domains. To understand the practical use of Kronecker product in a Machine Learning scenario let us consider the following problem statement and its solution.\n\n\n\n\n\n\nProblem statement\n\n\n\nIn the realm of recommendation systems, predicting user preferences for various product categories based on past interactions is a common challenge. Suppose we have data on user preferences for different products and categories. We can use this data to recommend the best products for each user by employing mathematical tools such as the Kronecker product. The User Preference and Category relationships are given in Table 2.4 and Table 2.5 .\n\n\n\nTable 2.4: User Preference\n\n\n\n\n\nUser/Item\nElectronics\nClothing\nBooks\n\n\n\n\nUser 1\n5\n3\n4\n\n\nUser 2\n2\n4\n5\n\n\nUser 3\n3\n4\n4\n\n\n\n\n\n\n\n\n\nTable 2.5: Category Relationships\n\n\n\n\n\nCategory/Feature\nFeature 1\nFeature 2\nFeature 3\n\n\n\n\nElectronics\n1\n0\n0\n\n\nClothing\n0\n1\n1\n\n\nBooks\n0\n1\n1\n\n\n\n\n\n\nPredict user preferences for different product categories using the Kronecker product matrix.\n\n\n\nSolution Procedure\n\n\nCompute the Kronecker Product: Calculate the Kronecker product of matrices \\(U\\) and \\(C\\) to obtain matrix \\(K\\).\nTo model the problem, we use the Kronecker product of the user preference matrix \\(U\\) and the category relationships matrix \\(C\\). This product allows us to predict the user’s rating for each category by combining their preferences with the category features.\n\nFormulating Matrices\nUser Preference Matrix (U): - Dimension: \\(3\\times 3\\) (3 users, 3 items) - from the User preference data, we can create the User Preference Matrix as follows:\n\\[U = \\begin{pmatrix}5 & 3 & 4 \\\\2 & 4 & 5 \\\\3 & 4 & 4 \\end{pmatrix}\\]\nCategory Relationships Matrix (C): - Dimension: \\(3 \\times 3\\) (3 categories) - from the Category Relationships data, we can create the Category Relationship Matrix as follows:\n\\[C = \\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1\\end{pmatrix}\\]\nKronecker Product Calculation\nThe Kronecker product \\(K\\) of \\(U\\) and \\(C\\) is calculated as follows:\n\nMatrix Dimensions:\n\n\n\\(U\\) is \\(3 \\times 3\\) (3 users, 3 items).\n\\(C\\) is \\(3 \\times 3\\) (3 categories, 3 features).\n\n\nCalculate Kronecker Product:\n\n\nFor each element \\(u_{ij}\\) in \\(U\\), multiply by the entire matrix \\(C\\).\n\nThe Kronecker product \\(K\\) is computed as:\n\\[K = U \\otimes C\\]\nExplicitly, the Kronecker product \\(K\\) is:\n\\[K = \\begin{pmatrix}5 \\cdot C & 3 \\cdot C & 4 \\cdot C \\\\ 2 \\cdot C & 4 \\cdot C & 5 \\cdot C \\\\    3 \\cdot C & 4 \\cdot C & 4 \\cdot C\\end{pmatrix}\\]\nAs an example the blocks in first row are:\n\\[5 \\cdot C = \\begin{pmatrix}   5 & 0 & 0 \\\\   0 & 5 & 5 \\\\   0 & 5 & 5   \\end{pmatrix}, \\quad    3 \\cdot C = \\begin{pmatrix}   3 & 0 & 0 \\\\   0 & 3 & 3 \\\\   0 & 3 & 3   \\end{pmatrix}, \\quad   4 \\cdot C = \\begin{pmatrix}   4 & 0 & 0 \\\\   0 & 4 & 4 \\\\   0 & 4 & 4   \\end{pmatrix}\\]\nCombining these blocks:\n\\[K = \\begin{pmatrix}   5 & 0 & 0 & 3 & 0 & 0 & 4 & 0 & 0\\\\   0 & 5 & 5 & 0 & 3 & 3 & 0 & 4 & 4\\\\   0 & 5 & 5 & 0 & 3 & 3 & 0 & 4 & 4\\\\   2 & 0 & 0 & 4 & 0 & 0 & 5 & 0 & 0\\\\   0 & 2 & 2 & 0 & 4 & 4 & 0 & 5 & 5\\\\   0 & 2 & 2 & 0 & 4 & 4 & 0 & 5 & 5\\\\   3 & 0 & 0 & 4 & 0 & 0 & 4 & 0 & 0\\\\   0 & 3 & 3 & 0 & 4 & 4 & 0 & 4 & 4\\\\   0 & 3 & 3 & 0 & 4 & 4 & 0 & 4 & 4\\end{pmatrix}\\]\n\nInterpret the Kronecker Product Matrix: The resulting matrix \\(K\\) represents all possible combinations of user preferences and category features.\nPredict Ratings: For each user, use matrix \\(K\\) to predict the rating for each category by summing up the values in the corresponding rows.\nGenerate Recommendations: Identify the top categories with the highest predicted ratings for each user.\n\nThe python code to solve this problem computationally is given below.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define the matrices\nU = np.array([[5, 3, 4],\n              [2, 4, 5],\n              [3, 4, 4]])\n\nC = np.array([[1, 0, 0],\n              [0, 1, 1],\n              [0, 1, 1]])\n\n# Compute the Kronecker product\nK = np.kron(U, C)\n\n# Create a DataFrame to visualize the Kronecker product matrix\ndf_K = pd.DataFrame(K, \n                    columns=['Electronics_F1', 'Electronics_F2', 'Electronics_F3', \n                             'Clothing_F1', 'Clothing_F2', 'Clothing_F3', \n                             'Books_F1', 'Books_F2', 'Books_F3'],\n                    index=['User 1 Electronics', 'User 1 Clothing', 'User 1 Books', \n                           'User 2 Electronics', 'User 2 Clothing', 'User 2 Books', \n                           'User 3 Electronics', 'User 3 Clothing', 'User 3 Books'])\n\n# Print the Kronecker product matrix\nprint(\"Kronecker Product Matrix (K):\\n\", df_K)\n\n# Predict ratings and create recommendations\ndef recommend(user_index, top_n=3):\n    \"\"\" Recommend top_n categories for a given user based on Kronecker product matrix. \"\"\"\n    user_ratings = K[user_index * len(C):(user_index + 1) * len(C), :]\n    predicted_ratings = np.sum(user_ratings, axis=0)\n    recommendations = np.argsort(predicted_ratings)[::-1][:top_n]\n    return recommendations\n\n# Recommendations for User 1\nuser_index = 0  # User 1\ntop_n = 3\nrecommendations = recommend(user_index, top_n)\n\nprint(f\"\\nTop {top_n} recommendations for User {user_index + 1}:\")\nfor rec in recommendations:\n    print(df_K.columns[rec])\n\nKronecker Product Matrix (K):\n                     Electronics_F1  Electronics_F2  Electronics_F3  \\\nUser 1 Electronics               5               0               0   \nUser 1 Clothing                  0               5               5   \nUser 1 Books                     0               5               5   \nUser 2 Electronics               2               0               0   \nUser 2 Clothing                  0               2               2   \nUser 2 Books                     0               2               2   \nUser 3 Electronics               3               0               0   \nUser 3 Clothing                  0               3               3   \nUser 3 Books                     0               3               3   \n\n                    Clothing_F1  Clothing_F2  Clothing_F3  Books_F1  Books_F2  \\\nUser 1 Electronics            3            0            0         4         0   \nUser 1 Clothing               0            3            3         0         4   \nUser 1 Books                  0            3            3         0         4   \nUser 2 Electronics            4            0            0         5         0   \nUser 2 Clothing               0            4            4         0         5   \nUser 2 Books                  0            4            4         0         5   \nUser 3 Electronics            4            0            0         4         0   \nUser 3 Clothing               0            4            4         0         4   \nUser 3 Books                  0            4            4         0         4   \n\n                    Books_F3  \nUser 1 Electronics         0  \nUser 1 Clothing            4  \nUser 1 Books               4  \nUser 2 Electronics         0  \nUser 2 Clothing            5  \nUser 2 Books               5  \nUser 3 Electronics         0  \nUser 3 Clothing            4  \nUser 3 Books               4  \n\nTop 3 recommendations for User 1:\nElectronics_F2\nElectronics_F3\nBooks_F3\n\n\nA simple visualization of this recomendation system is shown in Fig 2.11.\n\n# Visualization\ndef plot_recommendations(user_index):\n    \"\"\" Plot the predicted ratings for each category for a given user. \"\"\"\n    user_ratings = K[user_index * len(C):(user_index + 1) * len(C), :]\n    predicted_ratings = np.sum(user_ratings, axis=0)\n    categories = df_K.columns\n    plt.figure(figsize=(6, 5))\n    plt.bar(categories, predicted_ratings)\n    plt.xlabel('Categories')\n    plt.ylabel('Predicted Ratings')\n    plt.title(f'Predicted Ratings for User {user_index + 1}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Plot recommendations for User 1\nplot_recommendations(user_index)\n\n\n\n\n\n\n\nFigure 2.11: EDA for the Recommendation System\n\n\n\n\n\nThis micro project illustrate one of the popular use of Kronecker product on ML application.\n\n\n\n2.2.3 Matrix Measures of Practical Importance\nMatrix measures, such as rank and determinant, play crucial roles in linear algebra. While both rank and determinant provide valuable insights into the properties of a matrix, they serve different purposes. Understanding their roles and applications is essential for solving complex problems in computer science, engineering, and applied mathematics.\n\n2.2.3.1 Determinant\nDeterminant of a \\(2\\times 2\\) matrix \\(A=\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\\) is defined as \\(|A|=ad-bc\\). Determinant of higher order square matrices can be found using the Laplace method or Sarrus method.\nThe determinant of a matrix provides information about the matrix’s invertibility and scaling factor for volume transformation. Specifically:\n\nInvertibility: A matrix is invertible if and only if its determinant is non-zero.\nVolume Scaling: The absolute value of the determinant gives the scaling factor by which the matrix transforms volume.\nParallelism: If the determinant of a matrix composed of vectors is zero, the vectors are linearly dependent, meaning they are parallel or redundant.\nRedundancy: A zero determinant indicates that the vectors span a space of lower dimension than the number of vectors, showing redundancy.\n\n\n\n\n\n\n\nLeast Possible Values of Determinant\n\n\n\n\nLeast Positive Determinant: For a \\(1\\times 1\\) matrix, the smallest non-zero determinant is any positive value, typically \\(\\epsilon\\), where \\(\\epsilon\\) is a small positive number.\nLeast Non-Zero Determinant: For higher-dimensional matrices, the smallest non-zero determinant is a non-zero value that represents the smallest area or volume spanned by the matrix’s rows or columns. For example a \\(2\\times 2\\) matrix with determinant \\(\\epsilon\\) could be: \\[B=\\begin{pmatrix}\\epsilon&0\\\\ 0&\\epsilon\\end{pmatrix}\\] Here, \\(\\epsilon\\) is a small positive number, indicating a very small but non-zero area.\n\n\n\nNow let’s look into the most important matrix measure for advanced application in Linear Algebra.\nAs we know the matrix is basically a representation tool that make things abstract- remove unnecessary details. Then the matrix itself can be represented in many ways. This is the real story telling with this most promising mathematical structure. Consider a context of collecting feedback about a product in three aspects- cost, quality and practicality. For simplicity in calculation, we consider responses from 3 customers only. The data is shown in Table 2.6.\n\n\n\nTable 2.6: User rating of a consumer product\n\n\n\n\n\nUser\nCost\nQuality\nPracticality\n\n\n\n\nUser-1\n1\n4\n5\n\n\nUser-2\n3\n2\n5\n\n\nUser-3\n2\n1\n3\n\n\n\n\n\n\nIt’s perfect and nice looking. But both mathematics and a computer can’t handle this table as it is. So we create an abstract representation of this data- the rating matrix. Using the traditional approach, let’s represent this rating data as: \\[A=\\begin{bmatrix}1&4&5\\\\3&2&5\\\\2&1&3\\end{bmatrix}\\]\nNow both the column names and row indices were removed and the data is transformed into the abstract form. This representation has both advantages and disadvantages. Be positive! So we are focused only in the advantages.\nJust consider the product. Its sales fully based on its features. So the product sales perspective will be represented in terms of the features- cost, quality and practicality. These features are columns of our rating matrix. Definitly people will have different rating for these features. Keeping all these in mind let’s introduce the concept of linear combination. This leads to a new matrix product as shown below.\n\\[\\begin{align*}\nAx&=\\begin{bmatrix}\n1&4&5\\\\\n3&2&5\\\\\n2&1&3\n\\end{bmatrix}x\\\\\n&=\\begin{bmatrix}\n1&4&5\\\\\n3&2&5\\\\\n2&1&3\n\\end{bmatrix}\\cdot\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1\\\\3\\\\2\\end{bmatrix}x_1+\\begin{bmatrix}4\\\\2\\\\1\\end{bmatrix}x_2+\\begin{bmatrix}5\\\\5\\\\3\\end{bmatrix}x_3\n\\end{align*}\\]\nAs the number of users increases, the product sales perspective become more informative. In short the span of the features define the feature space of the product. In real cases, a manufacture wants to know what are the features really inflence the customers. This new matrix product will help the manufactures to identify that features!\nSo we are going to define this new matrix product as the feature space, that will provide more insights to this context as:\n\\[A=CR\\]\nWhere \\(C\\) is the column space and \\(R\\) is the row reduced Echelon form of \\(A\\). But the product is not the usual scalar projection, Instead the weight of linear combination of elements in the column space.\nLet’s formally illustrate this in our example. From the first observation itself, it is clear that last column is just the sum of first and second columns (That is in our context the feature ‘practicality’ is just depends on ‘cost’ and ‘quality’. meaningful?). So only first columns are independent and so spans the column space.\n\\[C=\\begin{bmatrix}1&4\\\\3&2\\\\2&1\\end{bmatrix}\\]\nNow look into the matrix \\(R\\). Applying elementary row tansformations, \\(A\\) will transformed into:\n\\[R=\\begin{bmatrix}1&0&1\\\\0&1&1\\\\0&0&0\\end{bmatrix}\\]\nHence we can form a decomposition for the given rating matrix, \\(A\\) as: \\[\\begin{align*}\nA&=CR\\\\\n&=\\begin{bmatrix}1&4\\\\3&2\\\\2&1\\end{bmatrix}\\begin{bmatrix}1&0&1\\\\0&1&1\\\\\\mbox{}&&\\end{bmatrix}\n\\end{align*}\\]\nThis decomposition says that there are only two independent features (columns) and the third feature (column) is the sum of first two features (columns).\n\n\n\n\n\n\nInterpretation of the \\(R\\) matrix\n\n\n\nEach column in the \\(R\\) matrix represents the weights for linear combination of vectors in the column space to get that column in \\(A\\). In this example, third column of \\(R\\) is \\(\\begin{bmatrix}1\\\\1\\end{bmatrix}\\). This means that third column of \\(A\\) will be \\(1\\times C_1+1\\times C_2\\) of the column space, \\(C\\)!\n\n\nThis first matrix decompostion donate a new type of matrix product (outer product) and a new measure- the number of independent columns and number of independent rows. This count is called the rank of the matrix \\(A\\). In the case of features, if the rank of the column space is less than the number of features then definitly a less number of feature set will perfectly represent the data. This will help us to reduce the dimension of the dataset and there by reducing computational complexities in data analysis and machine Learning jobs.\nIn the above discussion, we consider only the columns of \\(A\\). Now we will mention the row space. It is the set of all linearly independent rows of \\(A\\). For any matrix \\(A\\), both the row space and column space are of same rank. This correspondance is a helpful result in many practical applications.\nNow we consider a stable equation, \\(Ax=0\\). With the usual notation of dot product, it implies that \\(x\\) is orthogonal to \\(A\\). Set of all those independent vectors which are orthogonal to \\(A\\) constitute a new space of interest. It is called the null space of \\(A\\). If \\(A\\) represents a linear transformation, then the null space will be populated by those non-zero vectors which are nullified by the transformation \\(A\\). As a summary of this discussion, the row space and null space of a matrix \\(A\\) creates an orthogonal system. Considering the relationship between \\(A\\) and \\(A^T\\), it is clear that row space of \\(A\\) is same as the column space of \\(A^T\\) and vice verse are. So we can restate the orthogonality as: ‘the null space of \\(A\\) is orthogonal to the column space of \\(A^T\\)’ and ‘the null space of \\(A^T\\) is orthogonal to the column space of \\(A\\)’. Mathematically this property can be represents as follows.\n\n\n\n\n\n\nNote\n\n\n\n\\[\\begin{align*}\n\\mathcal{N}(A)&\\perp \\mathcal{C}(A^T)\\\\\n\\mathcal{N}(A^T)&\\perp \\mathcal{C}(A)\n\\end{align*}\\]\n\n\nIn the given example, solving \\(Ax=0\\) we get \\(x=\\begin{bmatrix}1&1&-1\\end{bmatrix}^T\\).\nSo the rank of \\(\\mathcal{N}(A)=1\\). Already we have rank of \\(A=2\\). This leads to an interesting result:\n\\[\\text{Rank}(A)+\\text{Rank}(\\mathcal{N}(A))=3\\]\nThis observation can be framed as a theorem.\n\n\n\n2.2.4 Rank Nullity Theorem\nThe rank-nullity theorem is a fundamental theorem in linear algebra that is important for understanding the connections between mathematical operations in engineering, physics, and computer science. It states that the sum of the rank and nullity of a matrix equals the number of columns in the matrix. The rank is the maximum number of linearly independent columns, and the nullity is the dimension of the nullspace.\n\nTheorem 2.1 (Rank Nullitty Theorem) The Rank-Nullity Theorem states that for any \\(m \\times n\\) matrix \\(A\\), the following relationship holds:\n\\[\n\\text{Rank}(A) + \\text{Nullity}(A) = n\n\\]\nwhere: - Rank of \\(A\\) is the dimension of the column space of \\(A\\), which is also equal to the dimension of the row space of \\(A\\). - Nullity of \\(A\\) is the dimension of the null space of \\(A\\), which is the solution space to the homogeneous system \\(A \\mathbf{x} = \\mathbf{0}\\).\n\nSteps to Formulate for Matrix \\(A\\)\n\nFind the Rank of \\(A\\): The rank of a matrix is the maximum number of linearly independent columns (or rows). It can be determined by transforming \\(A\\) into its row echelon form or reduced row echelon form (RREF).\nFind the Nullity of \\(A\\): The nullity is the dimension of the solution space of \\(A \\mathbf{x} = \\mathbf{0}\\). This can be found by solving the homogeneous system and counting the number of free variables.\nApply the Rank-Nullity Theorem: Use the rank-nullity theorem to verify the relationship.\n\n\nExample 1: Calculate the rank and nullity of \\(A=\\begin{bmatrix}   1 & 4 & 5 \\\\   3 & 2 & 5 \\\\   2 & 1 & 3   \\end{bmatrix}\\) and verify the rank nullity theorem.\n\nRow Echelon Form:\nPerform Gaussian elimination on \\(A\\):\n\\[A = \\begin{bmatrix} 1 & 4 & 5 \\\\  3 & 2 & 5 \\\\   2 & 1 & 3   \\end{bmatrix}\\]\nPerform row operations to get it to row echelon form:\n\nSubtract 3 times row 1 from row 2: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     2 & 1 & 3     \\end{bmatrix}\\]\nSubtract 2 times row 1 from row 3: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     0 & -7 & -7     \\end{bmatrix}\\]\nAdd \\(\\frac{7}{10}\\) times row 2 to row 3: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     0 & 0 & 0     \\end{bmatrix}\\]\n\nThe matrix is now in row echelon form.\nRank is the number of non-zero rows, which is 2.\nFind the Nullity: The matrix \\(A\\) has 3 columns. The number of free variables in the solution of \\(A \\mathbf{x} = \\mathbf{0}\\) is \\(3 - \\text{Rank}\\).\nSo, \\[\\text{Nullity}(A) = 3 - 2 = 1\\]\nApply the Rank-Nullity Theorem: \\[\\text{Rank}(A) + \\text{Nullity}(A) = 2 + 1 = 3\\]\nThis matches the number of columns of \\(A\\), confirming the theorem.\n\n\n\n2.2.5 Fundamental Subspaces\nIn section (note-ortho?), we have seen that for any matrix \\(A\\), there is two pairs of inter-related orthogonal spaces. This leads to the concept of Fundamental sup spaces.\nMatrices are not just arrays of numbers; they can represent linear transformations too. A linear transformation maps vectors from one vector space to another while preserving vector addition and scalar multiplication. The matrix \\(A\\) can be viewed as a representation of a linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) where:\n\\[T(\\mathbf{x}) = A \\mathbf{x}\\]\nIn this context:\n\nThe column space of \\(A\\) represents the range of \\(T\\), which is the set of all possible outputs.\nThe null space of \\(A\\) represents the kernel of \\(T\\), which is the set of vectors that are mapped to the zero vector.\n\nThe Four Fundamental Subspaces\nUnderstanding the four fundamental subspaces helps in analyzing the properties of a linear transformation. These subspaces are:\n\nDefinition 2.1 (Four Fundamental Subspaces) Let \\(T:\\mathbb{R^n}\\longrightarrow \\mathbb{R^m}\\) be a linear transformation and \\(A\\) represents the matrix of transformation. The four fundamental subspaces are defined as:\n\nColumn Space (Range): The set of all possible outputs of the transformation. For matrix \\(A\\), this is the span of its columns. It represents the image of \\(\\mathbb{R}^n\\) under \\(T\\).\nNull Space (Kernel): The set of all vectors that are mapped to the zero vector by the transformation. For matrix \\(A\\), this is the solution space of \\(A \\mathbf{x} = \\mathbf{0}\\).\nRow Space: The span of the rows of \\(A\\). This space is crucial because it helps in understanding the rank of \\(A\\). The dimension of the row space is equal to the rank of \\(A\\), which represents the maximum number of linearly independent rows.\nLeft Null Space: The set of all vectors \\(\\mathbf{y}\\) such that \\(A^T \\mathbf{y} = \\mathbf{0}\\). It provides insight into the orthogonal complement of the row space.\n\n\n\n\n\n\n\ngraph TD\n    MatrixA[Matrix A] --&gt;|Contains Columns| ColumnSpace[Column Space]\n    MatrixA --&gt;|Contains Rows| RowSpace[Row Space]\n    MatrixA --&gt;|Contains Vectors Mapping to Zero| NullSpace[Null Space]\n    MatrixA --&gt;|Contains Vectors Orthogonal to Row Space| LeftNullSpace[Left Null Space]\n    ColumnSpace --&gt;|Orthogonal Complement| NullSpace\n    RowSpace --&gt;|Orthogonal Complement| LeftNullSpace\n\n\n\n\n\n\nThis idea is depicted as a ‘Big picture of the four sub spaces of a matrix’ in the Strang’s text book on Linear algebra for every one (Strang 2020). This ‘Big Picture’ is shown in Fig- 2.12.\n\n\n\n\n\n\nFigure 2.12: The Big Pictue of Fundamental Subspaces\n\n\n\nA video session from Strang’s session is here:\n\n\n2.2.5.1 Practice Problems\nProblem 1: Express the vector \\((1,-2,5)\\) as a linear combination of the vectors \\((1,1,1)\\), \\((1,2,3)\\) and \\((2,-1,1)\\).\nProblem 2: Show that the feature vector \\((2,-5,3)\\) is not linearly associated with the features \\((1,-3,2)\\), \\((2,-4,-1)\\) and \\((1,-5,7)\\).\nProblem 3: Show that the feature vectors \\((1,1,1)\\), \\((1,2,3)\\) and \\((2,-1,1)\\) are non-redundant.\nProblem 4: Prove that the features \\((1,-1,1)\\), \\((0,1,2)\\) and \\((3,0,-1)\\) form basis for the feature space.\nProblem 5: Check whether the vectors \\((1,2,1)\\), \\((2,1,4)\\) and \\((4,5,6)\\) form a basis for \\(\\mathbb{R}^3\\).\nProblem 6: Find the four fundamental subspaces of the feature space created by \\((1,2,1)\\), \\((2,1,4)\\) and \\((4,5,6)\\).\nProblem 7: Find the four fundamental subspaces and its dimensions of the matrix \\(\\begin{bmatrix}1&2&4\\\\2&1&5\\\\1&4&6\\end{bmatrix}\\).\nProblem 8: Express \\(A=\\begin{bmatrix}1&2&-1\\\\3&1&-1\\\\2&-1&0\\end{bmatrix}\\) as the Kronecker product of the column space and the row space in the form \\(A=C\\otimes R\\).\nProblem 9: Find the four fundamental subspaces of \\(A=\\begin{bmatrix} 1&2&0&2&5\\\\-2&-5&1&-1&-8\\\\0&-3&3&4&1\\\\3&6&0&-7&2\\end{bmatrix}\\).\nProblem 10: Find the four fundamental subspaces of \\(A=\\begin{bmatrix}-1&2&-1&5&6\\\\4&-4&-4&-12&-8\\\\2&0&-6&-2&4\\\\-3&1&7&-2&12\\end{bmatrix}\\).\nProblem 11: Express \\(A=\\begin{bmatrix}2&3&-1&-1\\\\1&-1&-2&-4\\\\3&1&3&-2\\\\6&3&0&-7\\end{bmatrix}\\) in \\(A=C\\otimes R\\), where \\(C\\) is the column space and \\(R\\) is the row space of \\(A\\).\nProblem 12: Express \\(A=\\begin{bmatrix}0&1&-3&-1\\\\1&0&1&1\\\\3&1&0&2\\\\1&1&-2&0\\end{bmatrix}\\) in \\(A=C\\otimes R\\), where \\(C\\) is the column space and \\(R\\) is the row space of \\(A\\).\nProblem 13: Show that the feature vectors \\((2,3,0)\\), \\((1,2,0)\\) and \\((8,13,0)\\) are redundant and hence find the relationship between them.\nProblem 14: Show that the feature vectors \\((1,2,1)\\), \\((4,1,2)\\), \\((-3,8,1)\\) and \\((6,5,4)\\) are redundant and hence find the relationship between them.\nProblem 15: Show that the feature vectors \\((1,2,-1,0)\\), \\((1,3,1,2)\\), \\((4,2,1,0)\\) and \\((6,1,0,1)\\) are redundant and hence find the relationship between them.\n\n\n\n\n\n\nImportant\n\n\n\nThree Parts of the Fundamental theorem The fundamental theorem of linear algebra relates all four of the fundamental subspaces in a number of different ways. There are main parts to the theorem:\nPart 1:(Rank nullity theorem) The column and row spaces of an \\(m\\times n\\) matrix \\(A\\) both have dimension \\(r\\), the rank of the matrix. The nullspace has dimension \\(n−r\\), and the left nullspace has dimension \\(m−r\\).\nPart 2:(Orthogonal subspaces) The nullspace and row space are orthogonal. The left nullspace and the column space are also orthogonal.\nPart 3:(Matrix decomposition) The final part of the fundamental theorem of linear algebra constructs an orthonormal basis, and demonstrates a singular value decomposition: any matrix \\(M\\) can be written in the form \\(M=U\\Sigma V^T\\) , where \\(U_{m\\times m}\\) and \\(V_{n\\times n}\\) are unitary matrices, \\(\\Sigma_{m\\times n}\\) matrix with nonnegative values on the diagonal.\nThis part of the fundamental theorem allows one to immediately find a basis of the subspace in question. This can be summarized in the following table.\n\n\n\n\n\n\n\n\n\n\nSubspace\nSubspace of\nSymbol\nDimension\nBasis\n\n\n\n\nColumn space\n\\(\\mathbb{R}^m\\)\n\\(\\operatorname{im}(A)\\)\n\\(r\\)\nFirst \\(r\\) columns of \\(U\\)\n\n\nNullspace (kernel)\n\\(\\mathbb{R}^n\\)\n\\(\\ker(A)\\)\n\\(n - r\\)\nLast \\(n - r\\) columns of \\(V\\)\n\n\nRow space\n\\(\\mathbb{R}^n\\)\n\\(\\operatorname{im}(A^T)\\)\n\\(r\\)\nFirst \\(r\\) columns of \\(V\\)\n\n\nLeft nullspace (kernel)\n\\(\\mathbb{R}^m\\)\n\\(\\ker(A^T)\\)\n\\(m - r\\)\nLast \\(m - r\\) columns of \\(U\\)\n\n\n\n\n\n\n\n2.2.5.2 Computational methods to find all the four fundamental subspaces of a matrix\nThere are different approaches to find the four fundamental subspaces of a matrix using Python. Simplest method is just convert our mathematical procedure into Python functions and call them to find respective spaces. This method is illustrated below.\n\n# importing numpy library for numerical computation\nimport numpy as np\n# define the function create the row-reduced Echelon form of given matrix\ndef row_echelon_form(A):\n    \"\"\"Convert matrix A to its row echelon form.\"\"\"\n    A = A.astype(float)\n    rows, cols = A.shape\n    for i in range(min(rows, cols)):\n        # Pivot: find the maximum element in the current column\n        max_row = np.argmax(np.abs(A[i:, i])) + i\n        if A[max_row, i] == 0:\n            continue  # Skip if the column is zero\n        # Swap the current row with the max_row\n        A[[i, max_row]] = A[[max_row, i]]\n        # Eliminate entries below the pivot\n        for j in range(i + 1, rows):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n    return A\n\n# define function to generate null space from the row-reduced echelon form\ndef null_space_of_matrix(A, rtol=1e-5):\n    \"\"\"Compute the null space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    rows, cols = A_reduced.shape\n    # Identify pivot columns\n    pivots = []\n    for i in range(rows):\n        for j in range(cols):\n            if np.abs(A_reduced[i, j]) &gt; rtol:\n                pivots.append(j)\n                break\n    free_vars = set(range(cols)) - set(pivots)\n    \n    null_space = []\n    for free_var in free_vars:\n        null_vector = np.zeros(cols)\n        null_vector[free_var] = 1\n        for pivot, row in zip(pivots, A_reduced[:len(pivots)]):\n            null_vector[pivot] = -row[free_var]\n        null_space.append(null_vector)\n    \n    return np.array(null_space).T\n\n# define the function to generate the row-space of A\n\ndef row_space_of_matrix(A):\n    \"\"\"Compute the row space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    # The non-zero rows of the reduced matrix form the row space\n    non_zero_rows = A_reduced[~np.all(A_reduced == 0, axis=1)]\n    return non_zero_rows\n\n# define the function to generate the column space of A\n\ndef column_space_of_matrix(A):\n    \"\"\"Compute the column space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    rows, cols = A_reduced.shape\n    # Identify pivot columns\n    pivots = []\n    for i in range(rows):\n        for j in range(cols):\n            if np.abs(A_reduced[i, j]) &gt; 1e-5:\n                pivots.append(j)\n                break\n    column_space = A[:, pivots]\n    return column_space\n\n\n\n2.2.5.3 Examples:\n\nFind all the fundamental subspaces of \\(A=\\begin{pmatrix}1&2&3\\\\ 4&5&6\\\\7&8&9\\end{pmatrix}\\).\n\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nprint(\"Matrix A:\")\nprint(A)\n\n# Null Space\nnull_space_A = null_space_of_matrix(A)\nprint(\"\\nNull Space of A:\")\nprint(null_space_A)\n\n# Row Space\nrow_space_A = row_space_of_matrix(A)\nprint(\"\\nRow Space of A:\")\nprint(row_space_A)\n\n# Column Space\ncolumn_space_A = column_space_of_matrix(A)\nprint(\"\\nColumn Space of A:\")\nprint(column_space_A)\n\nMatrix A:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nNull Space of A:\n[[-9.        ]\n [-1.71428571]\n [ 1.        ]]\n\nRow Space of A:\n[[7.00000000e+00 8.00000000e+00 9.00000000e+00]\n [0.00000000e+00 8.57142857e-01 1.71428571e+00]\n [0.00000000e+00 5.55111512e-17 1.11022302e-16]]\n\nColumn Space of A:\n[[1 2]\n [4 5]\n [7 8]]\n\n\n\n\n2.2.5.4 Rank and Solution of System of Linear Equations\nIn linear algebra, the rank of a matrix is a crucial concept for understanding the structure of a system of linear equations. It provides insight into the solutions of these systems, helping us determine the number of independent equations and the nature of the solution space.\n\nDefinition 2.2 (Rank and System Consistency) The rank of a matrix \\(A\\) is defined as the maximum number of linearly independent rows or columns. When solving a system of linear equations represented by \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(A\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{b}\\) is a vector, the rank of \\(A\\) plays a crucial role in determining the solution’s existence and uniqueness.\nConsistency of the System\n\nConsistent System: A system of linear equations is consistent if there exists at least one solution. This occurs if the rank of the coefficient matrix \\(A\\) is equal to the rank of the augmented matrix \\([A|\\mathbf{b}]\\). Mathematically, this can be expressed as: \\[\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}])\\] If this condition is met, the system has solutions. The solutions can be:\n\nUnique if the rank equals the number of variables.\nInfinitely many if the rank is less than the number of variables.\n\nInconsistent System: A system is inconsistent if there are no solutions. This occurs when: \\[\\text{rank}(A) \\ne \\text{rank}([A|\\mathbf{b}])\\] In this case, the equations represent parallel or conflicting constraints that cannot be satisfied simultaneously.\n\n\n\n\n\n\n\n\nUse of Null space in creation of general solution from particular solution\n\n\n\nIf the system \\(AX=b\\) has many solutions, then the general solution of the system can be found using a particular solution and the elements in the null space of the coefficient matrix \\(A\\) as\n\\[X=x_p+tX_N\\]\nwhere \\(X\\) is the general solution and \\(t\\) is a free variable (parameter) and \\(X_N\\in N(A)\\).\n\n\n\n\n2.2.5.5 Computational method to solve system of linear equations.\nIf for a system \\(AX=b\\), \\(det(A)\\neq 0\\), then the system has a unique solution and can be found by solve() function from NumPy. If the system is consistant and many solutions, then computationally we will generate the general solution using the \\(N(A)\\). A detailed Python code is given below.\n\nimport numpy as np\n\ndef check_consistency(A, b):\n    \"\"\"\n    Check the consistency of a linear system Ax = b and return the solution if consistent.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix.\n    b (numpy.ndarray): Right-hand side vector.\n    \n    Returns:\n    tuple: A tuple with consistency status, particular solution (if consistent), and null space (if infinite solutions).\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Augment the matrix A with vector b\n    augmented_matrix = np.column_stack((A, b))\n    \n    # Compute ranks\n    rank_A = np.linalg.matrix_rank(A)\n    rank_augmented = np.linalg.matrix_rank(augmented_matrix)\n    \n    # Check for consistency\n    if rank_A == rank_augmented:\n        if rank_A == A.shape[1]:\n            # Unique solution\n            solution = np.linalg.solve(A, b)\n            return \"Consistent and has a unique solution\", solution, None\n        else:\n            # Infinitely many solutions\n            particular_solution = np.linalg.lstsq(A, b, rcond=None)[0]\n            null_space = null_space_of_matrix(A)\n            return \"Consistent but has infinitely many solutions\", particular_solution, null_space\n    else:\n        return \"Inconsistent system (no solution)\", None, None\n\ndef null_space_of_matrix(A):\n    \"\"\"\n    Compute the null space of matrix A, which gives the set of solutions to Ax = 0.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix.\n    \n    Returns:\n    numpy.ndarray: Basis for the null space of A.\n    \"\"\"\n    u, s, vh = np.linalg.svd(A)\n    null_mask = (s &lt;= 1e-10)  # Singular values near zero\n    null_space = np.compress(null_mask, vh, axis=0)\n    return null_space.T\n\n\nExample 1: Solve \\[\\begin{align*}\n2x-y+z&=1\\\\\nx+2y&=3\\\\\n3x+2y+z&=4\n\\end{align*}\\]\n\n\n# Example usage 1: System with a unique solution\nA1 = np.array([[2, -1, 1], [1, 0, 2], [3, 2, 1]])\nb1 = np.array([1, 3, 4])\n\nstatus1, solution1, null_space1 = check_consistency(A1, b1)\nprint(\"Example 1 - Status:\", status1)\n\nif solution1 is not None:\n    print(\"Solution:\", solution1)\nif null_space1 is not None:\n    print(\"Null Space:\", null_space1)\n\nExample 1 - Status: Consistent and has a unique solution\nSolution: [0.27272727 0.90909091 1.36363636]\n\n\n\nExample 2: Solve the system of equations, \\[\\begin{align*}\nx+2y+z&=3\\\\\n2x+4y+2z&=6\\\\\nx+y+z&=2\n\\end{align*}\\]\n\n\n# Example usage 2: System with infinitely many solutions\nA2 = np.array([[1, 2, 1], [2, 4, 2], [1, 1, 1]])\nb2 = np.array([3, 6, 2])\n\nstatus2, solution2, null_space2 = check_consistency(A2, b2)\nprint(\"\\nExample 2 - Status:\", status2)\n\nif solution2 is not None:\n    print(\"Particular Solution:\", solution2)\nif null_space2 is not None:\n    print(\"Null Space (Basis for infinite solutions):\", null_space2)\n\n\nExample 2 - Status: Consistent but has infinitely many solutions\nParticular Solution: [0.5 1.  0.5]\nNull Space (Basis for infinite solutions): [[ 7.07106781e-01]\n [ 1.11022302e-16]\n [-7.07106781e-01]]\n\n\n\n\n\n\nStrang, Gilbert. 2020. Linear Algebra for Everyone. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_2.html#footnotes",
    "href": "module_2.html#footnotes",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "",
    "text": "A regularization techniques in Deep learning. This approach deactivate some selected neurons to control model over-fitting↩︎\nRemember that the covariance of \\(X\\) is defined as \\(Cov(X)=\\dfrac{\\sum (X-\\bar{X})^2}{n-1}\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nStrang, Gilbert. 2020. Linear Algebra for Everyone. SIAM.\n\n\n———. 2022. Introduction to Linear Algebra. SIAM.",
    "crumbs": [
      "References"
    ]
  }
]