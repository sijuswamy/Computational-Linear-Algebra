[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Linear Algebra",
    "section": "",
    "text": "Preface\n\nWelcome to the course on Computational Linear Algebra. This course is designed to provide a practical perspective on linear algebra, bridging the gap between mathematical theory and real-world applications. As we delve into the intricacies of linear algebra, our focus will be on equipping you with the skills to effectively utilize these concepts in the design, development, and manipulation of data-driven processes applicable to Computer Science and Engineering.\nThroughout this course, you will explore linear algebra not just as a set of abstract mathematical principles, but as a powerful tool for solving complex problems and optimizing processes. The curriculum integrates robust mathematical theory with hands-on implementation, enabling you to apply linear algebra techniques in practical scenarios.\nFrom understanding fundamental operations to applying advanced concepts in data-driven contexts, this course aims to build a strong foundation that supports both theoretical knowledge and practical expertise. Whether you’re tackling computational challenges or developing innovative solutions, the skills and insights gained here will be invaluable in your academic and professional endeavors Knuth (1984).\nWe look forward to guiding you through this journey of blending theory with practice and helping you harness the full potential of linear algebra in your work.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction to Computational Linear Algebra\nWelcome to the Computational Linear Algebra course, a pivotal component of our Computational Mathematics for Engineering minor program. This course is meticulously designed to connect theoretical linear algebra concepts with their practical applications in Artificial Intelligence (AI) and Data Science.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-computational-linear-algebra",
    "href": "intro.html#introduction-to-computational-linear-algebra",
    "title": "Introduction",
    "section": "",
    "text": "Course Themes\n\nPractical Application Proficiency\n\nOur primary focus is on seamlessly translating theoretical concepts into practical solutions for real-world challenges.\nDevelop robust problem-solving skills applicable to AI, Data Science, and advanced engineering scenarios.\n\nMathematical Expertise for Data Insights\n\nGain in-depth proficiency in computational linear algebra, covering essential topics like matrix operations, eigendecomposition, and singular value decomposition.\nLeverage linear algebra techniques to derive meaningful insights and make informed decisions in data science applications.\n\nHands-On Learning\n\nEngage in immersive, project-based learning experiences with a strong emphasis on Python implementation.\nApply linear algebra principles to practical problems, including linear regression, principal component analysis (PCA), and neural networks.\n\n\n\n\nRelevance and Impact\nIn today’s technology-driven landscape, linear algebra forms the backbone of many critical algorithms and applications in AI and Data Science. This course will not only enhance your analytical and computational skills but also prepare you to address complex engineering problems with confidence.\nBy the end of this course, you will have acquired a comprehensive understanding of the role of linear algebra in computational mathematics and its practical applications. This knowledge will equip you with the tools necessary to excel in the rapidly evolving tech industry.\nLet us start this educational journey together, where theoretical knowledge meets practical application, and explore the fascinating and impactful world of Computational Linear Algebra.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "module_1.html",
    "href": "module_1.html",
    "title": "1  Python for Linear Algebra",
    "section": "",
    "text": "1.1 Pseudocode: the new language for algorithm design\nPseudocode is a way to describe algorithms in a structured but plain language. It helps in planning the logic without worrying about the syntax of a specific programming language. In this module, we’ll use Python-flavored pseudocode to describe various matrix operations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#pseudocode-the-new-language-for-algorithm-design",
    "href": "module_1.html#pseudocode-the-new-language-for-algorithm-design",
    "title": "1  Python for Linear Algebra",
    "section": "",
    "text": "Caution\n\n\n\nThere are varities of approaches in writing pseudocode. Students can adopt any of the standard approach to write pseudocode.\n\n\n\n1.1.1 Matrix Sum\nMathematical Procedure:\nTo add two matrices \\(A\\) and \\(B\\), both matrices must have the same dimensions. The sum \\(C\\) of two matrices \\(A\\) and \\(B\\) is calculated element-wise:\n\\[C[i][j] = A[i][j] + B[i][j]\\]\nExample:\nLet \\(A\\) and \\(B\\) be two \\(2 \\times 2\\) matrices:\n\\[ A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\]\nThe sum \\(C\\) is:\n\\[ C = A + B = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix} \\]\nPseudocode:\nFUNCTION matrix_sum(A, B):\n    Get the number of rows and columns in matrix A\n    Create an empty matrix C with the same dimensions\n    FOR each row i:\n        FOR each column j:\n            Set C[i][j] to the sum of A[i][j] and B[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrix \\(A\\).\nCreate a new matrix \\(C\\) with the same dimensions.\nLoop through each element of the matrices and add corresponding elements.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.2 Matrix Difference\nMathematical Procedure:\nTo subtract matrix \\(B\\) from matrix \\(A\\), both matrices must have the same dimensions. The difference \\(C\\) of two matrices \\(A\\) and \\(B\\) is calculated element-wise:\n\\[ C[i][j] = A[i][j] - B[i][j] \\]\nExample:\nLet \\(A\\) and \\(B\\) be two \\(2 \\times 2\\) matrices:\n\\[ A = \\begin{bmatrix} 9 & 8 \\\\ 7 & 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\]\nThe difference \\(C\\) is:\n\\[ C = A - B = \\begin{bmatrix} 9-1 & 8-2 \\\\ 7-3 & 6-4 \\end{bmatrix} = \\begin{bmatrix} 8 & 6 \\\\ 4 & 2 \\end{bmatrix} \\]\nPseudocode:\nFUNCTION matrix_difference(A, B):\n    # Determine the number of rows and columns in matrix A\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Create an empty matrix C with the same dimensions as A and B\n    C = create_matrix(rows, cols)\n    \n    # Iterate through each row\n    FOR i FROM 0 TO rows-1:\n        # Iterate through each column\n        FOR j FROM 0 TO cols-1:\n            # Calculate the difference for each element and store it in C\n            C[i][j] = A[i][j] - B[i][j]\n    \n    # Return the result matrix C\n    RETURN C\nEND FUNCTION\nIn more human readable format the above pseudocode can be written as:\nFUNCTION matrix_difference(A, B):\n    Get the number of rows and columns in matrix A\n    Create an empty matrix C with the same dimensions\n    FOR each row i:\n        FOR each column j:\n            Set C[i][j] to the difference of A[i][j] and B[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrix \\(A\\).\nCreate a new matrix \\(C\\) with the same dimensions.\nLoop through each element of the matrices and subtract corresponding elements.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.3 Matrix Product\nMathematical Procedure:\nTo find the product of two matrices \\(A\\) and \\(B\\), the number of columns in \\(A\\) must be equal to the number of rows in \\(B\\). The element \\(C[i][j]\\) in the product matrix \\(C\\) is computed as:\n\\[C[i][j] = \\sum_{k} A[i][k] \\cdot B[k][j]\\]\nExample:\nLet \\(A\\) be a \\(2 \\times 3\\) matrix and \\(B\\) be a \\(3 \\times 2\\) matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix}\\]\nThe product \\(C\\) is:\n\\[C = A \\cdot B = \\begin{bmatrix} 58 & 64 \\\\ 139 & 154 \\end{bmatrix}\\]\nPseudocode:\nFUNCTION matrix_product(A, B):\n    # Get the dimensions of A and B\n    rows_A = number_of_rows(A)\n    cols_A = number_of_columns(A)\n    rows_B = number_of_rows(B)\n    cols_B = number_of_columns(B)\n    \n    # Check if multiplication is possible\n    IF cols_A != rows_B:\n        RAISE Error(\"Incompatible matrix dimensions\")\n    \n    # Initialize result matrix C\n    C = create_matrix(rows_A, cols_B)\n    \n    # Calculate matrix product\n    FOR each row i FROM 0 TO rows_A-1:\n        FOR each column j FROM 0 TO cols_B-1:\n            # Compute the sum for C[i][j]\n            sum = 0\n            FOR each k FROM 0 TO cols_A-1:\n                sum = sum + A[i][k] * B[k][j]\n            C[i][j] = sum\n    \n    RETURN C\nEND FUNCTION\nA more human readable version of the pseudocode is shown below:\nFUNCTION matrix_product(A, B):\n    Get the number of rows and columns in matrix A\n    Get the number of columns in matrix B\n    Create an empty matrix C with dimensions rows_A x cols_B\n    FOR each row i in A:\n        FOR each column j in B:\n            Initialize C[i][j] to 0\n            FOR each element k in the common dimension:\n                Add the product of A[i][k] and B[k][j] to C[i][j]\n    RETURN the matrix C\nEND FUNCTION\nExplanation:\n\nDetermine the number of rows and columns in matrices \\(A\\) and \\(B\\).\nCreate a new matrix \\(C\\) with dimensions \\(\\text{rows}(A)\\times \\text{columns}(B)\\).\nLoop through each element of the resulting matrix \\(C[i][j]\\) and calculate the dot product of \\(i\\) the row of \\(A\\) to \\(j\\) th column of \\(B\\) for each element.\nReturn the resulting matrix \\(C\\).\n\n\n\n1.1.4 Determinant\nMathematical Procedure:\nTo find the determinant of a square matrix \\(A\\), we can use the Laplace expansion, which involves breaking the matrix down into smaller submatrices. For a \\(2 \\times 2\\) matrix, the determinant is calculated as:\n\\[\\text{det}(A) = A[0][0] \\cdot A[1][1] - A[0][1] \\cdot A[1][0]\\]\nFor larger matrices, the determinant is calculated recursively.\nExample:\nLet \\(A\\) be a \\(2 \\times 2\\) matrix:\n\\[A = \\begin{bmatrix} 4 & 3 \\\\ 6 & 3 \\end{bmatrix}\\]\nThe determinant of \\(A\\) is:\n\\[\\text{det}(A) = (4 \\cdot 3) - (3 \\cdot 6) = 12 - 18 = -6\\]\nPseudocode:\nFUNCTION determinant(A):\n    # Step 1: Get the size of the matrix\n    n = number_of_rows(A)\n    \n    # Base case for a 2x2 matrix\n    IF n == 2:\n        RETURN A[0][0] * A[1][1] - A[0][1] * A[1][0]\n    \n    # Step 2: Initialize determinant to 0\n    det = 0\n    \n    # Step 3: Loop through each column of the first row\n    FOR each column j FROM 0 TO n-1:\n        # Get the submatrix excluding the first row and current column\n        submatrix = create_submatrix(A, 0, j)\n        # Recursive call to determinant\n        sub_det = determinant(submatrix)\n        # Alternating sign and adding to the determinant\n        det = det + ((-1) ^ j) * A[0][j] * sub_det\n    \n    RETURN det\nEND FUNCTION\n\nFUNCTION create_sub_matrix(A, row, col):\n    sub_matrix = create_matrix(number_of_rows(A)-1, number_of_columns(A)-1)\n    sub_i = 0\n    FOR i FROM 0 TO number_of_rows(A)-1:\n        IF i == row:\n            CONTINUE\n        sub_j = 0\n        FOR j FROM 0 TO number_of_columns(A)-1:\n            IF j == col:\n                CONTINUE\n            sub_matrix[sub_i][sub_j] = A[i][j]\n            sub_j = sub_j + 1\n        sub_i = sub_i + 1\n    RETURN sub_matrix\nEND FUNCTION\nA human readable version of the same pseudocode is shown below:\nFUNCTION determinant(A):\n    IF the size of A is 2x2:\n        RETURN the difference between the product of the diagonals\n    END IF\n    Initialize det to 0\n    FOR each column c in the first row:\n        Create a sub_matrix by removing the first row and column c\n        Add to det: the product of (-1)^c, the element A[0][c], and the determinant of the sub_matrix\n    RETURN det\nEND FUNCTION\n\nFUNCTION create_sub_matrix(A, row, col):\n    Create an empty sub_matrix with dimensions one less than A\n    Set sub_i to 0\n    FOR each row i in A:\n        IF i is the row to be removed:\n            CONTINUE to the next row\n        Set sub_j to 0\n        FOR each column j in A:\n            IF j is the column to be removed:\n                CONTINUE to the next column\n            Copy the element A[i][j] to sub_matrix[sub_i][sub_j]\n            Increment sub_j\n        Increment sub_i\n    RETURN sub_matrix\nEND FUNCTION\nExplanation:\n\nIf the matrix is \\(2×2\\), calculate the determinant directly.\nFor larger matrices, use the Laplace expansion to recursively calculate the determinant.\nCreate submatrices by removing the current row and column.\nSum the determinants of the submatrices, adjusted for the sign and the current element.\n\n\n\n1.1.5 Rank of a Matrix\nMathematical Procedure:\nThe rank of a matrix \\(A\\) is the maximum number of linearly independent rows or columns in \\(A\\). This can be found using Gaussian elimination to transform the matrix into its row echelon form (REF) and then counting the number of non-zero rows.\nExample:\nLet \\(A\\) be a \\(3 \\times 3\\) matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\nAfter performing Gaussian elimination, we obtain:\n\\[\\text{REF}(A) = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\nThe rank of \\(A\\) is the number of non-zero rows, which is 2.\nPseudocode:\nFUNCTION matrix_rank(A):\n    # Step 1: Get the dimensions of the matrix\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Step 2: Transform the matrix to row echelon form\n    row_echelon_form(A, rows, cols)\n    \n    # Step 3: Count non-zero rows\n    rank = 0\n    FOR each row i FROM 0 TO rows-1:\n        non_zero = FALSE\n        FOR each column j FROM 0 TO cols-1:\n            IF A[i][j] != 0:\n                non_zero = TRUE\n                BREAK\n        IF non_zero:\n            rank = rank + 1\n    \n    RETURN rank\nEND FUNCTION\n\nFUNCTION row_echelon_form(A, rows, cols):\n    # Perform Gaussian elimination\n    lead = 0\n    FOR r FROM 0 TO rows-1:\n        IF lead &gt;= cols:\n            RETURN\n        i = r\n        WHILE A[i][lead] == 0:\n            i = i + 1\n            IF i == rows:\n                i = r\n                lead = lead + 1\n                IF lead == cols:\n                    RETURN\n        # Swap rows i and r\n        swap_rows(A, i, r)\n        # Make A[r][lead] = 1\n        lv = A[r][lead]\n        A[r] = [m / float(lv) for m in A[r]]\n        # Make all rows below r have 0 in column lead\n        FOR i FROM r + 1 TO rows-1:\n            lv = A[i][lead]\n            A[i] = [iv - lv * rv for rv, iv in zip(A[r], A[i])]\n        lead = lead + 1\nEND FUNCTION\n\nFUNCTION swap_rows(A, row1, row2):\n    temp = A[row1]\n    A[row1] = A[row2]\n    A[row2] = temp\nEND FUNCTION\nA more human readable version of the above pseudocode is shown below:\nFUNCTION rank(A):\n    Get the number of rows and columns in matrix A\n    Initialize the rank to 0\n    FOR each row i in A:\n        IF the element in the current row and column is non-zero:\n            Increment the rank\n            FOR each row below the current row:\n                Calculate the multiplier to zero out the element below the diagonal\n                Subtract the appropriate multiple of the current row from each row below\n        ELSE:\n            Initialize a variable to track if a swap is needed\n            FOR each row below the current row:\n                IF a non-zero element is found in the current column:\n                    Swap the current row with the row having the non-zero element\n                    Set the swap variable to True\n                    BREAK the loop\n            IF no swap was made:\n                Decrement the rank\n    RETURN the rank\nEND FUNCTION\nExplanation:\n\nInitialize the rank to 0.\nLoop through each row of the matrix.\nIf the diagonal element is non-zero, increment the rank and perform row operations to zero out the elements below the diagonal.\nIf the diagonal element is zero, try to swap with a lower row that has a non-zero element in the same column.\nIf no such row is found, decrement the rank.\nReturn the resulting rank of the matrix.\n\n\n\n1.1.6 Practice Problems\nFind the rank of the following matrices.\n\n\\(\\begin{pmatrix} 1&1&3\\\\ 2&2&6\\\\ 2&5&3\\end{pmatrix}\\).\n\\(\\begin{pmatrix} 2&0&2\\\\ 1&2&3\\\\ 3&2&7\\end{pmatrix}\\)\n\n\n\n1.1.7 Solving a System of Equations\nMathematical Procedure:\nTo solve a system of linear equations represented as \\(A \\mathbf{x} = \\mathbf{b}\\), where\\(A\\) is the coefficient matrix, \\(\\mathbf{x}\\) is the vector of variables, and\\(\\mathbf{b}\\) is the constant vector, we can use Gaussian elimination to transform the augmented matrix \\([A | \\mathbf{b}]\\) into its row echelon form (REF) and then perform back substitution to find the solution vector \\(\\mathbf{x}\\) Strang (2022).\nExample:\nConsider the system of equations:\n\\[\\begin{cases}\nx + 2y + 3z &= 9 \\\\\n4x + 5y + 6z& = 24 \\\\\n7x + 8y + 9z& = 39\n\\end{cases}\\]\nThe augmented matrix is:\n\\[[A | \\mathbf{b}] = \\begin{bmatrix} 1 & 2 & 3 & | & 9 \\\\ 4 & 5 & 6 & | & 24 \\\\ 7 & 8 & 9 & | & 39 \\end{bmatrix}\\]\nAfter performing Gaussian elimination on the augmented matrix, we get:\n\\[\\text{REF}(A) = \\begin{bmatrix} 1 & 2 & 3 & | & 9 \\\\ 0 & -3 & -6 & | & -12 \\\\ 0 & 0 & 0 & | & 0 \\end{bmatrix}\\]\nPerforming back substitution, we solve for \\(z\\),\\(y\\), and \\(x\\):\n\\[\\begin{cases}\nz = 1 \\\\\ny = 0 \\\\\nx = 3\n\\end{cases}\\]\nTherefore, the solution vector is \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\nPseudocode:\nFUNCTION solve_system_of_equations(A, b):\n    # Step 1: Get the dimensions of the matrix\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    \n    # Step 2: Create the augmented matrix\n    augmented_matrix = create_augmented_matrix(A, b)\n    \n    # Step 3: Transform the augmented matrix to row echelon form\n    row_echelon_form(augmented_matrix, rows, cols)\n    \n    # Step 4: Perform back substitution\n    solution = back_substitution(augmented_matrix, rows, cols)\n    \n    RETURN solution\nEND FUNCTION\n\nFUNCTION create_augmented_matrix(A, b):\n    # Combine A and b into an augmented matrix\n    augmented_matrix = []\n    FOR i FROM 0 TO number_of_rows(A)-1:\n        augmented_matrix.append(A[i] + [b[i]])\n    RETURN augmented_matrix\nEND FUNCTION\n\nFUNCTION row_echelon_form(augmented_matrix, rows, cols):\n    # Perform Gaussian elimination\n    lead = 0\n    FOR r FROM 0 TO rows-1:\n        IF lead &gt;= cols:\n            RETURN\n        i = r\n        WHILE augmented_matrix[i][lead] == 0:\n            i = i + 1\n            IF i == rows:\n                i = r\n                lead = lead + 1\n                IF lead == cols:\n                    RETURN\n        # Swap rows i and r\n        swap_rows(augmented_matrix, i, r)\n        # Make augmented_matrix[r][lead] = 1\n        lv = augmented_matrix[r][lead]\n        augmented_matrix[r] = [m / float(lv) for m in augmented_matrix[r]]\n        # Make all rows below r have 0 in column lead\n        FOR i FROM r + 1 TO rows-1:\n            lv = augmented_matrix[i][lead]\n            augmented_matrix[i] = [iv - lv * rv for rv, iv in zip(augmented_matrix[r], augmented_matrix[i])]\n        lead = lead + 1\nEND FUNCTION\n\nFUNCTION back_substitution(augmented_matrix, rows, cols):\n    # Initialize the solution vector\n    solution = [0 for _ in range(rows)]\n    # Perform back substitution\n    FOR i FROM rows-1 DOWNTO 0:\n        solution[i] = augmented_matrix[i][cols-1]\n        FOR j FROM i+1 TO cols-2:\n            solution[i] = solution[i] - augmented_matrix[i][j] * solution[j]\n    RETURN solution\nEND FUNCTION\n\nFUNCTION swap_rows(matrix, row1, row2):\n    temp = matrix[row1]\n    matrix[row1] = matrix[row2]\n    matrix[row2] = temp\nEND FUNCTION\nExplanation:\n\nAugment the coefficient matrix \\(A\\) with the constant matrix \\(B\\).\nPerform Gaussian elimination to reduce the augmented matrix to row echelon form.\nBack-substitute to find the solution vector \\(X\\).\nReturn the solution vector \\(X\\).\n\n\n\n1.1.8 Review Problems\nQ1: Fill in the missing parts of the pseudocode to yield a meaningful algebraic operation on of two matrices.\nPseudocode:\nFUNCTION matrix_op1(A, B):\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    result = create_matrix(rows, cols, 0)\n    \n    FOR i FROM 0 TO rows-1:\n        FOR j FROM 0 TO cols-1:\n            result[i][j] = A[i][j] + ---\n    \n    RETURN result\nEND FUNCTION\nQ2: Write the pseudocode to get useful derivable from a given a matrix by fill in the missing part.\nPseudocode:\nFUNCTION matrix_op2(A):\n    rows = number_of_rows(A)\n    cols = number_of_columns(A)\n    result = create_matrix(cols, rows, 0)\n    \n    FOR i FROM 0 TO rows-1:\n        FOR j FROM 0 TO cols-1:\n            result[j][i] = A[i][--]\n    \n    RETURN result\nEND FUNCTION",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#transition-from-pseudocode-to-python-programming",
    "href": "module_1.html#transition-from-pseudocode-to-python-programming",
    "title": "1  Python for Linear Algebra",
    "section": "1.2 Transition from Pseudocode to Python Programming",
    "text": "1.2 Transition from Pseudocode to Python Programming\nIn this course, our initial approach to understanding and solving linear algebra problems has been through pseudocode. Pseudocode allows us to focus on the logical steps and algorithms without getting bogged down by the syntax of a specific programming language. This method helps us build a strong foundation in the computational aspects of linear algebra.\nHowever, to fully leverage the power of computational tools and prepare for real-world applications, it is essential to implement these algorithms in a practical programming language. Python is a highly versatile and widely-used language in the fields of data science, artificial intelligence, and engineering. By transitioning from pseudocode to Python, we align with the following course objectives:\n\nPractical Implementation: Python provides numerous libraries and tools, such as NumPy and SciPy, which are specifically designed for numerical computations and linear algebra. Implementing our algorithms in Python allows us to perform complex calculations efficiently and accurately.\nHands-On Experience: Moving to Python programming gives students hands-on experience in coding, debugging, and optimizing algorithms. This practical experience is crucial for developing the skills required in modern computational tasks.\nIndustry Relevance: Python is extensively used in industry for data analysis, machine learning, and scientific research. Familiarity with Python and its libraries ensures that students are well-prepared for internships, research projects, and future careers in these fields.\nIntegration with Other Tools: Python’s compatibility with various tools and platforms allows for seamless integration into larger projects and workflows. This integration is vital for tackling real-world problems that often require multi-disciplinary approaches.\nEnhanced Learning: Implementing algorithms in Python helps reinforce theoretical concepts by providing immediate feedback through code execution and results visualization. This iterative learning process deepens understanding and retention of the material.\n\nBy transitioning to Python programming, we not only achieve our course objectives but also equip students with valuable skills that are directly applicable to their academic and professional pursuits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#python-fundamentals",
    "href": "module_1.html#python-fundamentals",
    "title": "1  Python for Linear Algebra",
    "section": "1.3 Python Fundamentals",
    "text": "1.3 Python Fundamentals\n\n1.3.1 Python Programming Overview\nPython is a high-level, interpreted programming language that was created by Guido van Rossum and first released in 1991. Its design philosophy emphasizes code readability and simplicity, making it an excellent choice for both beginners and experienced developers. Over the years, Python has undergone significant development and improvement, with major releases adding new features and optimizations. The language’s versatility and ease of use have made it popular in various domains, including web development, data science, artificial intelligence, scientific computing, automation, and more. Python’s extensive standard library and active community contribute to its widespread adoption, making it one of the most popular programming languages in the world today.\n\n\n1.3.2 Variables\nIn Python, variables are used to store data that can be used and manipulated throughout a program. Variables do not need explicit declaration to reserve memory space. The declaration happens automatically when a value is assigned to a variable.\nBasic Input/Output Functions\nPython provides built-in functions for basic input and output operations. The print() function is used to display output, while the input() function is used to take input from the user.\nOutput with print() function\n\nExample 1\n\n# Printing text\nprint(\"Hello, World!\")\n\n# Printing multiple values\nx = 5\ny = 10\nprint(\"The value of x is:\", x, \"and the value of y is:\", y)\n\nExample 2\n\n# Assigning values to variables\na = 10\nb = 20.5\nname = \"Alice\"\n\n# Printing the values\nprint(\"Values Stored in the Variables:\")\nprint(a)\nprint(b)\nprint(name)\nInput with input() Function:\n# Taking input from the user\nname = input(\"Enter usr name: \")\nprint(\"Hello, \" + name + \"!\")\n\n# Taking numerical input\nage = int(input(\"Enter usr age: \"))\nprint(\"us are\", age, \"years old.\")\n\n\n\n\n\n\nNote\n\n\n\nThe print() function in Python, defined in the built-in __builtin__ module, is used to display output on the screen, providing a simple way to output text and variable values to the console.\n\n\nCombining Variables and Input/Output\nus can combine variables and input/output functions to create interactive programs.\n\nExample:\n\n# Program to calculate the sum of two numbers\nnum1 = float(input(\"Enter first number: \"))\nnum2 = float(input(\"Enter second number: \"))\n\n# Calculate sum\nsum = num1 + num2\n\n# Display the result\nprint(\"The sum of\", num1, \"and\", num2, \"is\", sum)\n\n\n1.3.3 Python Programming Style\n\n1.3.3.1 Indentation\nPython uses indentation to define the blocks of code. Proper indentation is crucial as it affects the program’s flow. Use 4 spaces per indentation level.\nif a &gt; b:\n    print(\"a is greater than b\")\nelse:\n    print(\"b is greater than or equal to a\")\n\n\n1.3.3.2 Comments\nUse comments to explain user code. Comments begin with the # symbol and extend to the end of the line. Write comments that are clear and concise. See the example:\n# This is a comment\na = 10  # This is an inline comment\n\n\n1.3.3.3 Variable Naming\nUse meaningful variable names to make usr code more understandable. Variable names should be in lowercase with words separated by underscores.\nstudent_name = \"John\"\ntotal_score = 95\n\n\n1.3.3.4 Consistent Style\nFollow the PEP 8 style guide for Python code to maintain consistency and readability. Use blank lines to separate different sections of usr code. See the following example of function definition:\n\ndef calculate_sum(x, y):\n    return x + y\n\nresult = calculate_sum(5, 3)\nprint(result)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#basic-datatypes-in-python",
    "href": "module_1.html#basic-datatypes-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.4 Basic Datatypes in Python",
    "text": "1.4 Basic Datatypes in Python\nIn Python, a datatype is a classification that specifies which type of value a variable can hold. Understanding datatypes is essential as it helps in performing appropriate operations on variables. Python supports various built-in datatypes, which can be categorized into several groups.\n\n1.4.1 Numeric Types\nNumeric types represent data that consists of numbers. Python has three distinct numeric types:\n\nIntegers (int):\n\nWhole numbers, positive or negative, without decimals.\nExample: a = 10, b = -5.\n\nFloating Point Numbers (float):\n\nNumbers that contain a decimal point.\nExample: pi = 3.14, temperature = -7.5.\n\nComplex Numbers (complex):\n\nNumbers with a real and an imaginary part.\nExample: z = 3 + 4j.\n\n\n# Examples of numeric types\na = 10         # Integer\npi = 3.14      # Float\nz = 3 + 4j     # Complex\n\n\n1.4.2 Sequence Types\nSequence types are used to store multiple items in a single variable. Python has several sequence types, including:\n\n1.4.2.1 String Type\nStrings in Python are sequences of characters enclosed in quotes. They are used to handle and manipulate textual data.\nCharacteristics of Strings\n\nOrdered: Characters in a string have a defined order.\nImmutable: Strings cannot be modified after they are created.\nHeterogeneous: Strings can include any combination of letters, numbers, and symbols.\n\nCreating Strings\nStrings can be created using single quotes, double quotes, or triple quotes for multiline strings.\n\nExample:\n\n# Creating strings with different types of quotes\nsingle_quoted = 'Hello, World!'\ndouble_quoted = \"Hello, World!\"\nmultiline_string = \"\"\"This is a\nmultiline string\"\"\"\nAccessing String Characters\nCharacters in a string are accessed using their index, with the first character having an index of 0. Negative indexing can be used to access characters from the end.\n\nExample:\n\n# Accessing characters in a string\nfirst_char = single_quoted[0]  # Output: 'H'\nlast_char = single_quoted[-1]  # Output: '!'\nCommon String Methods\nPython provides various methods for string manipulation:\n\nupper(): Converts all characters to uppercase.\nlower(): Converts all characters to lowercase.\nstrip(): Removes leading and trailing whitespace.\nreplace(old, new): Replaces occurrences of a substring with another substring.\nsplit(separator): Splits the string into a list based on a separator.\n\n\nExample:\n\n# Using string methods\ntext = \"   hello, world!   \"\nuppercase_text = text.upper()       # Result: \"   HELLO, WORLD!   \"\nstripped_text = text.strip()        # Result: \"hello, world!\"\nreplaced_text = text.replace(\"world\", \"Python\")  # Result: \"   hello, Python!   \"\nwords = text.split(\",\")             # Result: ['hello', ' world!   ']\n\n\n1.4.2.2 List Type\nLists are one of the most versatile and commonly used sequence types in Python. They allow for the storage and manipulation of ordered collections of items.\n\nCharacteristics of Lists\n\n\nOrdered: The items in a list have a defined order, which will not change unless explicitly modified.\nMutable: The content of a list can be changed after its creation (i.e., items can be added, removed, or modified).\nDynamic: Lists can grow or shrink in size as items are added or removed.\nHeterogeneous: Items in a list can be of different data types (e.g., integers, strings, floats).\n\nCreating Lists\nLists are created by placing comma-separated values inside square brackets.\n\nExample:\n\n# Creating a list of fruits\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Creating a mixed list\nmixed_list = [1, \"Hello\", 3.14]\nAccessing List Items\nList items are accessed using their index, with the first item having an index of 0.\n\nExample:\n\n\n# Accessing the first item\nfirst_fruit = fruits[0]  # Output: \"apple\"\n\n# Accessing the last item\nlast_fruit = fruits[-1]  # Output: \"cherry\"\nModifying Lists\nLists can be modified by changing the value of specific items, adding new items, or removing existing items.\n\nExample:\n\n# Changing the value of an item\nfruits[1] = \"blueberry\"  # fruits is now [\"apple\", \"blueberry\", \"cherry\"]\n\n# Adding a new item\nfruits.append(\"orange\")  # fruits is now [\"apple\", \"blueberry\", \"cherry\", \"orange\"]\n\n# Removing an item\nfruits.remove(\"blueberry\")  # fruits is now [\"apple\", \"cherry\", \"orange\"]\nList Methods\nPython provides several built-in methods to work with lists:\n\nappend(item): Adds an item to the end of the list.\ninsert(index, item): Inserts an item at a specified index.\nremove(item): Removes the first occurrence of an item.\npop(index): Removes and returns the item at the specified index.\nsort(): Sorts the list in ascending order.\nreverse(): Reverses the order of the list.\n\n\nExample:\n\n# Using list methods\nnumbers = [5, 2, 9, 1]\n\nnumbers.append(4)     # numbers is now [5, 2, 9, 1, 4]\nnumbers.sort()        # numbers is now [1, 2, 4, 5, 9]\nnumbers.reverse()     # numbers is now [9, 5, 4, 2, 1]\nfirst_number = numbers.pop(0)  # first_number is 9, numbers is now [5, 4, 2, 1]\n\n\n1.4.2.3 Tuple Type\nTuples are a built-in sequence type in Python that is used to store an ordered collection of items. Unlike lists, tuples are immutable, which means their contents cannot be changed after creation.\nCharacteristics of Tuples\n\nOrdered: Tuples maintain the order of items, which is consistent throughout their lifetime.\nImmutable: Once a tuple is created, its contents cannot be modified. This includes adding, removing, or changing items.\nFixed Size: The size of a tuple is fixed; it cannot grow or shrink after creation.\nHeterogeneous: Tuples can contain items of different data types, such as integers, strings, and floats.\n\nCreating Tuples\nTuples are created by placing comma-separated values inside parentheses. Single-element tuples require a trailing comma.\n\nExample:\n\n# Creating a tuple with multiple items\ncoordinates = (10, 20, 30)\n\n# Creating a single-element tuple\nsingle_element_tuple = (5,)\n\n# Creating a tuple with mixed data types\nmixed_tuple = (1, \"Hello\", 3.14)\nAccessing Tuple Items\nTuple items are accessed using their index, with the first item having an index of 0. Negative indexing can be used to access items from the end.\n\nExample:\n\n# Accessing the first item\nx = coordinates[0]  # Output: 10\n\n# Accessing the last item\nz = coordinates[-1]  # Output: 30\nModifying Tuples\nSince tuples are immutable, their contents cannot be modified. However, us can create new tuples by combining or slicing existing ones.\n\nExample:\n\n# Combining tuples\nnew_coordinates = coordinates + (40, 50)  # Result: (10, 20, 30, 40, 50)\n\n# Slicing tuples\nsub_tuple = coordinates[1:3]  # Result: (20, 30)\nTuple Methods\nTuples have a limited set of built-in methods compared to lists:\n\ncount(item): Returns the number of occurrences of the specified item.\nindex(item): Returns the index of the first occurrence of the specified item.\n\n\nExample:\n\n# Using tuple methods\nnumbers = (1, 2, 3, 1, 2, 1)\n\n# Counting occurrences of an item\ncount_1 = numbers.count(1)  # Result: 3\n\n# Finding the index of an item\nindex_2 = numbers.index(2)  # Result: 1\n\n\n\n1.4.3 Mapping Types\nMapping types in Python are used to store data in key-value pairs. Unlike sequences, mappings do not maintain an order and are designed for quick lookups of data.\n\n1.4.3.1 Dictionary (dict)\nThe primary mapping type in Python is the dict. Dictionaries store data as key-value pairs, where each key must be unique, and keys are used to access their corresponding values.\nCharacteristics of Dictionaries\n\nUnordered: The order of items is not guaranteed and may vary.\nMutable: us can add, remove, and change items after creation.\nKeys: Must be unique and immutable (e.g., strings, numbers, tuples).\nValues: Can be of any data type and can be duplicated.\n\nCreating Dictionaries\nDictionaries are created using curly braces {} with key-value pairs separated by colons :.\n\nExample:\n\n# Creating a dictionary\nstudent = {\n    \"name\": \"Alice\",\n    \"age\": 21,\n    \"major\": \"Computer Science\"\n}\nAccessing and Modifying Dictionary Items\nItems in a dictionary are accessed using their keys. us can also modify, add, or remove items.\n\nExample:\n\n# Accessing a value\nname = student[\"name\"]  # Output: \"Alice\"\n\n# Modifying a value\nstudent[\"age\"] = 22  # Updates the age to 22\n\n# Adding a new key-value pair\nstudent[\"graduation_year\"] = 2024\n\n# Removing a key-value pair\ndel student[\"major\"]\nDictionary Methods\nPython provides several built-in methods to work with dictionaries:\n\nkeys(): Returns a view object of all keys.\nvalues(): Returns a view object of all values.\nitems(): Returns a view object of all key-value pairs.\nget(key, default): Returns the value for the specified key, or a default value if the key is not found.\npop(key, default): Removes and returns the value for the specified key, or a default value if the key is not found.\n\n\nExample:\n\n# Using dictionary methods\nkeys = student.keys()        # Result: dict_keys(['name', 'age', 'graduation_year'])\nvalues = student.values()    # Result: dict_values(['Alice', 22, 2024])\nitems = student.items()      # Result: dict_items([('name', 'Alice'), ('age', 22), ('graduation_year', 2024)])\nname = student.get(\"name\")  # Result: \"Alice\"\nage = student.pop(\"age\")    # Result: 22\n\n\n\n1.4.4 Set Types\nSets are a built-in data type in Python used to store unique, unordered collections of items. They are particularly useful for operations involving membership tests, set operations, and removing duplicates.\nCharacteristics of Sets\n\nUnordered : The items in a set do not have a specific order and may change.\nMutable : us can add or remove items from a set after its creation.\nUnique : Sets do not allow duplicate items; all items must be unique.\nUnindexed : Sets do not support indexing or slicing.\n\nCreating Sets\nSets are created using curly braces {} with comma-separated values, or using the set() function.\n\nExample:\n\n# Creating a set using curly braces\nfruits = {\"apple\", \"banana\", \"cherry\"}\n\n# Creating a set using the set() function\nnumbers = set([1, 2, 3, 4, 5])\nAccessing and Modifying Set Items\nWhile us cannot access individual items by index, us can check for membership and perform operations like adding or removing items.\n\nExample:\n\n# Checking membership\nhas_apple = \"apple\" in fruits  # Output: True\n\n# Adding an item\nfruits.add(\"orange\")\n\n# Removing an item\nfruits.remove(\"banana\")  # Raises KeyError if item is not present\nSet Operations Sets support various mathematical set operations, such as union, intersection, and difference.\n\nExample:\n\n# Union of two sets\nset1 = {1, 2, 3}\nset2 = {3, 4, 5}\nunion = set1 | set2  # Result: {1, 2, 3, 4, 5}\n\n# Intersection of two sets\nintersection = set1 & set2  # Result: {3}\n\n# Difference between two sets\ndifference = set1 - set2  # Result: {1, 2}\n\n# Symmetric difference (items in either set, but not in both)\nsymmetric_difference = set1 ^ set2  # Result: {1, 2, 4, 5}\nSet Methods\nPython provides several built-in methods for set operations:\n\nadd(item): Adds an item to the set.\nremove(item): Removes an item from the set; raises KeyError if item is not present.\ndiscard(item): Removes an item from the set if present; does not raise an error if item is not found.\npop(): Removes and returns an arbitrary item from the set.\nclear(): Removes all items from the set.\n\n\nExample:\n\n# Using set methods\nset1 = {1, 2, 3}\n\nset1.add(4)        # set1 is now {1, 2, 3, 4}\nset1.remove(2)     # set1 is now {1, 3, 4}\nset1.discard(5)    # No error, set1 remains {1, 3, 4}\nitem = set1.pop()  # Removes and returns an arbitrary item, e.g., 1\nset1.clear()      # set1 is now an empty set {}\n\n1.4.4.1 ## Frozen Sets\nFrozen sets are a built-in data type in Python that are similar to sets but are immutable. Once created, a frozen set cannot be modified, making it suitable for use as a key in dictionaries or as elements of other sets.\nCharacteristics of Frozen Sets\n\nUnordered : The items in a frozen set do not have a specific order and may change.\nImmutable : Unlike regular sets, frozen sets cannot be altered after creation. No items can be added or removed.\nUnique : Like sets, frozen sets do not allow duplicate items; all items must be unique.\nUnindexed : Frozen sets do not support indexing or slicing.\n\nCreating Frozen Sets\nFrozen sets are created using the frozenset() function, which takes an iterable as an argument.\n\nExample:\n\n# Creating a frozen set\nnumbers = frozenset([1, 2, 3, 4, 5])\n\n# Creating a frozen set from a set\nfruits = frozenset({\"apple\", \"banana\", \"cherry\"})\nAccessing and Modifying Frozen Set Items\nFrozen sets do not support modification operations such as adding or removing items. However, us can perform membership tests and other set operations.\n\nExample:\n\n# Checking membership\nhas_apple = \"apple\" in fruits  # Output: True\n\n# Since frozenset is immutable, us cannot use add() or remove() methods\nSet Operations with Frozen Sets\nFrozen sets support various mathematical set operations similar to regular sets, such as union, intersection, and difference. These operations return new frozen sets and do not modify the original ones.\n\nExample:\n\n# Union of two frozen sets\nset1 = frozenset([1, 2, 3])\nset2 = frozenset([3, 4, 5])\nunion = set1 | set2  # Result: frozenset({1, 2, 3, 4, 5})\n\n# Intersection of two frozen sets\nintersection = set1 & set2  # Result: frozenset({3})\n\n# Difference between two frozen sets\ndifference = set1 - set2  # Result: frozenset({1, 2})\n\n# Symmetric difference (items in either set, but not in both)\nsymmetric_difference = set1 ^ set2  # Result: frozenset({1, 2, 4, 5})\nFrozen Set Methods\nFrozen sets have a subset of the methods available to regular sets. The available methods include:\n\ncopy() : Returns a shallow copy of the frozen set.\ndifference(other) : Returns a new frozen set with elements in the original frozen set but not in other.\nintersection(other) : Returns a new frozen set with elements common to both frozen sets.\nunion(other) : Returns a new frozen set with elements from both frozen sets.\nsymmetric_difference(other) : Returns a new frozen set with elements in either frozen set but not in both.\n\n\nExample:\n\n# Using frozen set methods\nset1 = frozenset([1, 2, 3])\nset2 = frozenset([3, 4, 5])\n\n# Getting the difference\ndifference = set1.difference(set2)  # Result: frozenset({1, 2})\n\n# Getting the intersection\nintersection = set1.intersection(set2)  # Result: frozenset({3})\n\n# Getting the union\nunion = set1.union(set2)  # Result: frozenset({1, 2, 3, 4, 5})\n\n# Getting the symmetric difference\nsymmetric_difference = set1.symmetric_difference(set2)  # Result: frozenset({1, 2, 4, 5})",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#control-structures-in-python",
    "href": "module_1.html#control-structures-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.5 Control Structures in Python",
    "text": "1.5 Control Structures in Python\nControl structures in Python allow us to control the flow of execution in our programs. They help manage decision-making, looping, and the execution of code blocks based on certain conditions. Python provides several key control structures: if statements, for loops, while loops, and control flow statements like break, continue, and pass.\n\n1.5.1 Conditional Statements\nConditional statements are used to execute code based on certain conditions. The primary conditional statement in Python is the if statement, which can be combined with elif and else to handle multiple conditions.\n\nSyntax:\n\nif condition:\n    # Code block to execute if condition is True\nelif another_condition:\n    # Code block to execute if another_condition is True\nelse:\n    # Code block to execute if none of the above conditions are True\n\nExample: Program to classify a person based on his/her age.\n\nage = 20\n\nif age &lt; 18:\n    print(\"us are a minor.\")\nelif age &lt; 65:\n    print(\"us are an adult.\")\nelse:\n    print(\"us are a senior citizen.\")\n\n\n1.5.2 Looping Statements\nLooping statements are used to repeat a block of code multiple times. Python supports for loops and while loops.\n\n1.5.2.1 For Loop\nThe for loop iterates over a sequence (like a list, tuple, or string) and executes a block of code for each item in the sequence.\n\nSyntax:\n\nfor item in sequence:\n    # Code block to execute for each item\n\nExample: Program to print names of fruits saved in a list.\n\n# Iterating over a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\n\n1.5.2.2 While Loop\nThe while loop repeatedly executes a block of code as long as a specified condition is True.\n\nSyntax:\n\nwhile condition:\n    # Code block to execute while condition is True\n\nExample: Print all counting numbers less than 5.\n\n# Counting from 0 to 4\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n\n\n\n1.5.3 Control Flow Statements\nControl flow statements alter the flow of execution within loops and conditionals.\n\n1.5.3.1 Break Statement\nThe break statement exits the current loop, regardless of the loop’s condition.\n\nExample: Program to exit from the printing of whole numbers less than 10, while trigger 5.\n\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n# Output: 0 1 2 3 4\n\n\n1.5.3.2 Continue Statement\nThe continue statement skips the rest of the code inside the current loop iteration and proceeds to the next iteration.\n\nExample: Program to print all the whole numbers in the range 5 except 2.\n\nfor i in range(5):\n    if i == 2:\n        continue\n    print(i) \n# Output: 0 1 3 4\n\n\n1.5.3.3 Pass Statement\nThe pass statement is a placeholder that does nothing and is used when a statement is syntactically required but no action is needed.\n\nExample: Program to print all the whole numbers in the range 5 except 3.\n\nfor i in range(5):\n    if i == 3:\n        pass  # Placeholder for future code\n    else:\n        print(i)\n# Output: 0 1 2 4\n\n\n\n\n\n\nCautions When Using Control Flow Structures\n\n\n\n\n\nControl flow structures are essential in Python programming for directing the flow of execution. However, improper use of these structures can lead to errors, inefficiencies, and unintended behaviors. Here are some cautions to keep in mind:\nInfinite Loops\n\nIssue: A while loop with a condition that never becomes False can lead to an infinite loop, which will cause the program to hang or become unresponsive.\nCaution: Always ensure that the condition in a while loop will eventually become False, and include logic within the loop to modify the condition.\n\nExample:\n# Infinite loop example\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    # Missing count increment, causing an infinite loop",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#functions-in-python-programming",
    "href": "module_1.html#functions-in-python-programming",
    "title": "1  Python for Linear Algebra",
    "section": "1.6 Functions in Python Programming",
    "text": "1.6 Functions in Python Programming\nFunctions are a fundamental concept in Python programming that enable code reuse, modularity, and organization. They allow us to encapsulate a block of code that performs a specific task, which can be executed whenever needed. Functions are essential for writing clean, maintainable, and scalable code, making them a cornerstone of effective programming practices.\nWhat is a Function?\nA function is a named block of code designed to perform a specific task. Functions can take inputs, called parameters or arguments, and can return outputs, which are the results of the computation or task performed by the function. By defining functions, we can write code once and reuse it multiple times, which enhances both efficiency and readability.\nDefining a Function\nIn Python, functions are defined using the def keyword, followed by the function name, parentheses containing any parameters, and a colon. The function body, which contains the code to be executed, is indented below the function definition.\n\nSyntax:\n\ndef function_name(parameters):\n    # Code block\n    return result\n\nExample:\n\ndef greet(name):\n    \"\"\"\n    Returns a greeting message for the given name.\n    \"\"\"\n    return f\"Hello, {name}!\"\n\n1.6.0.1 Relevance of functions in Programming\n\nCode Reusability : Functions allow us to define a piece of code once and reuse it in multiple places. This reduces redundancy and helps maintain consistency across our codebase.\nModularity : Functions break down complex problems into smaller, manageable pieces. Each function can be focused on a specific task, making it easier to understand and maintain the code.\nAbstraction : Functions enable us to abstract away the implementation details. We can use a function without needing to know its internal workings, which simplifies the code we write and enhances readability.\nTesting and Debugging : Functions allow us to test individual components of our code separately. This isolation helps in identifying and fixing bugs more efficiently.\nLibrary Creation : Functions are the building blocks of libraries and modules. By organizing related functions into libraries, we can create reusable components that can be shared and utilized across different projects.\n\n\nExample: Creating a Simple Library\n\nStage 1: Define Functions in a Module\n# my_library.py\n\ndef add(a, b):\n    \"\"\"\n    Returns the sum of two numbers.\n    \"\"\"\n    return a + b\n\ndef multiply(a, b):\n    \"\"\"\n    Returns the product of two numbers.\n    \"\"\"\n    return a * b\nStage 2: Use the Library in Another Program\n# main.py\n\nimport my_library\n\nresult_sum = my_library.add(5, 3)\nresult_product = my_library.multiply(5, 3)\n\nprint(f\"Sum: {result_sum}\")\nprint(f\"Product: {result_product}\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#object-oriented-programming-oop-in-python",
    "href": "module_1.html#object-oriented-programming-oop-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.7 Object-Oriented Programming (OOP) in Python",
    "text": "1.7 Object-Oriented Programming (OOP) in Python\nObject-Oriented Programming (OOP) is a programming paradigm that uses “objects” to design and implement software. It emphasizes the organization of code into classes and objects, allowing for the encapsulation of data and functionality. OOP promotes code reusability, scalability, and maintainability through key principles such as encapsulation, inheritance, and polymorphism.\n\n1.7.1 Key Concepts of OOP\n\nClasses and Objects\n\n\nClass: A class is a blueprint for creating objects. It defines a set of attributes and methods that the created objects will have. A class can be thought of as a template or prototype for objects.\nObject: An object is an instance of a class. It is a specific realization of the class with actual values for its attributes.\n\n\n1.7.1.1 Example\n# Defining a class\nclass Dog:\n    def __init__(self, name, age):\n        self.name = name  # Attribute\n        self.age = age    # Attribute\n    \n    def bark(self):\n        return \"Woof!\"   # Method\n\n# Creating an object of the class\nmy_dog = Dog(name=\"Buddy\", age=3)\n\n# Accessing attributes and methods\nprint(my_dog.name)  # Output: Buddy\nprint(my_dog.age)   # Output: 3\nprint(my_dog.bark())  # Output: Woof!\n\nEncapsulation\n\nEncapsulation is the concept of bundling data (attributes) and methods (functions) that operate on the data into a single unit, or class. It restricts direct access to some of the object’s components and can help protect the internal state of the object from unintended modifications.\n\nExample: Controll the access to member variables using encapsulation.\n\nclass Account:\n    def __init__(self, balance):\n        self.__balance = balance  # Private attribute\n    \n    def deposit(self, amount):\n        if amount &gt; 0:\n            self.__balance += amount\n    \n    def get_balance(self):\n        return self.__balance\n\n# Creating an object of the class\nmy_account = Account(balance=1000)\nmy_account.deposit(500)\n\nprint(my_account.get_balance())  # Output: 1500\n# print(my_account.__balance)  # This will raise an AttributeError\n\nInheritance\n\nInheritance is a mechanism in which a new class (child or derived class) inherits attributes and methods from an existing class (parent or base class). It allows for code reuse and the creation of a hierarchy of classes.\n\nExample: Demonstrating usage of attributes of base class in the derived classes.\n\n# Base class\nclass Animal:\n    def __init__(self, name):\n        self.name = name\n    \n    def speak(self):\n        return \"Some sound\"\n\n# Derived class\nclass Dog(Animal):\n    def __init__(self, name, breed):\n        super().__init__(name)  # Calling the constructor of the base class\n        self.breed = breed\n    \n    def speak(self):\n        return \"Woof!\"\n\n# Another derived class\nclass Cat(Animal):\n    def __init__(self, name, color):\n        super().__init__(name)  # Calling the constructor of the base class\n        self.color = color\n    \n    def speak(self):\n        return \"Meow!\"\n\n# Creating objects of the derived classes\ndog = Dog(name=\"Buddy\", breed=\"Golden Retriever\")\ncat = Cat(name=\"Whiskers\", color=\"Gray\")\n\nprint(f\"{dog.name} is a {dog.breed} and says {dog.speak()}\")  # Output: Buddy is a Golden Retriever and says Woof!\nprint(f\"{cat.name} is a {cat.color} cat and says {cat.speak()}\")  # Output: Whiskers is a Gray cat and says Meow!\n\nPolymorphism\n\nPolymorphism allows objects of different classes to be treated as objects of a common superclass. It enables a single interface to be used for different data types. In Python, polymorphism is often achieved through method overriding, where a method in a derived class has the same name as a method in the base class but implements different functionality.\n\nExample:\n\nclass Bird:\n    def fly(self):\n        return \"Flies in the sky\"\n\nclass Penguin(Bird):\n    def fly(self):\n        return \"Cannot fly, swims instead\"\n\n# Creating objects of different classes\nbird = Bird()\npenguin = Penguin()\n\nprint(bird.fly())      # Output: Flies in the sky\nprint(penguin.fly())  # Output: Cannot fly, swims instead",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#working-with-files-in-python",
    "href": "module_1.html#working-with-files-in-python",
    "title": "1  Python for Linear Algebra",
    "section": "1.8 Working with Files in Python",
    "text": "1.8 Working with Files in Python\nFile handling is an essential part of programming that allows us to work with data stored in files. Python provides built-in functions and methods to create, read, write, and manage files efficiently. This section will cover basic file operations, including opening, reading, writing, and closing files.\nOpening a File\nIn Python, we use the open() function to open a file. This function returns a file object, which provides methods and attributes to interact with the file. The open() function requires at least one argument: the path to the file. we can also specify the mode in which the file should be opened.\n\nSyntax:\n\nfile_object = open(file_path, mode)\nWhere,\n\nfile_path : Path to the file (can be a relative or absolute path).\nmode : Specifies the file access mode (e.g., ‘r’ for reading, ‘w’ for writing, ‘a’ for appending).\n\n\nExample:\n\n# Opening a file in read mode\nfile = open('example.txt', 'r')\nReading from a File\nOnce a file is opened, we can read its contents using various methods. Common methods include read(), readline(), and readlines().\n\nread() : Reads the entire file content.\nreadline() : Reads a single line from the file.\nreadlines() : Reads all the lines into a list.\n\n\nExample:\n\n# Reading the entire file\nfile_content = file.read()\nprint(file_content)\n\n# Reading a single line\nfile.seek(0)  # Move cursor to the start of the file\nline = file.readline()\nprint(line)\n\n# Reading all lines\nfile.seek(0)\nlines = file.readlines()\nprint(lines)\nWriting to a File\nTo write data to a file, we need to open the file in write (‘w’) or append (‘a’) mode. When opened in write mode, the file is truncated (i.e., existing content is deleted). When opened in append mode, new data is added to the end of the file.\n\nExample:\n\n# Opening a file in write mode\nfile = open('example.txt', 'w')\n\n# Writing data to the file\nfile.write(\"Hello, World!\\n\")\nfile.write(\"Python file handling example.\")\n\n# Closing the file\nfile.close()\nClosing a File\nIt is important to close a file after performing operations to ensure that all changes are saved and resources are released. We can close a file using the close() method of the file object.\n\nExample:\n\nf_1 = open('example.txt', 'w') # open the file example.txt to f_1\nf_1.close() # close the file with handler 'f_1'\nUsing Context Managers\nContext managers provide a convenient way to handle file operations, automatically managing file opening and closing. We can use the with statement to ensure that a file is properly closed after its block of code is executed.\n\nExample:\n\n# Using context manager to open and write to a file\nwith open('example.txt', 'w') as file:\n    file.write(\"This is written using a context manager.\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#from-theory-to-practice",
    "href": "module_1.html#from-theory-to-practice",
    "title": "1  Python for Linear Algebra",
    "section": "1.9 From Theory to Practice",
    "text": "1.9 From Theory to Practice\nIn this section, we transition from theoretical concepts to practical applications by exploring how fundamental matrix operations can be used in the field of image processing. By leveraging the knowledge gained from understanding matrix addition, subtraction, multiplication, and other operations, we can tackle real-world problems such as image blending, sharpening, filtering, and transformations. This hands-on approach not only reinforces the theoretical principles but also demonstrates their utility in processing and enhancing digital images. Through practical examples and coding exercises, you’ll see how these mathematical operations are essential tools in modern image manipulation and analysis.\n\n1.9.1 Applications of Matrix Operations in Digital Image Processing\nMatrix operations play a pivotal role in digital image processing, enabling a wide range of techniques for manipulating and enhancing images. By leveraging fundamental matrix operations such as addition, subtraction, multiplication, and transformations, we can perform essential tasks like image blending, filtering, edge detection, and geometric transformations. These operations provide the mathematical foundation for various algorithms used in image analysis, compression, and reconstruction. Understanding and applying matrix operations is crucial for developing efficient and effective image processing solutions, making it an indispensable skill in fields like computer vision, graphics, and multimedia applications.\n\n1.9.1.1 Matrix Addition in Image Blending\nMatrix addition is a fundamental operation in image processing, particularly useful in the technique of image blending. Image blending involves combining two images to produce a single image that integrates the features of both original images. This technique is commonly used in applications such as image overlay, transition effects in videos, and creating composite images.\nConcept\nWhen working with grayscale images, each image can be represented as a matrix where each element corresponds to the intensity of a pixel. By adding corresponding elements (pixels) of two matrices (images), we can blend the images together. The resultant matrix represents the blended image, where each pixel is the sum of the corresponding pixels in the original images.\n\nExample:\n\nConsider two 2x2 grayscale images represented as matrices:\nimage1= [[100, 150],[200, 250]]\nimage2=[[50, 100],[100, 150]]\nTo blend these images, we add the corresponding pixel values as:\nblended_image[i][j] = image1[i][j] + image2[i][j]\nEnsure that the resulting pixel values do not exceed the maximum value allowed (255 for 8-bit images).\nPython Implementation of image blending\nBelow is the Python code for blending two images using matrix addition:\n\ndef matrix_addition(image1, image2):\n    rows = len(image1)\n    cols = len(image1[0])\n    blended_image = [[0] * cols for _ in range(rows)]\n\n    for i in range(rows):\n        for j in range(cols):\n            blended_pixel = image1[i][j] + image2[i][j]\n            blended_image[i][j] = min(blended_pixel, 255)  # Clip to 255\n\n    return blended_image\n\n# Example matrices (images)\nimage1 = [[100, 150], [200, 250]]\nimage2 = [[50, 100], [100, 150]]\n\nblended_image = matrix_addition(image1, image2)\nprint(\"Blended Image:\")\nfor row in blended_image:\n    print(row)\n\nBlended Image:\n[150, 250]\n[255, 255]\n\n\nImage blending is a powerful technique with numerous real-time applications. It is widely used in creating smooth transitions in video editing, overlaying images in augmented reality, and producing composite images in photography and graphic design. By understanding and applying matrix operations, we can develop efficient algorithms that seamlessly integrate multiple images, enhancing the overall visual experience. The practical implementation of matrix addition in image blending underscores the importance of mathematical foundations in achieving sophisticated image processing tasks in real-world applications.\n\n\n\n\n\n\nImage Blending as Basic Arithmetic with Libraries\n\n\n\nIn upcoming chapters, we will explore how specific libraries for image handling simplify the process of image blending to a basic arithmetic operation—adding two objects. Using these libraries, such as PIL (Python Imaging Library) or OpenCV, allows us to leverage efficient built-in functions that streamline tasks like resizing, matrix operations, and pixel manipulation.\n\n\nLet’s summarize a few more matrix operations and its uses in digital image processing tasks in the following sections.\n\n\n1.9.1.2 Matrix Subtraction in Image Sharpening\nMatrix subtraction is a fundamental operation in image processing, essential for techniques like image sharpening. Image sharpening enhances the clarity and detail of an image by increasing the contrast along edges and boundaries.\nConcept\nIn grayscale images, each pixel value represents the intensity of light at that point. Image sharpening involves subtracting a smoothed version of the image from the original. This process accentuates edges and fine details, making them more prominent.\n\nExample:\n\nConsider a grayscale image represented as a matrix:\noriginal_image [[100, 150, 200],[150, 200, 250],[200, 250, 100]]\nTo sharpen the image, we subtract a blurred version (smoothed image) from the original. This enhances edges and fine details:\nsharpened_image[i][j] = original_image[i][j] - blurred_image[i][j]\nPython Implementation\nBelow is a simplified Python example of image sharpening using matrix subtraction:\n# Original image matrix (grayscale values)\noriginal_image = [\n    [100, 150, 200],\n    [150, 200, 250],\n    [200, 250, 100]\n]\n\n# Function to apply Gaussian blur (for demonstration, simplified as average smoothing)\ndef apply_blur(image):\n    blurred_image = []\n    for i in range(len(image)):\n        row = []\n        for j in range(len(image[0])):\n            neighbors = []\n            for dx in [-1, 0, 1]:\n                for dy in [-1, 0, 1]:\n                    ni, nj = i + dx, j + dy\n                    if 0 &lt;= ni &lt; len(image) and 0 &lt;= nj &lt; len(image[0]):\n                        neighbors.append(image[ni][nj])\n            blurred_value = sum(neighbors) // len(neighbors)\n            row.append(blurred_value)\n        blurred_image.append(row)\n    return blurred_image\n\n# Function for matrix subtraction (image sharpening)\ndef image_sharpening(original_image, blurred_image):\n    sharpened_image = []\n    for i in range(len(original_image)):\n        row = []\n        for j in range(len(original_image[0])):\n            sharpened_value = original_image[i][j] - blurred_image[i][j]\n            row.append(sharpened_value)\n        sharpened_image.append(row)\n    return sharpened_image\n\n# Apply blur to simulate smoothed image\nblurred_image = apply_blur(original_image)\n\n# Perform matrix subtraction for image sharpening\nsharpened_image = image_sharpening(original_image, blurred_image)\n\n# Print the sharpened image\nprint(\"Sharpened Image:\")\nfor row in sharpened_image:\n    print(row)\n\n\n1.9.1.3 Matrix Multiplication in Image Filtering (Convolution)\nMatrix multiplication, specifically convolution in the context of image processing, is a fundamental operation used for various tasks such as smoothing, sharpening, edge detection, and more. Convolution involves applying a small matrix, known as a kernel or filter, to an image matrix. This process modifies the pixel values of the image based on the values in the kernel, effectively filtering the image.\nConcept In grayscale images, each pixel value represents the intensity of light at that point. Convolution applies a kernel matrix over the image matrix to compute a weighted sum of neighborhood pixels. This weighted sum determines the new value of each pixel in the resulting filtered image.\n\nExample:\n\nConsider a grayscale image represented as a matrix:\noriginal_image= [[100, 150, 200, 250],\n [150, 200, 250, 300],\n [200, 250, 300, 350],\n [250, 300, 350, 400]]\nTo perform smoothing (averaging) using a simple kernel:\n[[1/9, 1/9, 1/9],\n [1/9, 1/9, 1/9],\n [1/9, 1/9, 1/9]]\nThe kernel is applied over the image using convolution:\nsmoothed_image[i][j] = sum(original_image[ii][jj] * kernel[k][l] for all (ii, jj) in neighborhood around (i, j))\nPython Implementation\nHere’s a simplified Python example demonstrating convolution for image smoothing without external libraries:\n# Original image matrix (grayscale values)\noriginal_image = [\n    [100, 150, 200, 250],\n    [150, 200, 250, 300],\n    [200, 250, 300, 350],\n    [250, 300, 350, 400]\n]\n\n# Define a simple kernel/filter for smoothing (averaging)\nkernel = [\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n]\n\n# Function for applying convolution (image filtering)\ndef apply_convolution(image, kernel):\n    height = len(image)\n    width = len(image[0])\n    ksize = len(kernel)\n    kcenter = ksize // 2  # Center of the kernel\n\n    # Initialize result image\n    filtered_image = [[0]*width for _ in range(height)]\n\n    # Perform convolution\n    for i in range(height):\n        for j in range(width):\n            sum = 0.0\n            for k in range(ksize):\n                for l in range(ksize):\n                    ii = i + k - kcenter\n                    jj = j + l - kcenter\n                    if ii &gt;= 0 and ii &lt; height and jj &gt;= 0 and jj &lt; width:\n                        sum += image[ii][jj] * kernel[k][l]\n            filtered_image[i][j] = int(sum)\n    \n    return filtered_image\n\n# Apply convolution to simulate smoothed image (averaging filter)\nsmoothed_image = apply_convolution(original_image, kernel)\n\n# Print the smoothed image\nprint(\"Smoothed Image:\")\nfor row in smoothed_image:\n    print(row)\n\n\n1.9.1.4 Determinant: Image Transformation\nConcept The determinant of a transformation matrix helps understand how transformations like scaling affect an image. A transformation matrix determines how an image is scaled, rotated, or sheared.\n\nExample:\n\nHere, we compute the determinant of a scaling matrix to understand how the scaling affects the image area.\ndef calculate_determinant(matrix):\n    a, b = matrix[0]\n    c, d = matrix[1]\n    return a * d - b * c\n\n# Example transformation matrix (scaling)\ntransformation_matrix = [[2, 0], [0, 2]]\ndeterminant = calculate_determinant(transformation_matrix)\nprint(f\"Determinant of the transformation matrix: {determinant}\")\nThis value indicates how the transformation scales the image area.\n\n\n1.9.1.5 Rank: Image Rank and Data Compression\nConcept The rank of a matrix indicates the number of linearly independent rows or columns. In image compression, matrix rank helps approximate an image with fewer data.\n\nExample:\n\nHere, we compute the rank of a matrix representing an image. A lower rank might indicate that the image can be approximated with fewer data.\ndef matrix_rank(matrix):\n    def is_zero_row(row):\n        return all(value == 0 for value in row)\n\n    def row_echelon_form(matrix):\n        A = [row[:] for row in matrix]\n        m = len(A)\n        n = len(A[0])\n        rank = 0\n        for i in range(min(m, n)):\n            if A[i][i] != 0:\n                for j in range(i + 1, m):\n                    factor = A[j][i] / A[i][i]\n                    for k in range(i, n):\n                        A[j][k] -= factor * A[i][k]\n                rank += 1\n        return rank\n\n    return row_echelon_form(matrix)\n\n# Example matrix (image)\nimage_matrix = [[1, 2], [3, 4]]\nrank = matrix_rank(image_matrix)\nprint(f\"Rank of the image matrix: {rank}\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#matrix-operations-using-python-libraries",
    "href": "module_1.html#matrix-operations-using-python-libraries",
    "title": "1  Python for Linear Algebra",
    "section": "1.10 Matrix Operations Using Python Libraries",
    "text": "1.10 Matrix Operations Using Python Libraries\n\n1.10.1 Introduction\nIn this section, we will explore the computational aspects of basic matrix algebra using Python. We will utilize the SymPy library for symbolic mathematics, which allows us to perform matrix operations and convert results into LaTeX format. Additionally, the Pillow (PIL) library will be used for image manipulation to demonstrate practical applications of these matrix operations in digital image processing. By the end of this section, you’ll understand how to implement matrix operations and apply them to real-world problems such as image blending, sharpening, filtering, and solving systems of equations.\n\n\n1.10.2 Introduction to SymPy\nSymPy is a powerful Python library designed for symbolic mathematics. It provides tools for algebraic operations, equation solving, and matrix handling in a symbolic form. This makes it ideal for educational purposes and theoretical work where exact results are needed.\n\n1.10.2.1 Key Matrix Functions in SymPy\n\nMatrix Addition: Adds two matrices element-wise.\nMatrix Subtraction: Subtracts one matrix from another element-wise.\nMatrix Multiplication: Multiplies two matrices using the dot product.\nMatrix Power: Raises a matrix to a given power using matrix multiplication.\n\n\nExample 1: Matrix Addition\n\nPseudocode\nFUNCTION matrix_add():\n    # Define matrices A and B\n    A = [[1, 2], [3, 4]]\n    B = [[5, 6], [7, 8]]\n\n    # Check if matrices A and B have the same dimensions\n    if dimensions_of(A) != dimensions_of(B):\n         raise ValueError(\"Matrices must have the same dimensions\")\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(len(A[0]))] for _ in range(len(A))]\n\n    # Add corresponding elements from A and B\n    for i in range(len(A)):\n        for j in range(len(A[0])):\n            result[i][j] = A[i][j] + B[i][j]\n\n# Return the result matrix\n    return result\nENDFUNCTION\nPython implementation of the above pseudocode is given below:\n\nimport sympy as sy\nsy.init_printing()\n# Define matrices A and B\nA = sy.Matrix([[1, 2], [3, 4]])\nB = sy.Matrix([[5, 6], [7, 8]])\n\n# Add matrices\nC = A + B\n\n# Print the result in symbolic form\nprint(\"Matrix Addition Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\n#latex_code = sy.latex(C)\n#print(\"LaTeX Code for Addition Result:\")\n#print(latex_code)\n\nMatrix Addition Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}6 & 8\\\\10 & 12\\end{matrix}\\right]\\)\n\n\n\nExample 2: Matrix Subtraction\n\nPseudocode:\n# Define matrices A and B\nA = [[5, 6], [7, 8]]\nB = [[1, 2], [3, 4]]\n\n# Check if matrices A and B have the same dimensions\nif dimensions_of(A) != dimensions_of(B):\n    raise ValueError(\"Matrices must have the same dimensions\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(A[0]))] for _ in range(len(A))]\n\n# Subtract corresponding elements from A and B\nfor i in range(len(A)):\n    for j in range(len(A[0])):\n        result[i][j] = A[i][j] - B[i][j]\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is given below:\n\nfrom sympy import Matrix\n# Define matrices A and B\nA = Matrix([[5, 6], [7, 8]])\nB = Matrix([[1, 2], [3, 4]])\n\n# Subtract matrices\nC = A - B\n\n# Print the result in symbolic form\nprint(\"Matrix Subtraction Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(C)\nprint(\"LaTeX Code for Subtraction Result:\")\nprint(latex_code)\n\nMatrix Subtraction Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}4 & 4\\\\4 & 4\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Subtraction Result:\n\\left[\\begin{matrix}4 & 4\\\\4 & 4\\end{matrix}\\right]\n\n\n\nExample 3: Matrix Multiplication\n\nPseudocode:\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Check if the number of columns in A equals the number of rows in B\nif len(A[0]) != len(B):\n    raise ValueError(\"Number of columns in A must equal number of rows in B\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(B[0]))] for _ in range(len(A))]\n\n# Multiply matrices A and B\nfor i in range(len(A)):\n    for j in range(len(B[0])):\n        for k in range(len(B)):\n            result[i][j] += A[i][k] * B[k][j]\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is given below:\n\n# Define matrices A and B\nA = Matrix([[1, 2], [3, 4]])\nB = Matrix([[5, 6], [7, 8]])\n\n# Multiply matrices\nM = A * B\n\n# Print the result in symbolic form\nprint(\"Matrix Multiplication Result:\")\ndisplay(M)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(M)\nprint(\"LaTeX Code for Multiplication Result:\")\nprint(latex_code)\n\nMatrix Multiplication Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Multiplication Result:\n\\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\n\n\n\nExample 3: Matrix Multiplication\n\nPseudocode:\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Check if the number of columns in A equals the number of rows in B\nif len(A[0]) != len(B):\n    raise ValueError(\"Number of columns in A must equal number of rows in B\")\n\n# Initialize result matrix with zeros\nresult = [[0 for _ in range(len(B[0]))] for _ in range(len(A))]\n\n# Multiply matrices A and B\nfor i in range(len(A)):\n    for j in range(len(B[0])):\n        for k in range(len(B)):\n            result[i][j] += A[i][k] * B[k][j]\n\n# Return the result matrix\nreturn result\nPython code for implementing the above pseudocode is shown below:\n\n# Define matrices A and B\nA = Matrix([[1, 2], [3, 4]])\nB = Matrix([[5, 6], [7, 8]])\n\n# Multiply matrices\nM = A * B\n\n# Print the result in symbolic form\nprint(\"Matrix Multiplication Result:\")\ndisplay(M)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(M)\nprint(\"LaTeX Code for Multiplication Result:\")\nprint(latex_code)\n\nMatrix Multiplication Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Multiplication Result:\n\\left[\\begin{matrix}19 & 22\\\\43 & 50\\end{matrix}\\right]\n\n\n\nExample 4: Matrix Power\n\nPseudocode:\n# Define matrix A and power n\nA = [[1, 2], [3, 4]]\nn = 2\n\n# Initialize result matrix as identity matrix\nresult = identity_matrix_of(len(A))\n\n# Compute A raised to the power of n\nfor _ in range(n):\n    result = matrix_multiply(result, A)\n\n# Return the result matrix\nreturn result\nPython implementation of the above pseudocode is shown below:\n\n# Define matrix A\nA = Matrix([[1, 2], [3, 4]])\n\n# Compute matrix A raised to the power of 2\nn = 2\nC = A**n\n\n# Print the result in symbolic form\nprint(\"Matrix Power Result:\")\ndisplay(C)\n\n# Convert to LaTeX code for documentation or presentation\nlatex_code = sy.latex(C)\nprint(\"LaTeX Code for Power Result:\")\nprint(latex_code)\n\nMatrix Power Result:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}7 & 10\\\\15 & 22\\end{matrix}\\right]\\)\n\n\nLaTeX Code for Power Result:\n\\left[\\begin{matrix}7 & 10\\\\15 & 22\\end{matrix}\\right]\n\n\n\n\n1.10.2.2 Introduction to PIL for Image Manipulation\nThe PIL (Python Imaging Library), now known as Pillow, provides essential tools for opening, manipulating, and saving various image file formats. In this session, we will use Pillow to perform image operations such as resizing and blending to demonstrate the practical applications of these matrix operations in digital image processing.\nMatrix operations have significant applications in digital image processing. These operations can manipulate images in various ways, from blending to filtering. Below we will discuss how matrix addition, subtraction, and multiplication are used in real-time image processing tasks.\n\nMatrix Addition: Image Blending\n\nMatrix addition can be used to blend two images by adding their pixel values. This process can be straightforward or involve weighted blending.\nExample 1: Simple Image Blending\n\nimport numpy as np\nfrom PIL import Image\nimport urllib.request\nurllib.request.urlretrieve('http://lenna.org/len_top.jpg',\"input.jpg\")\nimg1 = Image.open(\"input.jpg\") #loading first image\n\nurllib.request.urlretrieve('https://www.keralatourism.org/images/destination/large/thekkekudi_cave_temple_in_pathanamthitta20131205062431_315_1.jpg',\"input2.jpg\")\n\nimg2 = Image.open(\"input2.jpg\")# loading second image\n\n# Resize second image to match the size of the first image\nimg2 = img2.resize(img1.size)\n# Convert images to numpy arrays\narr1 = np.array(img1)\narr2 = np.array(img2)\n\n# Add the images\nblended_arr = arr1 + arr2\n\n# Clip the values to be in the valid range [0, 255]\nblended_arr = np.clip(blended_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nblended_img = Image.fromarray(blended_arr)\n\n# Save or display the blended image\n#blended_img.save('blended_image.jpg')\n#blended_img.show()\n#blended_img #display the blended image\n\nThe input and output images are shown below:\n\nimg1\n\n\n\n\n\n\n\n\n\nimg2\n\n\n\n\n\n\n\n\n\nblended_img\n\n\n\n\n\n\n\n\n\nExample 2: Weighted Image Blending\n\n\n# Blend with weights\nalpha = 0.7\nblended_arr = alpha * arr1 + (1 - alpha) * arr2\n\n# Clip the values to be in the valid range [0, 255]\nblended_arr = np.clip(blended_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nblended_img = Image.fromarray(blended_arr)\n\n# Save or display the weighted blended image\n#blended_img.save('weighted_blended_image.jpg')\n#blended_img.show()\nblended_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.3 Matrix Subtraction: Image Sharpening\nMatrix subtraction can be used to sharpen images by subtracting a blurred version of the image from the original.\n\nExample 1: Sharpening by Subtracting Blurred Image\n\n\nfrom PIL import Image, ImageFilter\n# Convert image to grayscale for simplicity\nimg_gray = img1.convert('L')\narr = np.array(img_gray)\n\n# Apply Gaussian blur\nblurred_img = img_gray.filter(ImageFilter.GaussianBlur(radius=5))\nblurred_arr = np.array(blurred_img)\n\n# Sharpen the image by subtracting blurred image\nsharpened_arr = arr - blurred_arr\n\n# Clip the values to be in the valid range [0, 255]\nsharpened_arr = np.clip(sharpened_arr, 0, 255).astype(np.uint8)\n\n# Convert back to image\nsharpened_img = Image.fromarray(sharpened_arr)\n\n# Save or display the sharpened image\n#sharpened_img.save('sharpened_image.jpg')\n#sharpened_img.show()\nsharpened_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.4 Matrix Multiplication: Image Filtering (Convolution)\nMatrix multiplication is used in image filtering to apply convolution kernels for various effects.\n\nExample 1: Applying a Convolution Filter\n\n\n# Define a simple convolution kernel (e.g., edge detection)\nkernel = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n# Convert the image to grayscale for simplicity\nimg_gray = img1.convert('L')\narr = np.array(img_gray)\n\n# Apply convolution\nfiltered_arr = np.zeros_like(arr)\nfor i in range(1, arr.shape[0] - 1):\n    for j in range(1, arr.shape[1] - 1):\n        region = arr[i-1:i+2, j-1:j+2]\n        filtered_arr[i, j] = np.sum(region * kernel)\n\n# Convert back to image\nfiltered_img = Image.fromarray(np.clip(filtered_arr, 0, 255).astype(np.uint8))\n\n# Save or display the filtered image\n#filtered_img.save('filtered_image.jpg')\n#filtered_img.show()\nfiltered_img\n\n\n\n\n\n\n\n\n\nExample 2: Applying a Gaussian Blur Filter\n\n\n# Define a Gaussian blur filter\nblurred_img = img1.filter(ImageFilter.GaussianBlur(radius=5))\n\n# Save or display the blurred image\n#blurred_img.save('blurred_image.jpg')\n#blurred_img.show()\nblurred_img\n\n\n\n\n\n\n\n\n\n\n1.10.2.5 Solving Systems of Equations and Applications\nIntroduction\nSolving systems of linear equations is crucial in various image processing tasks, such as image transformation, camera calibration, and object detection. In this section, we will demonstrate how to solve systems of linear equations using Python and explore practical applications in image processing.\n\nExample 1: Solving a System of Equations\n\nConsider the system:\n\\[\\begin{cases}\n2x + 3y& = 13 \\\\\n4x - y &= 7\n\\end{cases}\\]\n\nPython Implementation\n\n\nfrom sympy import Matrix\n# Define the coefficient matrix and constant matrix\nA = Matrix([[2, 3], [4, -1]])\nB = Matrix([13, 7])\n# Solve the system of equations\nsolution = A.solve_least_squares(B)\n# Print the solution\nprint(\"Solution to the System of Equations:\")\nprint(solution)\n\nSolution to the System of Equations:\nMatrix([[17/7], [19/7]])",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#module-review",
    "href": "module_1.html#module-review",
    "title": "1  Python for Linear Algebra",
    "section": "1.11 Module review",
    "text": "1.11 Module review\n\nWrite Pseudocode for Matrix Addition.\n\nHint: Use nested loops to iterate over rows and columns and sum elements.\nPseudocode:\nInitialize C as an m x n zero matrix\nFor i = 1 to m:\n    For j = 1 to n:\n        C[i][j] = A[i][j] + B[i][j]\nEnd\nReturn C\n\nBlend Grayscale Images A and B Using Weighted Blending (α = 0.70).\n\nHint: Use \\(C[i][j] = \\alpha A[i][j] + (1 - \\alpha) B[i][j]\\).\n\nReduce a Matrix Into Row-Reduced Echelon Form and Find Its Row and Column Spaces.\n\nHint: Use Gaussian elimination to achieve RREF.\n\nWrite Pseudocode for Row-Reduced Echelon Form.\n\nHint: Use row operations to make pivots 1 and eliminate non-zero elements in pivot columns.\nPseudocode:\nFor each row i in A:\n    Normalize pivot to 1\n    For each other row j:\n        Subtract multiple of row i to make column i of row j zero\nEnd\n\nDifferentiate Between Array and Dictionary in Python With Examples.\n\nHint:\n\nArray: Indexed collection of elements.\nDictionary: Key-value pair mapping.\n\nExample:\narray = [1, 2, 3, 4]\ndictionary = {\"key1\": \"value1\", \"key2\": \"value2\"}\n\nWrite Pseudocode to Transpose a Matrix.\n\nHint: Swap rows with columns.\n\nPerform Scalar Multiplication of a Matrix With a Scalar k.\n\nHint: Multiply each element of \\(A\\) by \\(k\\).\n\nCreate a Random Grayscale Image and Perform Pixel-wise Addition With Another Image.\n\nHint: Use NumPy to generate and manipulate matrices.\n\nExtract the Diagonal Elements of a Matrix and Find Their Sum.\n\nHint: Diagonal elements are \\(A[i][i]\\) for all \\(i\\).\n\nDiscuss NumPy Arrays in Linear Algebra and How They Differ From Python Lists.\n\nHint: NumPy arrays support efficient numerical operations, unlike general-purpose lists.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_1.html#conclusion",
    "href": "module_1.html#conclusion",
    "title": "1  Python for Linear Algebra",
    "section": "1.12 Conclusion",
    "text": "1.12 Conclusion\nIn this chapter, we transitioned from understanding fundamental matrix operations to applying them in practical scenarios, specifically in the realm of image processing. We began by covering essential matrix operations such as addition, subtraction, multiplication, and determinant calculations, providing both pseudocode and detailed explanations. This foundational knowledge was then translated into Python code, demonstrating how to perform these operations computationally.\nWe further explored the application of these matrix operations to real-world image processing tasks. By applying techniques such as image blending, sharpening, filtering, and transformation, we illustrated how theoretical concepts can be used to manipulate and enhance digital images effectively. These practical examples highlighted the significance of matrix operations in solving complex image processing challenges.\nBy integrating theoretical understanding with practical implementation, this chapter reinforced how matrix operations form the backbone of many image processing techniques. This blend of theory and practice equips you with essential skills for tackling advanced problems and developing innovative solutions in the field of image processing and beyond.\n\n\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python for Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_2.html",
    "href": "module_2.html",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "",
    "text": "2.1 Introduction\nIn the first module, we established a solid foundation in matrix algebra by exploring pseudocode and implementing fundamental matrix operations using Python. We practiced key concepts such as matrix addition, subtraction, multiplication, and determinants through practical examples in image processing, leveraging the SymPy library for symbolic computation.\nAs we begin the second module, “Transforming Linear Algebra to Computational Language,” our focus will shift towards applying these concepts with greater depth and actionable insight. This module is designed to bridge the theoretical knowledge from matrix algebra with practical computational applications. You will learn to interpret and utilize matrix operations, solve systems of equations, and analyze the rank of matrices within a variety of real-world contexts.\nA new concept we will introduce is the Rank-Nullity Theorem, which provides a fundamental relationship between the rank of a matrix and the dimensions of its null space. This theorem is crucial for understanding the solution spaces of linear systems and the properties of linear transformations. By applying this theorem, you will be able to gain deeper insights into the structure of solutions and the behavior of matrix transformations.\nThis transition will not only reinforce your understanding of linear algebra but also enhance your ability to apply these concepts effectively in computational settings. Through engaging examples and practical exercises, you will gain valuable experience in transforming abstract mathematical principles into tangible solutions, setting a strong groundwork for advanced computational techniques.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_2.html#relearning-of-terms-and-operations-in-linear-algebra",
    "href": "module_2.html#relearning-of-terms-and-operations-in-linear-algebra",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "2.2 Relearning of Terms and Operations in Linear Algebra",
    "text": "2.2 Relearning of Terms and Operations in Linear Algebra\nIn this section, we will revisit fundamental matrix operations such as addition, subtraction, scaling, and more through practical examples. Our goal is to transform theoretical linear algebra into modern computational applications. We will demonstrate these concepts using Python, focusing on practical and industrial applications.\n\n2.2.1 Matrix Addition and Subtraction in Data Analysis\nMatrix addition and subtraction are fundamental operations that help in combining datasets and analyzing differences.\nSimple Example: Combining Quarterly Sales Data\nWe begin with quarterly sales data from different regions and combine them to get the total sales. The sales data is given in Table 2.1. A ar plot of the total sales is shown in Fig 2.1.\n\n\n\nTable 2.1: Quarterly Sales Data\n\n\n\n\n\nRegion\nQ1\nQ2\nQ3\nQ4\n\n\n\n\nA\n2500\n2800\n3100\n2900\n\n\nB\n1500\n1600\n1700\n1800\n\n\n\n\n\n\nFrom Scratch Python Implementation:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Quarterly sales data\nsales_region_a = np.array([2500, 2800, 3100, 2900])\nsales_region_b = np.array([1500, 1600, 1700, 1800])\n\n# Combine sales data\ntotal_sales = sales_region_a + sales_region_b\n\n# Visualization\nquarters = ['Q1', 'Q2', 'Q3', 'Q4']\nplt.bar(quarters, total_sales, color='skyblue')\nplt.xlabel('Quarter')\nplt.ylabel('Total Sales')\nplt.title('Combined Quarterly Sales Data for Regions A and B')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.1: Computing Total Sales using Numpy aggregation method\n\n\n\n\n\nIn the above Python code, we have performed the aggregation operation with the NumPy method. Same can be done in a more data analysis style using pandas inorder to handle tabular data meaningfully. In this approach, quarterly sales data of each region is stored as DataFrames(like an excel sheet). The we combine these two DataFrames into one. After that create a new row with index ‘Total’ and populate this row with sum of quarterly sales in Region A and Region B. Finally a bar plot is created using this ‘Total’ sales. Advantage of this approach is that we don’t need the matplotlib library to create visualizations!. The EDA using this approach is shown in Fig 2.2.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# DataFrames for quarterly sales data\ndf_a = pd.DataFrame({'Q1': [2500], 'Q2': [2800], 'Q3': [3100], 'Q4': [2900]}, index=['Region A'])\ndf_b = pd.DataFrame({'Q1': [1500], 'Q2': [1600], 'Q3': [1700], 'Q4': [1800]}, index=['Region B'])\n\n# Combine data\ndf_combined = df_a.add(df_b, fill_value=0)\ndf_combined.loc[\"Total\"] = df_combined.sum(axis=0)\n# Visualization\ndf_combined.loc[\"Total\"].plot(kind='bar', color=['green'])\nplt.xlabel('Quarter')\nplt.ylabel('Total Sales')\nplt.title('Combined Quarterly Sales Data for Regions A and B')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.2: Computation of Total Sales using Pandas method\n\n\n\n\n\nWe can extend this in to more advanced examples. Irrespective to the size of the data, for representation and aggregation tasks matrix models are best options and are used in industry as a standard. Let us consider an advanced example to analyse difference in stock prices. For this example we are using a simulated data. The python code for this simulation process is shown in Fig 2.3.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated observed and predicted stock prices\nobserved_prices = np.random.uniform(100, 200, size=(100, 5))\npredicted_prices = np.random.uniform(95, 210, size=(100, 5))\n\n# Calculate the difference matrix\nprice_differences = observed_prices - predicted_prices\n\n# Visualization\nplt.imshow(price_differences, cmap='coolwarm', aspect='auto')\nplt.colorbar()\nplt.title('Stock Price Differences')\nplt.xlabel('Stock Index')\nplt.ylabel('Day Index')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.3: Demonstration of Stock Price simulated from a Uniform Distribution\n\n\n\n\n\nAnother important matrix operation relevant to data analytics and Machine Learning application is scaling. This is considered as a statistical tool to make various features (attributes) in to same scale so as to avoid unnecessary misleading impact in data analysis and its intepretation. In Machine Learning context, this pre-processing stage is inevitable so as to make the model relevant and usable.\nSimple Example: Normalizing Employee Performance Data\n\n\n\nTable 2.2: Employee Performance Data\n\n\n\n\n\nEmployee\nMetric A\nMetric B\n\n\n\n\nX\n80\n700\n\n\nY\n90\n800\n\n\nZ\n100\n900\n\n\nA\n110\n1000\n\n\nB\n120\n1100\n\n\n\n\n\n\nUsing simple python code we can simulate the model for min-max scaling. The formula for min-max scaling is: \\[min_max(X)=\\dfrac{X-min(X)}{max(X)-min(X)}\\]\nFor example, while applying the min-max scaling in the first value of Metric A, the scaled value is \\[min_max(80)\\dfrac{80-80}{120-80}=0\\]\nSimilarly\n\\[min_max(100)\\dfrac{100-80}{120-80}=0.5\\]\nWhen we apply this formula to Metric A and Metric B, the scaled output from Table 2.2 will be as follows:\n\n\n\nTable 2.3: Employee Performance Data\n\n\n\n\n\nEmployee\nMetric A\nMetric B\n\n\n\n\nX\n0.00\n0.00\n\n\nY\n0.25\n0.25\n\n\nZ\n0.50\n0.50\n\n\nA\n0.75\n0.75\n\n\nB\n1.00\n1.00\n\n\n\n\n\n\nIt is interesting to look into the scaled data! In the orginal table (Table 2.2) it is looked like Metric B is superior. But from the scaled table (Table 2.3), it is clear that both the Metrics are representing same relative information. This will help us to identify the redundency in measure and so skip any one of the Metric before analysis!.\nThe same can be achieved through a matrix operation. The Python implementation of this scaling process is shown in Fig 2.4.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Employee performance data with varying scales\ndata = np.array([[80, 700], [90, 800], [100, 900], [110, 1000], [120, 1100]])\n\n# Manual scaling\nmin_vals = np.min(data, axis=0)\nmax_vals = np.max(data, axis=0)\nscaled_data = (data - min_vals) / (max_vals - min_vals)\n\n# Visualization\nplt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(data, cmap='viridis')\nplt.title('Original Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(scaled_data, cmap='viridis')\nplt.title('Scaled Data')\nplt.colorbar()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2.4: Total sales using pandas method\n\n\n\n\n\nFrom the first sub plot, it is clear that there is a significant difference in the distributions (Metric A and Metric B values). But the second sub plot shows that both the distributions have same pattern and the values ranges between 0 and 1. In short the visualization is more appealing and self explanatory in this case.\n\n\n\n\n\n\nNote\n\n\n\nThe min-max scaling method will confine the feature values (attributes) into the range \\([0,1]\\). So in effect all the features are scaled proportionally to the data spectrum.\n\n\nSimilarly, we can use the standard scaling (transformation to normal distribution) using the transformation \\(\\dfrac{x-\\bar{x}}{\\sigma}\\). Scaling table is given as a practice task to the reader. The python code for this operation is shown in Fig 2.5.\n\n# Standard scaling from scratch\ndef standard_scaling(data):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    scaled_data = (data - mean) / std\n    return scaled_data\n\n# Apply standard scaling\nscaled_data_scratch = standard_scaling(data)\n\nprint(\"Standard Scaled Data (from scratch):\\n\", scaled_data_scratch)\n\n# Visualization\nplt.figure(figsize=(6, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(data, cmap='viridis')\nplt.title('Original Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(scaled_data_scratch, cmap='viridis')\nplt.title('Scaled Data')\nplt.colorbar()\n\nplt.show()\n\nStandard Scaled Data (from scratch):\n [[-1.41421356 -1.41421356]\n [-0.70710678 -0.70710678]\n [ 0.          0.        ]\n [ 0.70710678  0.70710678]\n [ 1.41421356  1.41421356]]\n\n\n\n\n\n\n\n\nFigure 2.5: Min-max scaling using basic python\n\n\n\n\n\nTo understand the effect of standard scaling, let us consider Fig 2.6. This plot create the frequency distribution of the data as a histogram along with the density function. From the first sub-plot, it is clear that the distribution has multiple modes (peaks). When we apply the standard scaling, the distribution become un-modal(only one peek). This is demonstrated in the second sub-plot.\n\n# Standard scaling from scratch\nimport seaborn as sns\n# Create plots\nplt.figure(figsize=(6, 5))\n\n# Plot for original data\nplt.subplot(1, 2, 1)\nsns.histplot(data, kde=True, bins=10, palette=\"viridis\")\nplt.title('Original Data Distribution')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Plot for standard scaled data\nplt.subplot(1, 2, 2)\nsns.histplot(scaled_data_scratch, kde=True, bins=10, palette=\"viridis\")\nplt.title('Standard Scaled Data Distribution')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.6: Impact of standard scaling on the distribution\n\n\n\n\n\nA scatter plot showing the compare the impact of scaling on the given distribution is shown in Fig 2.7.\n\n# Plot original and scaled data\nplt.figure(figsize=(6, 5))\n\n# Original Data\nplt.subplot(1, 3, 1)\nplt.scatter(data[:, 0], data[:, 1], color='blue')\nplt.title('Original Data')\nplt.xlabel('Metric A')\nplt.ylabel('Metric B')\n\n# Standard Scaled Data\nplt.subplot(1, 3, 2)\nplt.scatter(scaled_data_scratch[:, 0], scaled_data_scratch[:, 1], color='green')\nplt.title('Standard Scaled Data')\nplt.xlabel('Metric A (Standard Scaled)')\nplt.ylabel('Metric B (Standard Scaled)')\n\n# Min-Max Scaled Data\nplt.subplot(1, 3, 3)\nplt.scatter(scaled_data[:, 0], scaled_data[:, 1], color='red')\nplt.title('Min-Max Scaled Data')\nplt.xlabel('Metric A (Min-Max Scaled)')\nplt.ylabel('Metric B (Min-Max Scaled)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.7: Comparison of impact of scaling on the distribution\n\n\n\n\n\nFrom the Fig 2.7, it is clear that the scaling does not affect the pattern of the data, instead it just scale the distribution proportionally!\nWe can use the scikit-learn library for do the same thing in a very simple handy approach. The python code for this job is shown below.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Min-max scaling using sklearn\nscaler = MinMaxScaler()\nmin_max_scaled_data_sklearn = scaler.fit_transform(data)\n\nprint(\"Min-Max Scaled Data (using sklearn):\\n\", min_max_scaled_data_sklearn)\n\nMin-Max Scaled Data (using sklearn):\n [[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [0.75 0.75]\n [1.   1.  ]]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standard scaling using sklearn\nscaler = StandardScaler()\nscaled_data_sklearn = scaler.fit_transform(data)\n\nprint(\"Standard Scaled Data (using sklearn):\\n\", scaled_data_sklearn)\n\nStandard Scaled Data (using sklearn):\n [[-1.41421356 -1.41421356]\n [-0.70710678 -0.70710678]\n [ 0.          0.        ]\n [ 0.70710678  0.70710678]\n [ 1.41421356  1.41421356]]\n\n\nA scatter plot showing the impact on scaling is shown in Fig 2.8. This plot compare the mmin-max and standard-scaling.\n\n# Plot original and scaled data\nplt.figure(figsize=(6, 5))\n\n# Original Data\nplt.subplot(1, 3, 1)\nplt.scatter(data[:, 0], data[:, 1], color='blue')\nplt.title('Original Data')\nplt.xlabel('Metric A')\nplt.ylabel('Metric B')\n\n# Standard Scaled Data\nplt.subplot(1, 3, 2)\nplt.scatter(scaled_data_sklearn[:, 0], scaled_data_sklearn[:, 1], color='green')\nplt.title('Standard Scaled Data')\nplt.xlabel('Metric A (Standard Scaled)')\nplt.ylabel('Metric B (Standard Scaled)')\n\n# Min-Max Scaled Data\nplt.subplot(1, 3, 3)\nplt.scatter(min_max_scaled_data_sklearn[:, 0], min_max_scaled_data_sklearn[:, 1], color='red')\nplt.title('Min-Max Scaled Data')\nplt.xlabel('Metric A (Min-Max Scaled)')\nplt.ylabel('Metric B (Min-Max Scaled)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2.8: Camparison of Min-max and standard scalings with original data\n\n\n\n\n\n\n\n2.2.2 More on Matrix Product and its Applications\nIn the first module of our course, we introduced matrix products as scalar projections, focusing on how matrices interact through basic operations. In this section, we will expand on this by exploring different types of matrix products that have practical importance in various fields. One such product is the Hadamard product, which is particularly useful in applications ranging from image processing to neural networks and statistical analysis. We will cover the definition, properties, and examples of the Hadamard product, and then delve into practical applications with simulated data.\n\n2.2.2.1 Hadamard Product\nThe Hadamard product (or element-wise product) of two matrices is a binary operation that combines two matrices of the same dimensions to produce another matrix of the same dimensions, where each element is the product of corresponding elements in the original matrices.\n\n\n\n\n\n\nDefinition (Hadamard Product):\n\n\n\nFor two matrices \\(A\\) and \\(B\\) of the same dimension \\(m \\times n\\), the Hadamard product \\(A \\circ B\\) is defined as:\n\\[(A \\circ B)_{ij} = A_{ij} \\cdot B_{ij}\\]\nwhere \\(\\cdot\\) denotes element-wise multiplication.\n\n\n\n\n\n\n\n\nProperties of Hadamard Product\n\n\n\n\nCommutativity: \\[A \\circ B = B \\circ A\\]\nAssociativity: \\[(A \\circ B) \\circ C = A \\circ (B \\circ C)\\]\nDistributivity: \\[A \\circ (B + C) = (A \\circ B) + (A \\circ C)\\]\n\n\n\nSome simple examples to demonstrate the Hadamard product is given below.\nExample 1: Basic Hadamard Product\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 & 6 \\\\7 & 8\\end{pmatrix}\\]\nThe Hadamard product \\(A \\circ B\\) is:\n\\[A \\circ B = \\begin{pmatrix}1 \\cdot 5 & 2 \\cdot 6 \\\\3 \\cdot 7 & 4 \\cdot 8\\end{pmatrix} = \\begin{pmatrix}5 & 12 \\\\21 & 32\\end{pmatrix}\\]\nExample 2: Hadamard Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{pmatrix}, \\quad B = \\begin{pmatrix}9 & 8 & 7 \\\\6 & 5 & 4 \\\\3 & 2 & 1\\end{pmatrix}\\]\nThe Hadamard product \\(A \\circ B\\) is:\n\\[A \\circ B = \\begin{pmatrix}1 \\cdot 9 & 2 \\cdot 8 & 3 \\cdot 7 \\\\4 \\cdot 6 & 5 \\cdot 5 & 6 \\cdot 4 \\\\7 \\cdot 3 & 8 \\cdot  & 9 \\cdot 1\\end{pmatrix} = \\begin{pmatrix}9 & 16 & 21 \\\\24 & 25 & 24 \\\\21 & 16 & 9\\end{pmatrix}\\]\nIn the following code chunks the computational process of Hadamard product is implemented in Python. Here both the from the scratch and use of external module versions are included.\n1. Compute Hadamard Product from Scratch (without Libraries)\nHere’s how you can compute the Hadamard product manually:\n\n# Define matrices A and B\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8, 9], [10, 11, 12]]\n\n# Function to compute Hadamard product\ndef hadamard_product(A, B):\n    # Get the number of rows and columns\n    num_rows = len(A)\n    num_cols = len(A[0])\n    \n    # Initialize the result matrix\n    result = [[0]*num_cols for _ in range(num_rows)]\n    \n    # Compute the Hadamard product\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result[i][j] = A[i][j] * B[i][j]\n    \n    return result\n\n# Compute Hadamard product\nhadamard_product_result = hadamard_product(A, B)\n\n# Display result\nprint(\"Hadamard Product (From Scratch):\")\nfor row in hadamard_product_result:\n    print(row)\n\nHadamard Product (From Scratch):\n[7, 16, 27]\n[40, 55, 72]\n\n\n2. Compute Hadamard Product Using SymPy\nHere’s how to compute the Hadamard product using SymPy:\n\nimport sympy as sp\n\n# Define matrices A and B\nA = sp.Matrix([[1, 2, 3], [4, 5, 6]])\nB = sp.Matrix([[7, 8, 9], [10, 11, 12]])\n\n# Compute Hadamard product using SymPy\nHadamard_product_sympy = A.multiply_elementwise(B)\n\n# Display result\nprint(\"Hadamard Product (Using SymPy):\")\nprint(Hadamard_product_sympy)\n\nHadamard Product (Using SymPy):\nMatrix([[7, 16, 27], [40, 55, 72]])\n\n\nPractical Applications\nApplication 1: Image Masking\nThe Hadamard product can be used for image masking. Here’s how you can apply a mask to an image and visualize it as shown in Fig 2.9.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulated large image (2D array) using NumPy\nimage = np.random.rand(100, 100)\n\n# Simulated mask (binary matrix) using NumPy\nmask = np.random.randint(0, 2, size=(100, 100))\n\n# Compute Hadamard product\nmasked_image = image * mask\n\n# Plot original image and masked image\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].imshow(image, cmap='gray')\nax[0].set_title('Original Image')\nax[1].imshow(masked_image, cmap='gray')\nax[1].set_title('Masked Image')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.9: Demonstration of Masking in DIP using Hadamard Product\n\n\n\n\n\nApplication 2: Element-wise Scaling in Neural Networks\nThe Hadamard product can be used for dropout1 in neural networks. A simple simulated example is given below.\n\n# Simulated large activations (2D array) using NumPy\nactivations = np.random.rand(100, 100)\n\n# Simulated dropout mask (binary matrix) using NumPy\ndropout_mask = np.random.randint(0, 2, size=(100, 100))\n\n# Apply dropout\ndropped_activations = activations * dropout_mask\n\n# Display results\nprint(\"Original Activations:\")\nprint(activations)\nprint(\"\\nDropout Mask:\")\nprint(dropout_mask)\nprint(\"\\nDropped Activations:\")\nprint(dropped_activations)\n\nOriginal Activations:\n[[0.88781917 0.77973735 0.71502043 ... 0.70927738 0.68343791 0.85490206]\n [0.94091147 0.78986199 0.02763739 ... 0.37259214 0.93203977 0.1555421 ]\n [0.54824772 0.29815948 0.1364929  ... 0.71099116 0.76634636 0.32922721]\n ...\n [0.97119387 0.79516013 0.5758719  ... 0.19004946 0.68772138 0.46923181]\n [0.71468272 0.40448927 0.80122613 ... 0.01444955 0.9065674  0.87750534]\n [0.0297411  0.48879943 0.96121465 ... 0.91907444 0.13853278 0.86988397]]\n\nDropout Mask:\n[[0 0 0 ... 1 1 0]\n [0 0 0 ... 1 0 1]\n [1 0 0 ... 1 0 1]\n ...\n [0 0 0 ... 1 0 0]\n [1 1 1 ... 1 1 0]\n [1 1 0 ... 1 1 1]]\n\nDropped Activations:\n[[0.         0.         0.         ... 0.70927738 0.68343791 0.        ]\n [0.         0.         0.         ... 0.37259214 0.         0.1555421 ]\n [0.54824772 0.         0.         ... 0.71099116 0.         0.32922721]\n ...\n [0.         0.         0.         ... 0.19004946 0.         0.        ]\n [0.71468272 0.40448927 0.80122613 ... 0.01444955 0.9065674  0.        ]\n [0.0297411  0.48879943 0.         ... 0.91907444 0.13853278 0.86988397]]\n\n\nApplication 3: Statistical Data Analysis\nIn statistics, the Hadamard product can be applied to scale covariance matrices. Here’s how we can compute the covariance matrix using matrix operations and apply scaling. Following Python code demonstrate this.\n\nimport sympy as sp\nimport numpy as np\n\n# Simulated large dataset (2D array) using NumPy\ndata = np.random.rand(100, 10)\n\n# Compute the mean of each column\nmean = np.mean(data, axis=0)\n\n# Center the data\ncentered_data = data - mean\n\n# Compute the covariance matrix using matrix product operation\ncov_matrix = (centered_data.T @ centered_data) / (centered_data.shape[0] - 1)\ncov_matrix_sympy = sp.Matrix(cov_matrix)\n\n# Simulated scaling factors (2D array) using SymPy Matrix\nscaling_factors = sp.Matrix(np.random.rand(10, 10))\n\n# Compute Hadamard product\nscaled_cov_matrix = cov_matrix_sympy.multiply(scaling_factors)\n\n# Display results\nprint(\"Covariance Matrix:\")\nprint(cov_matrix_sympy)\nprint(\"\\nScaling Factors:\")\nprint(scaling_factors)\nprint(\"\\nScaled Covariance Matrix:\")\nprint(scaled_cov_matrix)\n\nCovariance Matrix:\nMatrix([[0.0895454670621226, 0.00506673858728506, -0.0122617059880387, 0.00771501287103439, -0.00922404919582032, 0.000350197703069086, -0.00707483044727527, -0.0108974998182029, -0.000934122190049301, -0.00592999599260387], [0.00506673858728506, 0.0905833721897159, -0.0150116203876558, 0.00118608880027435, -0.00668429242872895, -0.00491351250353489, 0.000118546020205811, -0.00565669313019322, -0.0224073453710028, -0.00601602211668819], [-0.0122617059880387, -0.0150116203876558, 0.0769366938885529, -0.00142884352389254, -0.0140285438824249, 0.00658019261621183, 0.000861216785283437, -0.00571432869660984, 0.00208896513000880, 0.000870835847591384], [0.00771501287103439, 0.00118608880027435, -0.00142884352389254, 0.0856271795340780, -0.0114351618913364, -0.00170661881644738, -0.00939647381768544, 0.000616620069693271, -0.00316989435836717, -0.00892443302693954], [-0.00922404919582032, -0.00668429242872895, -0.0140285438824249, -0.0114351618913364, 0.0742695260866906, -0.00184331870216371, -0.00453626122608065, 0.000323943839837690, 0.000107976754713581, 0.000619297895839785], [0.000350197703069086, -0.00491351250353489, 0.00658019261621183, -0.00170661881644738, -0.00184331870216371, 0.0761802225116875, 0.0142527182683454, 0.00156337897472686, 0.00395395287690089, -0.0115027193730288], [-0.00707483044727527, 0.000118546020205811, 0.000861216785283437, -0.00939647381768544, -0.00453626122608065, 0.0142527182683454, 0.0787847561411008, -0.00831735470601343, 0.00652890458178006, 0.000649680370349564], [-0.0108974998182029, -0.00565669313019322, -0.00571432869660984, 0.000616620069693271, 0.000323943839837690, 0.00156337897472686, -0.00831735470601343, 0.0739131247982590, -0.00294751018372592, 0.00242476691658650], [-0.000934122190049301, -0.0224073453710028, 0.00208896513000880, -0.00316989435836717, 0.000107976754713581, 0.00395395287690089, 0.00652890458178006, -0.00294751018372592, 0.0804656607634648, 0.00642162813375892], [-0.00592999599260387, -0.00601602211668819, 0.000870835847591384, -0.00892443302693954, 0.000619297895839785, -0.0115027193730288, 0.000649680370349564, 0.00242476691658650, 0.00642162813375892, 0.0829132203083090]])\n\nScaling Factors:\nMatrix([[0.149123887379517, 0.839740138450937, 0.129503977616364, 0.0109657646421040, 0.432494423212665, 0.895326474547841, 0.704892289035451, 0.0183953051871907, 0.431246543610665, 0.329092427879570], [0.376185549579542, 0.501620665516177, 0.701979186176774, 0.779773871771837, 0.626299464333463, 0.263659955678404, 0.273596720915879, 0.435331075985775, 0.951680455423075, 0.161197927647610], [0.448314517486219, 0.341910042241392, 0.368143194620939, 0.880070017974792, 0.597818706899225, 0.354104376045155, 0.453205814931620, 0.0926402511295402, 0.338920659439390, 0.982517591496921], [0.515941955955973, 0.633595398275439, 0.175989378535222, 0.784228473236970, 0.0430601467019215, 0.313871072963456, 0.362233443374315, 0.937393139859576, 0.987462157109354, 0.628023550707556], [0.160792907859948, 0.579926611113712, 0.912188247124335, 0.846931701784238, 0.220414636870218, 0.468135688804971, 0.845694779009750, 0.332472526862352, 0.659289623161859, 0.168345512200029], [0.355614811880808, 0.0286750534618246, 0.126120553152945, 0.491792437685239, 0.222803085003053, 0.867230876407518, 0.396542667981172, 0.904153558049538, 0.0644066311312814, 0.950601142932153], [0.162987712704365, 0.435659682528017, 0.471933755294777, 0.902545515325877, 0.0699945724163668, 0.647620516450612, 0.318311611900999, 0.655519706238833, 0.00316708093003648, 0.852768446745883], [0.557155489278771, 0.792760496798053, 0.0557192199554101, 0.0382174177457184, 0.314119479055328, 0.677950115468161, 0.856343643329520, 0.299160987761560, 0.738330549899147, 0.770758625990662], [0.381395231406921, 0.400807056560292, 0.762669906851430, 0.571474653181100, 0.897072700431928, 0.0743163300153301, 0.698695102074031, 0.466709009635721, 0.836161404832459, 0.0732169306450695], [0.125691461686314, 0.190310473410272, 0.438140714307595, 0.413228670473791, 0.913252713427320, 0.114646761745664, 0.133887380782013, 0.657899041706242, 0.400161815091850, 0.987332380237857]])\n\nScaled Covariance Matrix:\nMatrix([[0.00405784192229298, 0.0598688014114624, -0.00362961729464618, -0.0172339995724411, 0.0227761756896027, 0.0628544726857378, 0.0410512260103151, -0.00503621238116517, 0.0296194636846890, 0.00150761713775161], [0.0134570950176397, 0.0267361262654676, 0.0322245770330443, 0.0349310138734034, 0.0200686914347278, 0.00997275393856692, -0.00688789315993853, 0.0165530010957246, 0.0544318868843850, -0.0153703366699505], [0.0242260682650162, -0.00352540013284762, 0.00604247772145596, 0.0482164253112509, 0.0305363155228423, 0.00793703292181903, 0.00930258072785354, 0.000715764253507429, -0.00585579193934441, 0.0694755297304241], [0.0391705093701763, 0.0475837696621650, -0.00499896647531082, 0.0424226430997460, -0.00744613880828600, 0.0198308306042681, 0.0199122004218540, 0.0621214016632313, 0.0750799332455026, 0.0349735740377823], [-0.00523248381331920, 0.0183191482352124, 0.0526824685726377, 0.0316032358592661, -0.000648568898774185, 0.0119528487146185, 0.0422394401910237, 0.00550865623935975, 0.0230238194421043, -0.0173265597474343], [0.0303240212811610, 0.00695777020099620, 0.0114339132819667, 0.0469585606463308, 0.0120320916739818, 0.0752807188326304, 0.0370144754910133, 0.0692313787138596, -0.000386276247170358, 0.0791149949745773], [0.00964534879816163, 0.0167074845956531, 0.0374718059816298, 0.0713598130893816, 0.00865243640494629, 0.0472330245859614, 0.0164518368003511, 0.0552027748835966, -0.0141699408488428, 0.0673147391477986], [0.0336174112560659, 0.0409329578059715, -0.00787709805733854, -0.0133971295729263, 0.0109799316218392, 0.0332111325194693, 0.0482110399770199, 0.0157862343071553, 0.0419552575102537, 0.0438700200076079], [0.0230741314178871, 0.0208384172810467, 0.0520571700274864, 0.0383232551628729, 0.0651586022640240, 0.00542616648562704, 0.0513035666130804, 0.0362380450575160, 0.0438726329126100, 0.0154466110401476], [0.00297503992094544, 0.00723354443536439, 0.0345400457430315, 0.0224899427923483, 0.0736659852350530, -0.00702623767405771, 0.00516941392429119, 0.0374894354192591, 0.0232079881179277, 0.0662553441833370]])\n\n\n\n\n2.2.2.2 Practice Problems\nProblem 1: Basic Hadamard Product\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&6\\\\7&8\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot 5&2\\cdot 6\\\\3\\cdot7&4\\cdot 8 \\end{bmatrix}=\\begin{bmatrix}5&12\\\\21&32\\end{bmatrix}\\]\nProblem 2: Hadamard Product with Identity Matrix\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\] \\[I=\\begin{bmatrix}1&0&0\\\\0&1&0\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ I\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot1&2\\cdot 0&3\\cdot 0\\\\4\\cdot 0&5\\cdot 1&6\\cdot 0 \\end{bmatrix}= \\begin{bmatrix} 1&0&0\\\\0&5&0\\end{bmatrix}\\]\nProblem 3: Hadamard Product with Zero Matrix\nGiven matrices: \\[A=\\begin{bmatrix}3&4\\\\5&6\\end{bmatrix}\\] \\[Z=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ Z\\).\nSolution:\n\\[C=\\begin{bmatrix}3\\cdot 0&4\\cdot 0\\\\ 5\\cdot 0&6\\cdot 0 \\end{bmatrix}=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}\\]\nProblem 4: Hadamard Product of Two Identity Matrices\nGiven identity matrices: \\[I_2=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[I_3=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}\\]\nFind the Hadamard product \\(C=I_2\\circ I_3\\) (extend \\(I_2\\) to match dimensions of \\(I_3\\)).\nSolution:\nExtend \\(I_2\\) to \\(I_3\\): \\[I_2=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}\\]\n\\[C=\\begin{bmatrix}1\\cdot 1&0\\cdot 0&0\\cdot 0\\\\0\\cdot 0&1\\cdot 1&0\\cdot 0\\\\0\\cdot 0&0\\cdot 0&0\\cdot 1\\end{bmatrix}=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}\\]\nProblem 5: Hadamard Product with Random Matrices\nGiven random matrices: \\[A=\\begin{bmatrix}2&3\\\\1&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&5\\\\6&2\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}2\\cdot 0&3\\cdot 5\\\\1\\cdot 6&4\\cdot 2\\end{bmatrix}=\\begin{bmatrix}0&15\\\\6&8\\end{bmatrix}\\]\nProblem 6: Hadamard Product of 3x3 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\\\7&8&9\\end{bmatrix}\\] \\[B=\\begin{bmatrix}9&8&7\\\\6&5&4\\\\3&2&1\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}1\\cdot 9&2\\cdot 8&3\\cdot 7\\\\4\\cdot 6&5\\cdot 5&6\\cdot 4\\\\7\\cdot 3&8\\cdot 2&9\\cdot 1\\end{bmatrix}=\\begin{bmatrix}9&16&21\\\\24&25&24\\\\21&16&9\\end{bmatrix}\\]\nProblem 7: Hadamard Product of Column Vectors\nGiven column vectors: \\[u=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[v=\\begin{bmatrix}5\\\\6\\end{bmatrix}\\]\nFind the Hadamard product \\(w=u\\circ v\\).\nSolution:\n\\[w=\\begin{bmatrix}2\\cdot 5\\\\3\\cdot 6\\end{bmatrix}=\\begin{bmatrix}10\\\\18\\end{bmatrix}\\]\nProblem 8: Hadamard Product with Non-Square Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\\\5&6\\end{bmatrix}\\] \\[B=\\begin{bmatrix}7&8\\\\9&10\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\) (extend \\(B\\) to match dimensions of \\(A\\)).\nSolution:\nExtend \\(B\\) to match dimensions of \\(A\\): \\[B=\\begin{bmatrix}7&8\\\\9&10\\\\7&8\\end{bmatrix}\\]\n\\[C=\\begin{bmatrix}1\\cdot 7&2\\cdot 8\\\\3\\cdot 9&4\\cdot 10\\\\5\\cdot 7&6\\cdot 8\\end{bmatrix}=\\begin{bmatrix}7&16\\\\27&40\\\\35&48\\end{bmatrix}\\]\nProblem 9: Hadamard Product in Image Processing\nGiven matrices representing image pixel values: \\[A=\\begin{bmatrix}10&20\\\\30&40\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0.5&1.5\\\\2.0&0.5\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}10\\cdot 0.5&20\\cdot 1.5\\\\30\\cdot 2.0&40\\cdot 0.5\\end{bmatrix}=\\begin{bmatrix}5&30\\\\60&20\\end{bmatrix}\\]\nProblem 10: Hadamard Product in Statistical Data\nGiven matrices representing two sets of statistical data:\n\\[A=\\begin{bmatrix}5&6&7\\\\8&9&10\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\]\nFind the Hadamard product \\(C=A\\circ B\\).\nSolution:\n\\[C=\\begin{bmatrix}5\\cdot 1&6\\cdot 2&7\\cdot 3\\\\8\\cdot 4&9\\cdot 5&10\\cdot 6\\end{bmatrix}=\\begin{bmatrix}5&12&21\\\\32&45&60\\end{bmatrix}\\]\n\n\n2.2.2.3 Inner Product of Matrices\nThe inner product of two matrices is a generalized extension of the dot product, where each matrix is treated as a vector in a high-dimensional space. For two matrices \\(A\\) and \\(B\\) of the same dimension \\(m \\times n\\), the inner product is defined as the sum of the element-wise products of the matrices.\n\n\n\n\n\n\nDefinition (Inner product)\n\n\n\nFor two matrices \\(A\\) and \\(B\\) of dimension \\(m \\times n\\), the inner product \\(\\langle A, B \\rangle\\) is given by:\n\\[\\langle A, B \\rangle = \\sum_{i=1}^{m} \\sum_{j=1}^{n} A_{ij} \\cdot B_{ij}\\]\nwhere \\(\\cdot\\) denotes element-wise multiplication.\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nCommutativity: \\[\\langle A, B \\rangle = \\langle B, A \\rangle\\]\nLinearity: \\[\\langle A + C, B \\rangle = \\langle A, B \\rangle + \\langle C, B \\rangle\\]\nPositive Definiteness: \\[\\langle A, A \\rangle \\geq 0\\] with equality if and only if \\(A\\) is a zero matrix.\n\n\n\nSome simple examples showing the mathematical process of calculating the inner product is given bellow.\nExample 1: Basic Inner Product\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 & 6 \\\\7 & 8\\end{pmatrix}\\]\nThe inner product \\(\\langle A, B \\rangle\\) is:\n\\[\\langle A, B \\rangle = 1 \\cdot 5 + 2 \\cdot 6 + 3 \\cdot 7 + 4 \\cdot 8 = 5 + 12 + 21 + 32 = 70\\]\nExample 2: Inner Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{pmatrix}, \\quad B = \\begin{pmatrix}9 & 8 & 7 \\\\6 & 5 & 4 \\\\3 & 2 & 1\\end{pmatrix}\\]\nThe inner product \\(\\langle A, B \\rangle\\) is calculated as: \\[\\begin{align*}\n\\langle A, B \\rangle &= 1 \\cdot 9 + 2 \\cdot 8 + 3 \\cdot 7 + 4 \\cdot 6 + 5 \\cdot 5 + 6 \\cdot 4 + 7 \\cdot 3 + 8 \\cdot 2 + 9 \\cdot 1\\\\\n&= 9 + 16 + 21 + 24 + 25 + 24 + 21 + 16 + 9\\\\\n&= 175\n\\end{align*}\\]\n\n\n2.2.2.4 Practice Problems\nProblem 1: Inner Product of 2x2 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&6\\\\7&8\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot5 + 2\\cdot6 + 3\\cdot7 + 4\\cdot8 \\\\\n&= 5 + 12 + 21 + 32 \\\\\n&= 70\n\\end{align*}\\]\n\nProblem 2: Inner Product of 3x3 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&0&2\\\\3&4&5\\\\6&7&8\\end{bmatrix}\\] \\[B=\\begin{bmatrix}8&7&6\\\\5&4&3\\\\2&1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot8 + 0\\cdot7 + 2\\cdot6 + \\\\\n&\\quad 3\\cdot5 + 4\\cdot4 + 5\\cdot3 + \\\\\n&\\quad 6\\cdot2 + 7\\cdot1 + 8\\cdot0 \\\\\n&= 8 + 0 + 12 + 15 + 16 + 15 + 12 + 7 + 0 \\\\\n&= 85\n\\end{align*}\\]\n\nProblem 3: Inner Product of Diagonal Matrices\nGiven diagonal matrices: \\[A=\\begin{bmatrix}2&0&0\\\\0&3&0\\\\0&0&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&0&0\\\\0&6&0\\\\0&0&7\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 2\\cdot5 + 0\\cdot0 + 0\\cdot0 + \\\\\n&\\quad 0\\cdot0 + 3\\cdot6 + 0\\cdot0 + \\\\\n&\\quad 0\\cdot0 + 0\\cdot0 + 4\\cdot7 \\\\\n&= 10 + 0 + 0 + 0 + 18 + 0 + 0 + 0 + 28 \\\\\n&= 56\n\\end{align*}\\]\n\nProblem 4: Inner Product of Column Vectors\nGiven column vectors: \\[u=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\] \\[v=\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle u,v \\rangle &= \\sum_{i} u_i v_i \\\\\n&= 1\\cdot4 + 2\\cdot5 + 3\\cdot6 \\\\\n&= 4 + 10 + 18 \\\\\n&= 32\n\\end{align*}\\]\n\nProblem 5: Inner Product with Random Matrices\nGiven matrices: \\[A=\\begin{bmatrix}3&2\\\\1&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&7\\\\8&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 3\\cdot5 + 2\\cdot7 + \\\\\n&\\quad 1\\cdot8 + 4\\cdot6 \\\\\n&= 15 + 14 + 8 + 24 \\\\\n&= 61\n\\end{align*}\\]\n\nProblem 6: Inner Product of 2x3 and 3x2 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}\\] \\[B=\\begin{bmatrix}7&8\\\\9&10\\\\11&12\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot7 + 2\\cdot8 + 3\\cdot11 + \\\\\n&\\quad 4\\cdot9 + 5\\cdot10 + 6\\cdot12 \\\\\n&= 7 + 16 + 33 + 36 + 50 + 72 \\\\\n&= 214\n\\end{align*}\\]\n\nProblem 7: Inner Product with Transpose Operation\nGiven matrices: \\[A=\\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\\] \\[B=\\begin{bmatrix}6&7\\\\8&9\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 2\\cdot6 + 3\\cdot7 + \\\\\n&\\quad 4\\cdot8 + 5\\cdot9 \\\\\n&= 12 + 21 + 32 + 45 \\\\\n&= 110\n\\end{align*}\\]\n\nProblem 8: Inner Product of Symmetric Matrices\nGiven symmetric matrices: \\[A=\\begin{bmatrix}1&2\\\\2&3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&5\\\\5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot4 + 2\\cdot5 + \\\\\n&\\quad 2\\cdot5 + 3\\cdot6 \\\\\n&= 4 + 10 + 10 + 18 \\\\\n&= 42\n\\end{align*}\\]\n\nProblem 9: Inner Product with Complex Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1+i&2-i\\\\3+i&4-i\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5-i&6+i\\\\7-i&8+i\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} \\text{Re}(A_{ij} \\overline{B_{ij}}) \\\\\n&= (1+i)\\cdot(5+i) + (2-i)\\cdot(6-i) + \\\\\n&\\quad (3+i)\\cdot(7+i) + (4-i)\\cdot(8+i) \\\\\n&= (5+i+5i-i^2) + (12-i-6i+i^2) + \\\\\n&\\quad (21+i+7i-i^2) + (32+i-8i-i^2) \\\\\n&= 5+5 + 12 - 6 + 21 + 32 - 2 \\\\\n&= 62\n\\end{align*}\\]\n\nProblem 10: Inner Product of 4x4 Matrices\nGiven matrices: \\[A=\\begin{bmatrix}1&2&3&4\\\\5&6&7&8\\\\9&10&11&12\\\\13&14&15&16\\end{bmatrix}\\] \\[B=\\begin{bmatrix}16&15&14&13\\\\12&11&10&9\\\\8&7&6&5\\\\4&3&2&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\n\\langle A,B \\rangle &= \\sum_{i,j} A_{ij} B_{ij} \\\\\n&= 1\\cdot16 + 2\\cdot15 + 3\\cdot14 + 4\\cdot13 + \\\\\n&\\quad 5\\cdot12 + 6\\cdot11 + 7\\cdot10 + 8\\cdot9 + \\\\\n&\\quad 9\\cdot8 + 10\\cdot7 + 11\\cdot6 + 12\\cdot5 + \\\\\n&\\quad 13\\cdot4 + 14\\cdot3 + 15\\cdot2 + 16\\cdot1 \\\\\n&= 16 + 30 + 42 + 52 + 60 + 66 + 70 + 72 + \\\\\n&\\quad 72 + 70 + 66 + 60 + 52 + 42 + 30 + 16 \\\\\n&= 696\n\\end{align*}\\]\n\nNow let’s look into the computational part of inner product.\n\nCompute Inner Product from Scratch (without Libraries)\n\nHere’s how you can compute the inner product from the scratch:\n\n# Define matrices A and B\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8, 9], [10, 11, 12]]\n\n# Function to compute inner product\ndef inner_product(A, B):\n    # Get the number of rows and columns\n    num_rows = len(A)\n    num_cols = len(A[0])\n    \n    # Initialize the result\n    result = 0\n    \n    # Compute the inner product\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result += A[i][j] * B[i][j]\n    \n    return result\n\n# Compute inner product\ninner_product_result = inner_product(A, B)\n\n# Display result\nprint(\"Inner Product (From Scratch):\")\nprint(inner_product_result)\n\nInner Product (From Scratch):\n217\n\n\n\nCompute Inner Product Using NumPy\n\nHere’s how to compute the inner product using Numpy:\n\nimport numpy as np\n# Define matrices A and B\nA = np.array([[1, 2, 3], [4, 5, 6]])\nB = np.array([[7, 8, 9], [10, 11, 12]])\n# calculating innerproduct\ninner_product = (A*B).sum() # calculate element-wise product, then column sum\n\nprint(\"Inner Product (Using numpy):\")\nprint(inner_product)\n\nInner Product (Using numpy):\n217\n\n\nThe same operation can be done using SymPy functions as follows.\n\nimport sympy as sp\nimport numpy as np  \n# Define matrices A and B\nA = sp.Matrix([[1, 2, 3], [4, 5, 6]])\nB = sp.Matrix([[7, 8, 9], [10, 11, 12]])\n\n# Compute element-wise product\nelementwise_product = A.multiply_elementwise(B)\n\n# Calculate sum of each column\ninner_product_sympy = np.sum(elementwise_product)\n\n# Display result\nprint(\"Inner Product (Using SymPy):\")\nprint(inner_product_sympy)\n\nInner Product (Using SymPy):\n217\n\n\nA vector dot product (in Physics) can be calculated using SymPy .dot() function as shown below.\nLet \\(A=\\begin{pmatrix}1&2&3\\end{pmatrix}\\) and \\(B=\\begin{pmatrix}4&5&6\\end{pmatrix}\\), then the dot product, \\(A\\cdot B\\) is computed as:\n\nimport sympy as sp\nA=sp.Matrix([1,2,3])\nB=sp.Matrix([4,5,6])\ndisplay(A.dot(B)) # calculate fot product of A and B\n\n\\(\\displaystyle 32\\)\n\n\n\n\n\n\n\n\nA word of caution\n\n\n\nIn SymPy , sp.Matrix([1,2,3]) create a column vector. But np.array([1,2,3]) creates a row vector. So be careful while applying matrix/ dot product operations on these objects.\n\n\nThe same dot product using numpy object can be done as follows:\n\nimport numpy as np\nA=np.array([1,2,3])\nB=np.array([4,5,6])\ndisplay(A.dot(B.T))# dot() stands for dot product B.T represents the transpose of B\n\n32\n\n\nPractical Applications\nApplication 1: Signal Processing\nIn signal processing, the inner product can be used to measure the similarity between two signals. Here the most popular measure of similarity is the cosine similarity. This measure is defined as:\n\\[\\cos \\theta=\\dfrac{A\\cdot B}{||A|| ||B||}\\]\nNow consider two digital signals are given. It’s cosine similarity measure can be calculated with a simulated data as shown below.\n\nimport numpy as np\n\n# Simulated large signals (1D array) using NumPy\nsignal1 = np.sin(np.random.rand(1000))\nsignal2 = np.cos(np.random.rand(1000))\n\n# Compute inner product\ninner_product_signal = np.dot(signal1, signal2)\n#cosine_sim=np.dot(signal1,signal2)/(np.linalg.norm(signal1)*np.linalg.norm(signal2))\n# Display result\ncosine_sim=inner_product_signal/(np.sqrt(np.dot(signal1,signal1))*np.sqrt(np.dot(signal2,signal2)))\nprint(\"Inner Product (Using numpy):\")\nprint(inner_product_signal)\nprint(\"Similarity of signals:\")\nprint(cosine_sim)\n\nInner Product (Using numpy):\n378.9406441683205\nSimilarity of signals:\n0.8629828119851786\n\n\nApplication 2: Machine Learning - Feature Similarity\nIn machine learning, the inner product is used to calculate the similarity between feature vectors.\n\nimport numpy as np\n\n# Simulated feature vectors (2D array) using NumPy\nfeatures1 = np.random.rand(100, 10)\nfeatures2 = np.random.rand(100, 10)\n\n# Compute inner product for each feature vector\ninner_products = np.einsum('ij,ij-&gt;i', features1, features2) # use Einstien's sum\n\n# Display results\nprint(\"Inner Products of Feature Vectors:\")\ndisplay(inner_products)\n\nInner Products of Feature Vectors:\n\n\narray([1.33795551, 1.56103317, 2.74136672, 2.04157983, 2.37232844,\n       2.30206213, 2.28100634, 1.17661034, 3.78103353, 2.32334519,\n       3.92815891, 3.35723344, 1.84540924, 1.70788605, 2.4710701 ,\n       2.27722239, 3.06056746, 2.40251111, 1.72890383, 2.20233636,\n       2.24115145, 2.05834403, 3.07598518, 1.36374136, 2.81833329,\n       1.80234032, 1.79794474, 2.98610171, 2.67326551, 1.70339239,\n       2.67773707, 3.03836   , 1.82916869, 1.00941242, 3.32411584,\n       1.56909104, 3.39252218, 1.75253093, 1.83207694, 2.15936904,\n       2.38390749, 2.62191584, 2.39137787, 3.21028456, 2.380432  ,\n       2.65472476, 1.37678443, 1.85821376, 2.12913631, 2.82894278,\n       3.5891085 , 2.61747977, 4.01408508, 2.85332189, 1.34407553,\n       2.09105655, 2.9614739 , 1.82113305, 3.19695206, 2.3224156 ,\n       2.19092195, 2.85587474, 2.76218651, 3.22686962, 2.29729548,\n       2.67716591, 2.9457948 , 3.69226138, 1.82107748, 1.68417146,\n       2.75694576, 2.31294489, 3.18220069, 2.64239327, 1.96427551,\n       1.26679594, 2.6585543 , 1.9102915 , 4.54105009, 1.60408074,\n       2.39230988, 0.9478251 , 2.86937657, 1.74272519, 2.12532546,\n       2.796169  , 2.9303729 , 2.11877115, 1.3960586 , 3.42893538,\n       2.09574578, 2.68633814, 1.6306202 , 2.89090271, 3.46615548,\n       2.0377857 , 2.52254363, 2.10219333, 4.02372017, 1.26648502])\n\n\nApplication 3: Covariance Matrix in Statistics\nThe inner product can be used to compute covariance matrices for statistical data analysis. If \\(X\\) is a given distribution and \\(x=X-\\bar{X}\\). Then the covariance of \\(X\\) can be calculated as \\(cov(X)=\\dfrac{1}{n-1}(x\\cdot x^T)\\) 2. The python code a simulated data is shown below.\n\nimport sympy as sp\nimport numpy as np\n\n# Simulated large dataset (2D array) using NumPy\ndata = np.random.rand(100, 10)\n\n# Compute the mean of each column\nmean = np.mean(data, axis=0)\n\n# Center the data\ncentered_data = data - mean\n\n# Compute the covariance matrix using matrix product operation\ncov_matrix = (centered_data.T @ centered_data) / (centered_data.shape[0] - 1)\ncov_matrix_sympy = sp.Matrix(cov_matrix)\n\n# Display results\nprint(\"Covariance Matrix:\")\ndisplay(cov_matrix_sympy)\n\nCovariance Matrix:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0870130112716819 & -0.00307931788790388 & -0.00651573540534953 & 0.0063172994557697 & 0.00429782391249482 & -0.00952134941418706 & -0.0027888767193374 & -0.00453126588407532 & 0.00977084461907187 & 0.0036409372558165\\\\-0.00307931788790388 & 0.0840998521037939 & -0.00218495709502566 & -0.00893536074971389 & 0.0064419673126759 & -0.00930229979032778 & -0.0115974511443938 & 0.00829813761977678 & 0.00295293660041834 & 0.000170628965350151\\\\-0.00651573540534953 & -0.00218495709502566 & 0.0863197391845039 & 0.00848139148464137 & 0.0037577659389431 & 0.0112631242037696 & -0.00491110424999679 & -0.00805062497051461 & 0.0118515139237483 & 0.000852009000586381\\\\0.0063172994557697 & -0.00893536074971389 & 0.00848139148464137 & 0.094614258502454 & 0.0113153421715336 & -0.0070811462351222 & 0.00077920880282858 & -0.0108390371722834 & -0.0112588108181841 & -0.00154406329782345\\\\0.00429782391249482 & 0.0064419673126759 & 0.0037577659389431 & 0.0113153421715336 & 0.08396703329838 & -0.00795775176567254 & -0.00340605284173895 & 0.000597686721275771 & -0.0117708916340247 & -0.0166558707211244\\\\-0.00952134941418706 & -0.00930229979032778 & 0.0112631242037696 & -0.0070811462351222 & -0.00795775176567254 & 0.084814966020648 & 0.000506765873542602 & -0.00480286421638728 & -0.00169568578343561 & -0.00485139938953018\\\\-0.0027888767193374 & -0.0115974511443938 & -0.00491110424999679 & 0.00077920880282858 & -0.00340605284173895 & 0.000506765873542602 & 0.0814060026122759 & 0.00036621045183408 & 0.00335491740886199 & 0.0127124644077578\\\\-0.00453126588407532 & 0.00829813761977678 & -0.00805062497051461 & -0.0108390371722834 & 0.000597686721275771 & -0.00480286421638728 & 0.00036621045183408 & 0.082834744666752 & -0.013224174865905 & -0.00653601803997389\\\\0.00977084461907187 & 0.00295293660041834 & 0.0118515139237483 & -0.0112588108181841 & -0.0117708916340247 & -0.00169568578343561 & 0.00335491740886199 & -0.013224174865905 & 0.0840482312301643 & -0.00971217623030726\\\\0.0036409372558165 & 0.000170628965350151 & 0.000852009000586381 & -0.00154406329782345 & -0.0166558707211244 & -0.00485139938953018 & 0.0127124644077578 & -0.00653601803997389 & -0.00971217623030726 & 0.0878642260656276\\end{matrix}\\right]\\)\n\n\nThese examples demonstrate the use of inner product and dot product in various applications.\n\n\n2.2.2.5 Outer Product\nThe outer product of two vectors results in a matrix, and it is a way to combine these vectors into a higher-dimensional representation.\n\n\n\n\n\n\nDefinition (Outer Product)\n\n\n\nFor two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) of dimensions \\(m\\) and \\(n\\) respectively, the outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is an \\(m \\times n\\) matrix defined as:\n\\[(\\mathbf{u} \\otimes \\mathbf{v})_{ij} = u_i \\cdot v_j\\]\nwhere \\(\\cdot\\) denotes the outer product operation. In matrix notation, for two column vectors \\(u,v\\), \\[u\\otimes v=uv^T\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nLinearity: \\[(\\mathbf{u} + \\mathbf{w}) \\otimes \\mathbf{v} = (\\mathbf{u} \\otimes \\mathbf{v}) + (\\mathbf{w} \\otimes \\mathbf{v})\\]\nDistributivity: \\[\\mathbf{u} \\otimes (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} \\otimes \\mathbf{v}) + (\\mathbf{u} \\otimes \\mathbf{w})\\]\nAssociativity: \\[(\\mathbf{u} \\otimes \\mathbf{v}) \\otimes \\mathbf{w} = \\mathbf{u} \\otimes (\\mathbf{v} \\otimes \\mathbf{w})\\]\n\n\n\nSome simple examples of outer product is given below.\nExample 1: Basic Outer Product\nGiven vectors:\n\\[\\mathbf{u} = \\begin{pmatrix}1 \\\\2\\end{pmatrix}, \\quad\\mathbf{v} = \\begin{pmatrix}3 \\\\4 \\\\5\\end{pmatrix}\\]\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix}1 \\cdot 3 & 1 \\cdot 4 & 1 \\cdot 5 \\\\2 \\cdot 3 & 2 \\cdot 4 & 2 \\cdot 5\\end{pmatrix} = \\begin{pmatrix}3 & 4 & 5 \\\\6 & 8 & 10\\end{pmatrix}\\]\nExample 2: Outer Product with Larger Vectors\nGiven vectors: \\[\\mathbf{u} = \\begin{pmatrix}1 \\\\2 \\\\3\\end{pmatrix}, \\quad\\mathbf{v} = \\begin{pmatrix}4 \\\\5\\end{pmatrix}\\]\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix}1 \\cdot 4 & 1 \\cdot 5 \\\\2 \\cdot 4 & 2 \\cdot 5 \\\\3 \\cdot 4 & 3 \\cdot 5\\end{pmatrix} = \\begin{pmatrix}4 & 5 \\\\8 & 10 \\\\12 & 15\\end{pmatrix}\\]\n\n\n2.2.2.6 Practice Problems\nFind the outer product of A and B where A and B are given as follows:\nProblem 1:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\2\\end{bmatrix} \\otimes \\begin{bmatrix}3&4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 3 & 1 \\cdot 4 \\\\\n2 \\cdot 3 & 2 \\cdot 4\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 4 \\\\\n6 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 2:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}4&5&6\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 4 & 1 \\cdot 5 & 1 \\cdot 6 \\\\\n2 \\cdot 4 & 2 \\cdot 5 & 2 \\cdot 6 \\\\\n3 \\cdot 4 & 3 \\cdot 5 & 3 \\cdot 6\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n4 & 5 & 6 \\\\\n8 & 10 & 12 \\\\\n12 & 15 & 18\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 3:\nFind the outer product of: \\[A=\\begin{bmatrix}1&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\end{bmatrix} \\otimes \\begin{bmatrix}3\\\\4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 3 & 1 \\cdot 4 \\\\\n2 \\cdot 3 & 2 \\cdot 4\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 4 \\\\\n6 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 4:\nFind the outer product of: \\[A=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0\\\\1\\end{bmatrix} \\otimes \\begin{bmatrix}1&-1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot 1 & 0 \\cdot -1 \\\\\n1 \\cdot 1 & 1 \\cdot -1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n1 & -1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 5:\nFind the outer product of: \\[A=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}5&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}5&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot 5 & 2 \\cdot -2 \\\\\n3 \\cdot 5 & 3 \\cdot -2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n10 & -4 \\\\\n15 & -6\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 6:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&-1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} \\otimes \\begin{bmatrix}2&-1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot 2 & 1 \\cdot -1 & 1 \\cdot 0 \\\\\n0 \\cdot 2 & 0 \\cdot -1 & 0 \\cdot 0 \\\\\n1 \\cdot 2 & 1 \\cdot -1 & 1 \\cdot 0\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 & -1 & 0 \\\\\n0 & 0 & 0 \\\\\n2 & -1 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 7:\nFind the outer product of: \\[A=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&0\\\\3&-1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\otimes \\begin{bmatrix}2&0\\\\3&-1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2 & 3&0&-1 \\\\\n-2&-3&0&1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 8:\nFind the outer product of: \\[A=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-2&3\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}3\\\\4\\end{bmatrix} \\otimes \\begin{bmatrix}1&-2&3\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 \\cdot 1 & 3 \\cdot -2 & 3 \\cdot 3 \\\\\n4 \\cdot 1 & 4 \\cdot -2 & 4 \\cdot 3\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & -6 & 9 \\\\\n4 & -8 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 9:\nFind the outer product of: \\[A=\\begin{bmatrix}2\\\\3\\\\-1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\\\-1\\end{bmatrix} \\otimes \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot 4 & 2 \\cdot -2 \\\\\n3 \\cdot 4 & 3 \\cdot -2 \\\\\n-1 \\cdot 4 & -1 \\cdot -2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n8 & -4 \\\\\n12 & -6 \\\\\n-4 & 2\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 10:\nFind the outer product of: \\[A=\\begin{bmatrix}0\\\\5\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0\\\\5\\end{bmatrix} \\otimes \\begin{bmatrix}3&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot 3 & 0 \\cdot 1 \\\\\n5 \\cdot 3 & 5 \\cdot 1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n15 & 5\n\\end{bmatrix}\n\\end{align*}\\]\n\n1. Compute Outer Product of Vectors from Scratch (without Libraries)\nHere’s how you can compute the outer product manually:\n\n# Define vectors u and v\nu = [1, 2]\nv = [3, 4, 5]\n\n# Function to compute outer product\ndef outer_product(u, v):\n    # Initialize the result\n    result = [[a * b for b in v] for a in u]\n    return result\n\n# Compute outer product\nouter_product_result = outer_product(u, v)\n\n# Display result\nprint(\"Outer Product of Vectors (From Scratch):\")\nfor row in outer_product_result:\n    print(row)\n\nOuter Product of Vectors (From Scratch):\n[3, 4, 5]\n[6, 8, 10]\n\n\n2. Compute Outer Product of Vectors Using SymPy\nHere’s how to compute the outer product using SymPy:\n\nimport sympy as sp\n\n# Define vectors u and v\nu = sp.Matrix([1, 2])\nv = sp.Matrix([3, 4, 5])\n\n# Compute outer product using SymPy\nouter_product_sympy = u * v.T\n\n# Display result\nprint(\"Outer Product of Vectors (Using SymPy):\")\ndisplay(outer_product_sympy)\n\nOuter Product of Vectors (Using SymPy):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}3 & 4 & 5\\\\6 & 8 & 10\\end{matrix}\\right]\\)\n\n\nOuter Product of Matrices\nThe outer product of two matrices extends the concept from vectors to higher-dimensional tensors. For two matrices \\(A\\) and \\(B\\), the outer product results in a higher-dimensional tensor and is generally expressed as block matrices.\n\n\n\n\n\n\nDefinition (Outer Product of Matrices)\n\n\n\nFor two matrices \\(A\\) of dimension \\(m \\times p\\) and \\(B\\) of dimension \\(q \\times n\\), the outer product \\(A \\otimes B\\) results in a tensor of dimension \\(m \\times q \\times p \\times n\\). The entries of the tensor are given by:\n\\[(A \\otimes B)_{ijkl} = A_{ij} \\cdot B_{kl}\\]\nwhere \\(\\cdot\\) denotes the outer product operation.\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nLinearity: \\[(A + C) \\otimes B = (A \\otimes B) + (C \\otimes B)\\]\nDistributivity: \\[A \\otimes (B + D) = (A \\otimes B) + (A \\otimes D)\\]\nAssociativity:\n\n\\[(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)\\]\n\n\nHere are some simple examples to demonstrate the mathematical procedure to find outer product of matrices.\nExample 1: Basic Outer Product of Matrices\nGiven matrices: \\[A = \\begin{pmatrix}1 & 2 \\\\3 & 4\\end{pmatrix}, \\quad B = \\begin{pmatrix}5 \\\\6\\end{pmatrix}\\]\nThe outer product \\(A \\otimes B\\) is:\n\\[A \\otimes B = \\begin{pmatrix}1 \\cdot 5 & 1 \\cdot 6 \\\\2 \\cdot 5 & 2 \\cdot 6 \\\\3 \\cdot 5 & 3 \\cdot 6 \\\\4 \\cdot 5 & 4 \\cdot 6\\end{pmatrix} = \\begin{pmatrix}5 & 6 \\\\10 & 12 \\\\15 & 18 \\\\20 & 24\\end{pmatrix}\\]\nExample 2: Outer Product with Larger Matrices\nGiven matrices:\n\\[A = \\begin{pmatrix}1 & 2 & 3 \\\\4 & 5 & 6\\end{pmatrix}, \\quad B = \\begin{pmatrix}7 \\\\8\\end{pmatrix}\\]\nThe outer product \\(A \\otimes B\\) is:\n\\[A \\otimes B = \\begin{pmatrix}1 \\cdot 7 & 1 \\cdot 8 \\\\2 \\cdot 7 & 2 \\cdot 8 \\\\3 \\cdot 7 & 3 \\cdot 8 \\\\4 \\cdot 7 & 4 \\cdot 8 \\\\5 \\cdot 7 & 5 \\cdot 8 \\\\6 \\cdot 7 & 6 \\cdot 8\\end{pmatrix} = \\begin{pmatrix}7 & 8 \\\\14 & 16 \\\\21 & 24 \\\\28 & 32 \\\\35 & 40 \\\\42 & 48\\end{pmatrix}\\]\nExample 3: Compute the outer product of the following vectors \\(\\mathbf{u} = [0, 1, 2]\\) and \\(\\mathbf{v} = [2, 3, 4]\\).\nTo find the outer product, we calculate each element \\((i, j)\\) as the product of the \\((i)\\)-th element of \\(\\mathbf{u}\\) and the \\((j)\\)-th element of \\(\\mathbf{v}\\). Mathematically:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{bmatrix}0 \\cdot 2 & 0 \\cdot 3 & 0 \\cdot 4 \\\\1 \\cdot 2 & 1 \\cdot 3 & 1 \\cdot 4 \\\\2 \\cdot 2 & 2 \\cdot 3 & 2 \\cdot 4\\end{bmatrix}= \\begin{bmatrix}0 & 0 & 0 \\\\2 & 3 & 4 \\\\4 & 6 & 8\\end{bmatrix}\\]\n1. Compute Outer Product of Matrices from Scratch (without Libraries)\nHere’s how you can compute the outer product manually:\n\n# Define matrices A and B\nA = [[1, 2], [3, 4]]\nB = [[5], [6]]\n\n# Function to compute outer product\ndef outer_product_matrices(A, B):\n    m = len(A)\n    p = len(A[0])\n    q = len(B)\n    n = len(B[0])\n    result = [[0] * (n * p) for _ in range(m * q)]\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(q):\n                for l in range(n):\n                    result[i*q + k][j*n + l] = A[i][j] * B[k][l]\n\n    return result\n\n# Compute outer product\nouter_product_result_matrices = outer_product_matrices(A, B)\n\n# Display result\nprint(\"Outer Product of Matrices (From Scratch):\")\nfor row in outer_product_result_matrices:\n    print(row)\n\nOuter Product of Matrices (From Scratch):\n[5, 10]\n[6, 12]\n[15, 20]\n[18, 24]\n\n\nHere is the Python code to compute the outer product of these vectors using the NumPy function .outer():\n\nimport numpy as np\n\n# Define vectors\nu = np.array([[1,2],[3,4]])\nv = np.array([[5],[4]])\n\n# Compute outer product\nouter_product = np.outer(u, v)\n\nprint(\"Outer Product of u and v:\")\ndisplay(outer_product)\n\nOuter Product of u and v:\n\n\narray([[ 5,  4],\n       [10,  8],\n       [15, 12],\n       [20, 16]])\n\n\nExample 3: Real-world Application in Recommendation Systems\nIn recommendation systems, the outer product can represent user-item interactions. A simple context is here. Let the user preferences of items is given as \\(u=[4, 3, 5]\\) and the item scores is given by \\(v=[2, 5, 4]\\). Now the recommendation score can be calculated as the outer product of these two vectors. Calculation of this score is shown below. The outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is calculated as follows:\n\\[\\mathbf{u} \\otimes \\mathbf{v} = \\begin{bmatrix}4 \\cdot 2 & 4 \\cdot 5 & 4 \\cdot 4 \\\\3 \\cdot 2 & 3 \\cdot 5 & 3 \\cdot 4 \\\\5 \\cdot 2 & 5 \\cdot 5 & 5 \\cdot 4\\end{bmatrix}= \\begin{bmatrix}8 & 20 & 16 \\\\6 & 15 & 12 \\\\10 & 25 & 20\\end{bmatrix}\\]\nThe python code for this task is given below.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the user and product ratings vectors\nuser_ratings = np.array([4, 3, 5])\nproduct_ratings = np.array([2, 5, 4])\n\n# Compute the outer product\npredicted_ratings = np.outer(user_ratings, product_ratings)\n\n# Print the predicted ratings matrix\nprint(\"Predicted Ratings Matrix:\")\ndisplay(predicted_ratings)\n\n# Plot the result\nplt.imshow(predicted_ratings, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.title('Predicted Ratings Matrix (Recommendation System)')\nplt.xlabel('Product Ratings')\nplt.ylabel('User Ratings')\nplt.xticks(ticks=np.arange(len(product_ratings)), labels=product_ratings)\nplt.yticks(ticks=np.arange(len(user_ratings)), labels=user_ratings)\nplt.show()\n\nPredicted Ratings Matrix:\n\n\narray([[ 8, 20, 16],\n       [ 6, 15, 12],\n       [10, 25, 20]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Properties & Definitions\n\n\n\n\nDefinition and Properties\nGiven two vectors:\n\n\\(\\mathbf{u} \\in \\mathbb{R}^m\\)\n\\(\\mathbf{v} \\in \\mathbb{R}^n\\)\n\nThe outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\) results in an \\(m \\times n\\) matrix where each element \\((i, j)\\) of the matrix is calculated as: \\[(\\mathbf{u} \\otimes \\mathbf{v})_{ij} = u_i \\cdot v_j\\]\nNon-Symmetry\nThe outer product is generally not symmetric. For vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the matrix \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is not necessarily equal to \\(\\mathbf{v} \\otimes \\mathbf{u}\\): \\[\\mathbf{u} \\otimes \\mathbf{v} \\neq \\mathbf{v} \\otimes \\mathbf{u}\\]\nRank of the Outer Product\nThe rank of the outer product matrix \\(\\mathbf{u} \\otimes \\mathbf{v}\\) is always 1, provided neither \\(\\mathbf{u}\\) nor \\(\\mathbf{v}\\) is a zero vector. This is because the matrix can be expressed as a single rank-1 matrix.\nDistributive Property\nThe outer product is distributive over vector addition. For vectors \\(\\mathbf{u}_1, \\mathbf{u}_2 \\in \\mathbb{R}^m\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\): \\[(\\mathbf{u}_1 + \\mathbf{u}_2) \\otimes \\mathbf{v} = (\\mathbf{u}_1 \\otimes \\mathbf{v}) + (\\mathbf{u}_2 \\otimes \\mathbf{v})\\]\nAssociativity with Scalar Multiplication\nThe outer product is associative with scalar multiplication. For a scalar \\(\\alpha\\) and vectors \\(\\mathbf{u} \\in \\mathbb{R}^m\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\): \\[\\alpha (\\mathbf{u} \\otimes \\mathbf{v}) = (\\alpha \\mathbf{u}) \\otimes \\mathbf{v} = \\mathbf{u} \\otimes (\\alpha \\mathbf{v})\\]\nMatrix Trace\nThe trace of the outer product of two vectors is given by: \\[\\text{tr}(\\mathbf{u} \\otimes \\mathbf{v}) = (\\mathbf{u}^T \\mathbf{v})= (\\mathbf{v}^T \\mathbf{u})\\] Here, \\(\\text{tr}\\) denotes the trace of a matrix, which is the sum of its diagonal elements.\nMatrix Norm\nThe Frobenius norm of the outer product matrix can be expressed in terms of the norms of the original vectors: \\[\\| \\mathbf{u} \\otimes \\mathbf{v} \\|_F = \\| \\mathbf{u} \\|_2 \\cdot \\| \\mathbf{v} \\|_2\\] where \\(\\| \\cdot \\|_2\\) denotes the Euclidean norm.\n\n\n\nExample Calculation in Python\nHere’s how to compute and visualize the outer product properties using Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define vectors\nu = np.array([1, 2, 3])\nv = np.array([4, 5])\n\n# Compute outer product\nouter_product = np.outer(u, v)\n\n# Display results\nprint(\"Outer Product Matrix:\")\nprint(outer_product)\n\n# Compute and display rank\nrank = np.linalg.matrix_rank(outer_product)\nprint(f\"Rank of Outer Product Matrix: {rank}\")\n\n# Compute Frobenius norm\nfrobenius_norm = np.linalg.norm(outer_product, 'fro')\nprint(f\"Frobenius Norm: {frobenius_norm}\")\n\n# Plot the result\nplt.imshow(outer_product, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.title('Outer Product Matrix')\nplt.xlabel('Vector v')\nplt.ylabel('Vector u')\nplt.xticks(ticks=np.arange(len(v)), labels=v)\nplt.yticks(ticks=np.arange(len(u)), labels=u)\nplt.show()\n\nOuter Product Matrix:\n[[ 4  5]\n [ 8 10]\n [12 15]]\nRank of Outer Product Matrix: 1\nFrobenius Norm: 23.958297101421877\n\n\n\n\n\n\n\n\nFigure 2.10: Demonstration of Outer Product and its Properties\n\n\n\n\n\n\n\n2.2.2.7 Kronecker Product\nIn mathematics, the Kronecker product, sometimes denoted by \\(\\otimes\\), is an operation on two matrices of arbitrary size resulting in a block matrix. It is a specialization of the tensor product (which is denoted by the same symbol) from vectors to matrices and gives the matrix of the tensor product linear map with respect to a standard choice of basis. The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is a \\(p \\times q\\) matrix, then the Kronecker product \\(A\\otimes B\\) is the \\(pm \\times qn\\) block matrix defined as: Each \\(a_{ij}\\) of \\(A\\) is replaced by the matrix \\(a_{ij}B\\). Symbolically this will result in a block matrix defined by:\n\\[A\\otimes B=A \\otimes B = \\begin{bmatrix}a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\\vdots & \\vdots & \\ddots & vdots \\\\a_{m1}B & a_{m2}B & \\cdots & a_{mn}B\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nProperties of the Kronecker Product\n\n\n\n\nAssociativity\nThe Kronecker product is associative. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), and \\(C \\in \\mathbb{R}^{r \\times s}\\): \\[(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)\\]\nDistributivity Over Addition\nThe Kronecker product distributes over matrix addition. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), and \\(C \\in \\mathbb{R}^{p \\times q}\\): \\[A \\otimes (B + C) = (A \\otimes B) + (A \\otimes C)\\]\nMixed Product Property\nThe Kronecker product satisfies the mixed product property with the matrix product. For matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), \\(C \\in \\mathbb{R}^{r \\times s}\\), and \\(D \\in \\mathbb{R}^{r \\times s}\\): \\[(A \\otimes B) (C \\otimes D) = (A C) \\otimes (B D)\\]\nTranspose\nThe transpose of the Kronecker product is given by: \\[(A \\otimes B)^T = A^T \\otimes B^T\\]\nNorm\nThe Frobenius norm of the Kronecker product can be computed as: \\[\\| A \\otimes B \\|_F = \\| A \\|_F \\cdot \\| B \\|_F\\] where \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.\n\n\n\n\n\n\n\n\n\n\nFrobenius Norm\n\n\n\nThe Frobenius norm, also known as the Euclidean norm for matrices, is a measure of a matrix’s magnitude. It is defined as the square root of the sum of the absolute squares of its elements. Mathematically, for a matrix \\(A\\) with elements \\(a_{ij}\\), the Frobenius norm is given by:\n\\[\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\\]\n\n\nExample 1: Calculation of Frobenius Norm\nConsider the matrix \\(A\\):\n\\[A = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\|A\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 4^2}= \\sqrt{1 + 4 + 9 + 16}= \\sqrt{30}\\approx 5.48\\]\nExample 2: Frobenius Norm of a Sparse Matrix\nConsider the sparse matrix \\(B\\):\n\\[B = \\begin{bmatrix}0 & 0 & 0 \\\\0 & 5 & 0 \\\\0 & 0 & 0\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\|B\\|_F = \\sqrt{0^2 + 0^2 + 0^2 + 5^2 + 0^2 + 0^2}= \\sqrt{25}= 5\\]\nExample 3: Frobenius Norm in a Large Matrix\nConsider the matrix \\(C\\) of size $3 $:\n\\[C = \\begin{bmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9\\end{bmatrix}\\]\nTo compute the Frobenius norm:\n\\[\\begin{align*}\n\\|C\\|_F &= \\sqrt{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 + 7^2 + 8^2 + 9^2}\\\\\n&= \\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81}\\\\\n&= \\sqrt{285}\\\\\n&\\approx 16.88\n\\end{align*}\\]\nApplications of the Frobenius Norm\n\nApplication 1: Image Compression: In image processing, the Frobenius norm can measure the difference between the original and compressed images, indicating how well the compression has preserved the original image quality.\nApplication 2: Matrix Factorization: In numerical analysis, Frobenius norm is used to evaluate the error in matrix approximations, such as in Singular Value Decomposition (SVD). A lower Frobenius norm of the error indicates a better approximation.\nApplication 3: Error Measurement in Numerical Solutions: In solving systems of linear equations, the Frobenius norm can be used to measure the error between the true solution and the computed solution, providing insight into the accuracy of numerical methods.\n\nThe linalg sub module of NumPy library can be used to calculate various norms. Basically norm is the generalized form of Euclidean distance.\n\nimport numpy as np\n\n# Example 1: Simple Matrix\nA = np.array([[1, 2], [3, 4]])\nfrobenius_norm_A = np.linalg.norm(A, 'fro')\nprint(f\"Frobenius Norm of A: {frobenius_norm_A:.2f}\")\n\n# Example 2: Sparse Matrix\nB = np.array([[0, 0, 0], [0, 5, 0], [0, 0, 0]])\nfrobenius_norm_B = np.linalg.norm(B, 'fro')\nprint(f\"Frobenius Norm of B: {frobenius_norm_B:.2f}\")\n\n# Example 3: Large Matrix\nC = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nfrobenius_norm_C = np.linalg.norm(C, 'fro')\nprint(f\"Frobenius Norm of C: {frobenius_norm_C:.2f}\")\n\nFrobenius Norm of A: 5.48\nFrobenius Norm of B: 5.00\nFrobenius Norm of C: 16.88\n\n\nFrobenius norm of Kronecker product\nLet us consider two matrices,\n\\[A = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nand\n\\[B = \\begin{bmatrix}0 & 5 \\\\6 & 7\\end{bmatrix}\\]\nThe Kronecker product \\(C = A \\otimes B\\) is:\n\\[C = \\begin{bmatrix}1 \\cdot B & 2 \\cdot B \\\\3 \\cdot B & 4 \\cdot B\\end{bmatrix}= \\begin{bmatrix}\\begin{bmatrix}0 & 5 \\\\6 & 7\\end{bmatrix} & \\begin{bmatrix}0 \\cdot 2 & 5 \\cdot 2 \\\\6 \\cdot 2 & 7 \\cdot 2\\end{bmatrix} \\\\\\begin{bmatrix}0 \\cdot 3 & 5 \\cdot 3 \\\\6 \\cdot 3 & 7 \\cdot 3\\end{bmatrix} & \\begin{bmatrix}0 \\cdot 4 & 5 \\cdot 4 \\\\6 \\cdot 4 & 7 \\cdot 4\\end{bmatrix}\\end{bmatrix}\\]\nThis expands to:\n\\[C = \\begin{bmatrix}0 & 5 & 0 & 10 \\\\6 & 7 & 12 & 14 \\\\0 & 15 & 0 & 20 \\\\18 & 21 & 24 & 28\\end{bmatrix}\\]\nComputing the Frobenius Norm\nTo compute the Frobenius norm of \\(C\\):\n\\[\\|C\\|_F = \\sqrt{\\sum_{i=1}^{4} \\sum_{j=1}^{4} |c_{ij}|^2}\\]\n\\[\\|C\\|_F = \\sqrt{0^2 + 5^2 + 0^2 + 10^2 + 6^2 + 7^2 + 12^2 + 14^2 + 0^2 + 15^2 + 0^2 + 20^2 + 18^2 + 21^2 + 24^2 + 28^2}\\]\n\\[\\|C\\|_F = \\sqrt{0 + 25 + 0 + 100 + 36 + 49 + 144 + 196 + 0 + 225 + 0 + 400 + 324 + 441 + 576 + 784}\\]\n\\[\\|C\\|_F = \\sqrt{2896}\\] \\[\\|C\\|_F \\approx 53.87\\]\n\n\n\n2.2.2.8 Practice Problems\nFind the Kronecker product of A and B where A and B are given as follows:\nProblem 1:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\\\3&4\\end{bmatrix} \\otimes \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 4 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 1 & 0 & 2 \\\\\n1 & 0 & 2& 0\\\\\n0 & 3 & 0 & 4 \\\\\n3 & 0 & 4 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 2:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\otimes \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} & 0 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}2&3\\\\4&5\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 & 3 & 0 & 0 \\\\\n4 & 5 & 0 & 0 \\\\\n0 & 0 & 2 & 3 \\\\\n0 & 0 & 4 & 5\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 3:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3\\\\4\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&2\\end{bmatrix} \\otimes \\begin{bmatrix}3\\\\4\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}3\\\\4\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}3\\\\4\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n3 & 6 \\\\\n4 & 8\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 4:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&-1\\\\2&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 &1&-1\\\\\n0 & 0&2&0 \\\\\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 5:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] \\[B=\\begin{bmatrix}4&-2\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\\\3\\end{bmatrix} \\otimes \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot \\begin{bmatrix}4&-2\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}4&-2\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n8 & -4 \\\\\n12 & -6\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 6:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&-1\\\\0&2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&-1\\\\0&2\\end{bmatrix} \\otimes \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & -1 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} & 2 \\cdot \\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 1 & 0 & -1 \\\\\n1 & 0 & -1 & 0 \\\\\n0 & 0 & 0 & 2 \\\\\n0 & 0 & 2 & 0\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 7:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2\\end{bmatrix}\\] \\[B=\\begin{bmatrix}3&4\\\\5&6\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2\\end{bmatrix} \\otimes \\begin{bmatrix}3&4\\\\5&6\\end{bmatrix} \\\\\n&= 2 \\cdot \\begin{bmatrix}3&4\\\\5&6\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 8:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 \\cdot \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 9:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\] \\[B=\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} \\otimes \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} & 0 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} \\\\\n0 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix} & 1 \\cdot \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\n\\end{align*}\\]\n\nProblem 10:\nFind the Kronecker product of: \\[A=\\begin{bmatrix}2&-1\\\\3&4\\end{bmatrix}\\] \\[B=\\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix}\\]\nSolution:\n\\[\\begin{align*}\nA \\otimes B &= \\begin{bmatrix}2&-1\\\\3&4\\end{bmatrix} \\otimes \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} & -1 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} \\\\\n3 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix} & 4 \\cdot \\begin{bmatrix}0&5\\\\-2&3\\end{bmatrix}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n0 & 10 & 0 & -5 \\\\\n-4 & 6 & 2 & -3 \\\\\n0 & 15 & 0 & 20 \\\\\n-6 & 9 & -8 & 12\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.2.2.9 Connection Between Outer Product and Kronecker Product\n\nConceptual Connection:\n\nThe outer product is a special case of the Kronecker product. Specifically, if \\(\\mathbf{A}\\) is a column vector and \\(\\mathbf{B}\\) is a row vector, then \\(\\mathbf{A}\\) is a \\(m \\times 1\\) matrix and \\(\\mathbf{B}\\) is a \\(1 \\times n\\) matrix. The Kronecker product of these two matrices will yield the same result as the outer product of these vectors.\nFor matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), the Kronecker product involves taking the outer product of each element of \\(\\mathbf{A}\\) with the entire matrix \\(\\mathbf{B}\\).\n\nMathematical Formulation:\n\nLet \\(\\mathbf{A} = \\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{bmatrix}\\) and \\(\\mathbf{B} = \\begin{bmatrix}b_{11} & b_{12}\\\\ b_{21} & b_{22}\\end{bmatrix}\\). Then:\n\n\\[\\mathbf{A} \\otimes \\mathbf{B} = \\begin{bmatrix} a_{11} \\mathbf{B} & a_{12} \\mathbf{B} \\\\ a_{21} \\mathbf{B} & a_{22} \\mathbf{B} \\end{bmatrix}\\]\n\nIf \\(\\mathbf{A} = \\mathbf{u} \\mathbf{v}^T\\) where \\(\\mathbf{u}\\) is a column vector and \\(\\mathbf{v}^T\\) is a row vector, then the Kronecker product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}^T\\) yields the same result as the outer product \\(\\mathbf{u} \\otimes \\mathbf{v}\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummary\n\nThe outer product is a specific case of the Kronecker product where one of the matrices is a vector (either row or column).\nThe Kronecker product generalizes the outer product to matrices and is more versatile in applications involving tensor products and higher-dimensional constructs.\n\n\n\n\n\n2.2.2.10 Matrix Multiplication as Kronecker Product\nGiven matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), where: - \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix - \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix\nThe product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) can be expressed using Kronecker products as:\n\\[\\mathbf{C} = \\sum_{k=1}^n (\\mathbf{A}_{:,k} \\otimes \\mathbf{B}_{k,:})\\]\nwhere: - \\(\\mathbf{A}_{:,k}\\) denotes the \\(k\\)-th column of matrix \\(\\mathbf{A}\\) - \\(\\mathbf{B}_{k,:}\\) denotes the \\(k\\)-th row of matrix \\(\\mathbf{B}\\)\nExample:\nLet:\n\\[\\mathbf{A} = \\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}\\]\nand:\n\\[\\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\1 & 0\\end{bmatrix}\\]\nTo find \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) using Kronecker products:\n\nCompute the Kronecker Product of Columns of \\(\\mathbf{A}\\) and Rows of \\(\\mathbf{B}\\):\n\nFor column \\(\\mathbf{A}_{:,1} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\\) and row \\(\\mathbf{B}_{1,:} = \\begin{bmatrix} 0 & 1 \\end{bmatrix}\\): \\[\\mathbf{A}_{:,1} \\otimes \\mathbf{B}_{1,:} = \\begin{bmatrix}     0 & 1 \\\\     0 & 3     \\end{bmatrix}\\]\nFor column \\(\\mathbf{A}_{:,2} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) and row \\(\\mathbf{B}_{2,:} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\\): \\[\\mathbf{A}_{:,2} \\otimes \\mathbf{B}_{2,:} = \\begin{bmatrix}2 & 0 \\\\ 4 & 0\\end{bmatrix}\\]\n\nSum the Kronecker Products:\n\\[\\mathbf{C} = \\begin{bmatrix}0 & 1 \\\\ 0 & 3\\end{bmatrix} +\\begin{bmatrix} 2 & 0 \\\\ 4 & 0 \\end{bmatrix}  = \\begin{bmatrix} 2 & 1 \\\\ 4 & 3\\end{bmatrix}\\]\n\n\nIn the previous block we have discussed the Frobenius norm and its applications. Now came back to the discussions on the Kronecker product. The Kronecker product is particularly useful in scenarios where interactions between different types of data need to be modeled comprehensively. In recommendation systems, it allows us to integrate user preferences with item relationships to improve recommendation accuracy.\nIn addition to recommendation systems, Kronecker products are used in various fields such as:\n\nSignal Processing: For modeling multi-dimensional signals.\nMachine Learning: In building features for complex models.\nCommunication Systems: For modeling network interactions.\n\nBy understanding the Kronecker product and its applications, we can extend it to solve complex problems and enhance systems across different domains. To understand the practical use of Kronecker product in a Machine Learning scenario let us consider the following problem statement and its solution.\n\n\n\n\n\n\nProblem statement\n\n\n\nIn the realm of recommendation systems, predicting user preferences for various product categories based on past interactions is a common challenge. Suppose we have data on user preferences for different products and categories. We can use this data to recommend the best products for each user by employing mathematical tools such as the Kronecker product. The User Preference and Category relationships are given in Table 2.4 and Table 2.5 .\n\n\n\nTable 2.4: User Preference\n\n\n\n\n\nUser/Item\nElectronics\nClothing\nBooks\n\n\n\n\nUser 1\n5\n3\n4\n\n\nUser 2\n2\n4\n5\n\n\nUser 3\n3\n4\n4\n\n\n\n\n\n\n\n\n\nTable 2.5: Category Relationships\n\n\n\n\n\nCategory/Feature\nFeature 1\nFeature 2\nFeature 3\n\n\n\n\nElectronics\n1\n0\n0\n\n\nClothing\n0\n1\n1\n\n\nBooks\n0\n1\n1\n\n\n\n\n\n\nPredict user preferences for different product categories using the Kronecker product matrix.\n\n\n\nSolution Procedure\n\n\nCompute the Kronecker Product: Calculate the Kronecker product of matrices \\(U\\) and \\(C\\) to obtain matrix \\(K\\).\nTo model the problem, we use the Kronecker product of the user preference matrix \\(U\\) and the category relationships matrix \\(C\\). This product allows us to predict the user’s rating for each category by combining their preferences with the category features.\n\nFormulating Matrices\nUser Preference Matrix (U): - Dimension: \\(3\\times 3\\) (3 users, 3 items) - from the User preference data, we can create the User Preference Matrix as follows:\n\\[U = \\begin{pmatrix}5 & 3 & 4 \\\\2 & 4 & 5 \\\\3 & 4 & 4 \\end{pmatrix}\\]\nCategory Relationships Matrix (C): - Dimension: \\(3 \\times 3\\) (3 categories) - from the Category Relationships data, we can create the Category Relationship Matrix as follows:\n\\[C = \\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1\\end{pmatrix}\\]\nKronecker Product Calculation\nThe Kronecker product \\(K\\) of \\(U\\) and \\(C\\) is calculated as follows:\n\nMatrix Dimensions:\n\n\n\\(U\\) is \\(3 \\times 3\\) (3 users, 3 items).\n\\(C\\) is \\(3 \\times 3\\) (3 categories, 3 features).\n\n\nCalculate Kronecker Product:\n\n\nFor each element \\(u_{ij}\\) in \\(U\\), multiply by the entire matrix \\(C\\).\n\nThe Kronecker product \\(K\\) is computed as:\n\\[K = U \\otimes C\\]\nExplicitly, the Kronecker product \\(K\\) is:\n\\[K = \\begin{pmatrix}5 \\cdot C & 3 \\cdot C & 4 \\cdot C \\\\ 2 \\cdot C & 4 \\cdot C & 5 \\cdot C \\\\    3 \\cdot C & 4 \\cdot C & 4 \\cdot C\\end{pmatrix}\\]\nAs an example the blocks in first row are:\n\\[5 \\cdot C = \\begin{pmatrix}   5 & 0 & 0 \\\\   0 & 5 & 5 \\\\   0 & 5 & 5   \\end{pmatrix}, \\quad    3 \\cdot C = \\begin{pmatrix}   3 & 0 & 0 \\\\   0 & 3 & 3 \\\\   0 & 3 & 3   \\end{pmatrix}, \\quad   4 \\cdot C = \\begin{pmatrix}   4 & 0 & 0 \\\\   0 & 4 & 4 \\\\   0 & 4 & 4   \\end{pmatrix}\\]\nCombining these blocks:\n\\[K = \\begin{pmatrix}   5 & 0 & 0 & 3 & 0 & 0 & 4 & 0 & 0\\\\   0 & 5 & 5 & 0 & 3 & 3 & 0 & 4 & 4\\\\   0 & 5 & 5 & 0 & 3 & 3 & 0 & 4 & 4\\\\   2 & 0 & 0 & 4 & 0 & 0 & 5 & 0 & 0\\\\   0 & 2 & 2 & 0 & 4 & 4 & 0 & 5 & 5\\\\   0 & 2 & 2 & 0 & 4 & 4 & 0 & 5 & 5\\\\   3 & 0 & 0 & 4 & 0 & 0 & 4 & 0 & 0\\\\   0 & 3 & 3 & 0 & 4 & 4 & 0 & 4 & 4\\\\   0 & 3 & 3 & 0 & 4 & 4 & 0 & 4 & 4\\end{pmatrix}\\]\n\nInterpret the Kronecker Product Matrix: The resulting matrix \\(K\\) represents all possible combinations of user preferences and category features.\nPredict Ratings: For each user, use matrix \\(K\\) to predict the rating for each category by summing up the values in the corresponding rows.\nGenerate Recommendations: Identify the top categories with the highest predicted ratings for each user.\n\nThe python code to solve this problem computationally is given below.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define the matrices\nU = np.array([[5, 3, 4],\n              [2, 4, 5],\n              [3, 4, 4]])\n\nC = np.array([[1, 0, 0],\n              [0, 1, 1],\n              [0, 1, 1]])\n\n# Compute the Kronecker product\nK = np.kron(U, C)\n\n# Create a DataFrame to visualize the Kronecker product matrix\ndf_K = pd.DataFrame(K, \n                    columns=['Electronics_F1', 'Electronics_F2', 'Electronics_F3', \n                             'Clothing_F1', 'Clothing_F2', 'Clothing_F3', \n                             'Books_F1', 'Books_F2', 'Books_F3'],\n                    index=['User 1 Electronics', 'User 1 Clothing', 'User 1 Books', \n                           'User 2 Electronics', 'User 2 Clothing', 'User 2 Books', \n                           'User 3 Electronics', 'User 3 Clothing', 'User 3 Books'])\n\n# Print the Kronecker product matrix\nprint(\"Kronecker Product Matrix (K):\\n\", df_K)\n\n# Predict ratings and create recommendations\ndef recommend(user_index, top_n=3):\n    \"\"\" Recommend top_n categories for a given user based on Kronecker product matrix. \"\"\"\n    user_ratings = K[user_index * len(C):(user_index + 1) * len(C), :]\n    predicted_ratings = np.sum(user_ratings, axis=0)\n    recommendations = np.argsort(predicted_ratings)[::-1][:top_n]\n    return recommendations\n\n# Recommendations for User 1\nuser_index = 0  # User 1\ntop_n = 3\nrecommendations = recommend(user_index, top_n)\n\nprint(f\"\\nTop {top_n} recommendations for User {user_index + 1}:\")\nfor rec in recommendations:\n    print(df_K.columns[rec])\n\nKronecker Product Matrix (K):\n                     Electronics_F1  Electronics_F2  Electronics_F3  \\\nUser 1 Electronics               5               0               0   \nUser 1 Clothing                  0               5               5   \nUser 1 Books                     0               5               5   \nUser 2 Electronics               2               0               0   \nUser 2 Clothing                  0               2               2   \nUser 2 Books                     0               2               2   \nUser 3 Electronics               3               0               0   \nUser 3 Clothing                  0               3               3   \nUser 3 Books                     0               3               3   \n\n                    Clothing_F1  Clothing_F2  Clothing_F3  Books_F1  Books_F2  \\\nUser 1 Electronics            3            0            0         4         0   \nUser 1 Clothing               0            3            3         0         4   \nUser 1 Books                  0            3            3         0         4   \nUser 2 Electronics            4            0            0         5         0   \nUser 2 Clothing               0            4            4         0         5   \nUser 2 Books                  0            4            4         0         5   \nUser 3 Electronics            4            0            0         4         0   \nUser 3 Clothing               0            4            4         0         4   \nUser 3 Books                  0            4            4         0         4   \n\n                    Books_F3  \nUser 1 Electronics         0  \nUser 1 Clothing            4  \nUser 1 Books               4  \nUser 2 Electronics         0  \nUser 2 Clothing            5  \nUser 2 Books               5  \nUser 3 Electronics         0  \nUser 3 Clothing            4  \nUser 3 Books               4  \n\nTop 3 recommendations for User 1:\nElectronics_F3\nElectronics_F2\nBooks_F3\n\n\nA simple visualization of this recomendation system is shown in Fig 2.11.\n\n# Visualization\ndef plot_recommendations(user_index):\n    \"\"\" Plot the predicted ratings for each category for a given user. \"\"\"\n    user_ratings = K[user_index * len(C):(user_index + 1) * len(C), :]\n    predicted_ratings = np.sum(user_ratings, axis=0)\n    categories = df_K.columns\n    plt.figure(figsize=(6, 5))\n    plt.bar(categories, predicted_ratings)\n    plt.xlabel('Categories')\n    plt.ylabel('Predicted Ratings')\n    plt.title(f'Predicted Ratings for User {user_index + 1}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Plot recommendations for User 1\nplot_recommendations(user_index)\n\n\n\n\n\n\n\nFigure 2.11: EDA for the Recommendation System\n\n\n\n\n\nThis micro project illustrate one of the popular use of Kronecker product on ML application.\n\n\n\n2.2.3 Matrix Measures of Practical Importance\nMatrix measures, such as rank and determinant, play crucial roles in linear algebra. While both rank and determinant provide valuable insights into the properties of a matrix, they serve different purposes. Understanding their roles and applications is essential for solving complex problems in computer science, engineering, and applied mathematics.\n\n2.2.3.1 Determinant\nDeterminant of a \\(2\\times 2\\) matrix \\(A=\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\\) is defined as \\(|A|=ad-bc\\). Determinant of higher order square matrices can be found using the Laplace method or Sarrus method.\nThe determinant of a matrix provides information about the matrix’s invertibility and scaling factor for volume transformation. Specifically:\n\nInvertibility: A matrix is invertible if and only if its determinant is non-zero.\nVolume Scaling: The absolute value of the determinant gives the scaling factor by which the matrix transforms volume.\nParallelism: If the determinant of a matrix composed of vectors is zero, the vectors are linearly dependent, meaning they are parallel or redundant.\nRedundancy: A zero determinant indicates that the vectors span a space of lower dimension than the number of vectors, showing redundancy.\n\n\n\n\n\n\n\nLeast Possible Values of Determinant\n\n\n\n\nLeast Positive Determinant: For a \\(1\\times 1\\) matrix, the smallest non-zero determinant is any positive value, typically \\(\\epsilon\\), where \\(\\epsilon\\) is a small positive number.\nLeast Non-Zero Determinant: For higher-dimensional matrices, the smallest non-zero determinant is a non-zero value that represents the smallest area or volume spanned by the matrix’s rows or columns. For example a \\(2\\times 2\\) matrix with determinant \\(\\epsilon\\) could be: \\[B=\\begin{pmatrix}\\epsilon&0\\\\ 0&\\epsilon\\end{pmatrix}\\] Here, \\(\\epsilon\\) is a small positive number, indicating a very small but non-zero area.\n\n\n\nNow let’s look into the most important matrix measure for advanced application in Linear Algebra.\nAs we know the matrix is basically a representation tool that make things abstract- remove unnecessary details. Then the matrix itself can be represented in many ways. This is the real story telling with this most promising mathematical structure. Consider a context of collecting feedback about a product in three aspects- cost, quality and practicality. For simplicity in calculation, we consider responses from 3 customers only. The data is shown in Table 2.6.\n\n\n\nTable 2.6: User rating of a consumer product\n\n\n\n\n\nUser\nCost\nQuality\nPracticality\n\n\n\n\nUser-1\n1\n4\n5\n\n\nUser-2\n3\n2\n5\n\n\nUser-3\n2\n1\n3\n\n\n\n\n\n\nIt’s perfect and nice looking. But both mathematics and a computer can’t handle this table as it is. So we create an abstract representation of this data- the rating matrix. Using the traditional approach, let’s represent this rating data as: \\[A=\\begin{bmatrix}1&4&5\\\\3&2&5\\\\2&1&3\\end{bmatrix}\\]\nNow both the column names and row indices were removed and the data is transformed into the abstract form. This representation has both advantages and disadvantages. Be positive! So we are focused only in the advantages.\nJust consider the product. Its sales fully based on its features. So the product sales perspective will be represented in terms of the features- cost, quality and practicality. These features are columns of our rating matrix. Definitly people will have different rating for these features. Keeping all these in mind let’s introduce the concept of linear combination. This leads to a new matrix product as shown below.\n\\[\\begin{align*}\nAx&=\\begin{bmatrix}\n1&4&5\\\\\n3&2&5\\\\\n2&1&3\n\\end{bmatrix}x\\\\\n&=\\begin{bmatrix}\n1&4&5\\\\\n3&2&5\\\\\n2&1&3\n\\end{bmatrix}\\cdot\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1\\\\3\\\\2\\end{bmatrix}x_1+\\begin{bmatrix}4\\\\2\\\\1\\end{bmatrix}x_2+\\begin{bmatrix}5\\\\5\\\\3\\end{bmatrix}x_3\n\\end{align*}\\]\nAs the number of users increases, the product sales perspective become more informative. In short the span of the features define the feature space of the product. In real cases, a manufacture wants to know what are the features really inflence the customers. This new matrix product will help the manufactures to identify that features!\nSo we are going to define this new matrix product as the feature space, that will provide more insights to this context as:\n\\[A=CR\\]\nWhere \\(C\\) is the column space and \\(R\\) is the row reduced Echelon form of \\(A\\). But the product is not the usual scalar projection, Instead the weight of linear combination of elements in the column space.\nLet’s formally illustrate this in our example. From the first observation itself, it is clear that last column is just the sum of first and second columns (That is in our context the feature ‘practicality’ is just depends on ‘cost’ and ‘quality’. meaningful?). So only first columns are independent and so spans the column space.\n\\[C=\\begin{bmatrix}1&4\\\\3&2\\\\2&1\\end{bmatrix}\\]\nNow look into the matrix \\(R\\). Applying elementary row tansformations, \\(A\\) will transformed into:\n\\[R=\\begin{bmatrix}1&0&1\\\\0&1&1\\\\0&0&0\\end{bmatrix}\\]\nHence we can form a decomposition for the given rating matrix, \\(A\\) as: \\[\\begin{align*}\nA&=CR\\\\\n&=\\begin{bmatrix}1&4\\\\3&2\\\\2&1\\end{bmatrix}\\begin{bmatrix}1&0&1\\\\0&1&1\\\\\\mbox{}&&\\end{bmatrix}\n\\end{align*}\\]\nThis decomposition says that there are only two independent features (columns) and the third feature (column) is the sum of first two features (columns).\n\n\n\n\n\n\nInterpretation of the \\(R\\) matrix\n\n\n\nEach column in the \\(R\\) matrix represents the weights for linear combination of vectors in the column space to get that column in \\(A\\). In this example, third column of \\(R\\) is \\(\\begin{bmatrix}1\\\\1\\end{bmatrix}\\). This means that third column of \\(A\\) will be \\(1\\times C_1+1\\times C_2\\) of the column space, \\(C\\)!\n\n\nThis first matrix decompostion donate a new type of matrix product (outer product) and a new measure- the number of independent columns and number of independent rows. This count is called the rank of the matrix \\(A\\). In the case of features, if the rank of the column space is less than the number of features then definitly a less number of feature set will perfectly represent the data. This will help us to reduce the dimension of the dataset and there by reducing computational complexities in data analysis and machine Learning jobs.\nIn the above discussion, we consider only the columns of \\(A\\). Now we will mention the row space. It is the set of all linearly independent rows of \\(A\\). For any matrix \\(A\\), both the row space and column space are of same rank. This correspondance is a helpful result in many practical applications.\nNow we consider a stable equation, \\(Ax=0\\). With the usual notation of dot product, it implies that \\(x\\) is orthogonal to \\(A\\). Set of all those independent vectors which are orthogonal to \\(A\\) constitute a new space of interest. It is called the null space of \\(A\\). If \\(A\\) represents a linear transformation, then the null space will be populated by those non-zero vectors which are nullified by the transformation \\(A\\). As a summary of this discussion, the row space and null space of a matrix \\(A\\) creates an orthogonal system. Considering the relationship between \\(A\\) and \\(A^T\\), it is clear that row space of \\(A\\) is same as the column space of \\(A^T\\) and vice verse are. So we can restate the orthogonality as: ‘the null space of \\(A\\) is orthogonal to the column space of \\(A^T\\)’ and ‘the null space of \\(A^T\\) is orthogonal to the column space of \\(A\\)’. Mathematically this property can be represents as follows.\n\n\n\n\n\n\nNote\n\n\n\n\\[\\begin{align*}\n\\mathcal{N}(A)&\\perp \\mathcal{C}(A^T)\\\\\n\\mathcal{N}(A^T)&\\perp \\mathcal{C}(A)\n\\end{align*}\\]\n\n\nIn the given example, solving \\(Ax=0\\) we get \\(x=\\begin{bmatrix}1&1&-1\\end{bmatrix}^T\\).\nSo the rank of \\(\\mathcal{N}(A)=1\\). Already we have rank of \\(A=2\\). This leads to an interesting result:\n\\[\\text{Rank}(A)+\\text{Rank}(\\mathcal{N}(A))=3\\]\nThis observation can be framed as a theorem.\n\n\n\n2.2.4 Rank Nullity Theorem\nThe rank-nullity theorem is a fundamental theorem in linear algebra that is important for understanding the connections between mathematical operations in engineering, physics, and computer science. It states that the sum of the rank and nullity of a matrix equals the number of columns in the matrix. The rank is the maximum number of linearly independent columns, and the nullity is the dimension of the nullspace.\n\nTheorem 2.1 (Rank Nullitty Theorem) The Rank-Nullity Theorem states that for any \\(m \\times n\\) matrix \\(A\\), the following relationship holds:\n\\[\n\\text{Rank}(A) + \\text{Nullity}(A) = n\n\\]\nwhere: - Rank of \\(A\\) is the dimension of the column space of \\(A\\), which is also equal to the dimension of the row space of \\(A\\). - Nullity of \\(A\\) is the dimension of the null space of \\(A\\), which is the solution space to the homogeneous system \\(A \\mathbf{x} = \\mathbf{0}\\).\n\nSteps to Formulate for Matrix \\(A\\)\n\nFind the Rank of \\(A\\): The rank of a matrix is the maximum number of linearly independent columns (or rows). It can be determined by transforming \\(A\\) into its row echelon form or reduced row echelon form (RREF).\nFind the Nullity of \\(A\\): The nullity is the dimension of the solution space of \\(A \\mathbf{x} = \\mathbf{0}\\). This can be found by solving the homogeneous system and counting the number of free variables.\nApply the Rank-Nullity Theorem: Use the rank-nullity theorem to verify the relationship.\n\n\nExample 1: Calculate the rank and nullity of \\(A=\\begin{bmatrix}   1 & 4 & 5 \\\\   3 & 2 & 5 \\\\   2 & 1 & 3   \\end{bmatrix}\\) and verify the rank nullity theorem.\n\nRow Echelon Form:\nPerform Gaussian elimination on \\(A\\):\n\\[A = \\begin{bmatrix} 1 & 4 & 5 \\\\  3 & 2 & 5 \\\\   2 & 1 & 3   \\end{bmatrix}\\]\nPerform row operations to get it to row echelon form:\n\nSubtract 3 times row 1 from row 2: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     2 & 1 & 3     \\end{bmatrix}\\]\nSubtract 2 times row 1 from row 3: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     0 & -7 & -7     \\end{bmatrix}\\]\nAdd \\(\\frac{7}{10}\\) times row 2 to row 3: \\[\\begin{bmatrix}     1 & 4 & 5 \\\\     0 & -10 & -10 \\\\     0 & 0 & 0     \\end{bmatrix}\\]\n\nThe matrix is now in row echelon form.\nRank is the number of non-zero rows, which is 2.\nFind the Nullity: The matrix \\(A\\) has 3 columns. The number of free variables in the solution of \\(A \\mathbf{x} = \\mathbf{0}\\) is \\(3 - \\text{Rank}\\).\nSo, \\[\\text{Nullity}(A) = 3 - 2 = 1\\]\nApply the Rank-Nullity Theorem: \\[\\text{Rank}(A) + \\text{Nullity}(A) = 2 + 1 = 3\\]\nThis matches the number of columns of \\(A\\), confirming the theorem.\n\n\n\n2.2.5 Fundamental Subspaces\nIn section (note-ortho?), we have seen that for any matrix \\(A\\), there is two pairs of inter-related orthogonal spaces. This leads to the concept of Fundamental sup spaces.\nMatrices are not just arrays of numbers; they can represent linear transformations too. A linear transformation maps vectors from one vector space to another while preserving vector addition and scalar multiplication. The matrix \\(A\\) can be viewed as a representation of a linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) where:\n\\[T(\\mathbf{x}) = A \\mathbf{x}\\]\nIn this context:\n\nThe column space of \\(A\\) represents the range of \\(T\\), which is the set of all possible outputs.\nThe null space of \\(A\\) represents the kernel of \\(T\\), which is the set of vectors that are mapped to the zero vector.\n\nThe Four Fundamental Subspaces\nUnderstanding the four fundamental subspaces helps in analyzing the properties of a linear transformation. These subspaces are:\n\nDefinition 2.1 (Four Fundamental Subspaces) Let \\(T:\\mathbb{R^n}\\longrightarrow \\mathbb{R^m}\\) be a linear transformation and \\(A\\) represents the matrix of transformation. The four fundamental subspaces are defined as:\n\nColumn Space (Range): The set of all possible outputs of the transformation. For matrix \\(A\\), this is the span of its columns. It represents the image of \\(\\mathbb{R}^n\\) under \\(T\\).\nNull Space (Kernel): The set of all vectors that are mapped to the zero vector by the transformation. For matrix \\(A\\), this is the solution space of \\(A \\mathbf{x} = \\mathbf{0}\\).\nRow Space: The span of the rows of \\(A\\). This space is crucial because it helps in understanding the rank of \\(A\\). The dimension of the row space is equal to the rank of \\(A\\), which represents the maximum number of linearly independent rows.\nLeft Null Space: The set of all vectors \\(\\mathbf{y}\\) such that \\(A^T \\mathbf{y} = \\mathbf{0}\\). It provides insight into the orthogonal complement of the row space.\n\n\nThis idea is depicted as a ‘Big picture of the four sub spaces of a matrix’ in the Strang’s text book on Linear algebra for every one (Strang 2020). This ‘Big Picture’ is shown in Fig- 2.12.\n\n\n\n\n\n\nFigure 2.12: The Big Pictue of Fundamental Subspaces\n\n\n\nA video session from Strang’s session is here:\n\n\n2.2.5.1 Practice Problems\nProblem 1: Express the vector \\((1,-2,5)\\) as a linear combination of the vectors \\((1,1,1)\\), \\((1,2,3)\\) and \\((2,-1,1)\\).\nProblem 2: Show that the feature vector \\((2,-5,3)\\) is not linearly associated with the features \\((1,-3,2)\\), \\((2,-4,-1)\\) and \\((1,-5,7)\\).\nProblem 3: Show that the feature vectors \\((1,1,1)\\), \\((1,2,3)\\) and \\((2,-1,1)\\) are non-redundant.\nProblem 4: Prove that the features \\((1,-1,1)\\), \\((0,1,2)\\) and \\((3,0,-1)\\) form basis for the feature space.\nProblem 5: Check whether the vectors \\((1,2,1)\\), \\((2,1,4)\\) and \\((4,5,6)\\) form a basis for \\(\\mathbb{R}^3\\).\nProblem 6: Find the four fundamental subspaces of the feature space created by \\((1,2,1)\\), \\((2,1,4)\\) and \\((4,5,6)\\).\nProblem 7: Find the four fundamental subspaces and its dimensions of the matrix \\(\\begin{bmatrix}1&2&4\\\\2&1&5\\\\1&4&6\\end{bmatrix}\\).\nProblem 8: Express \\(A=\\begin{bmatrix}1&2&-1\\\\3&1&-1\\\\2&-1&0\\end{bmatrix}\\) as the Kronecker product of the column space and the row space in the form \\(A=C\\otimes R\\).\nProblem 9: Find the four fundamental subspaces of \\(A=\\begin{bmatrix} 1&2&0&2&5\\\\-2&-5&1&-1&-8\\\\0&-3&3&4&1\\\\3&6&0&-7&2\\end{bmatrix}\\).\nProblem 10: Find the four fundamental subspaces of \\(A=\\begin{bmatrix}-1&2&-1&5&6\\\\4&-4&-4&-12&-8\\\\2&0&-6&-2&4\\\\-3&1&7&-2&12\\end{bmatrix}\\).\nProblem 11: Express \\(A=\\begin{bmatrix}2&3&-1&-1\\\\1&-1&-2&-4\\\\3&1&3&-2\\\\6&3&0&-7\\end{bmatrix}\\) in \\(A=C\\otimes R\\), where \\(C\\) is the column space and \\(R\\) is the row space of \\(A\\).\nProblem 12: Express \\(A=\\begin{bmatrix}0&1&-3&-1\\\\1&0&1&1\\\\3&1&0&2\\\\1&1&-2&0\\end{bmatrix}\\) in \\(A=C\\otimes R\\), where \\(C\\) is the column space and \\(R\\) is the row space of \\(A\\).\nProblem 13: Show that the feature vectors \\((2,3,0)\\), \\((1,2,0)\\) and \\((8,13,0)\\) are redundant and hence find the relationship between them.\nProblem 14: Show that the feature vectors \\((1,2,1)\\), \\((4,1,2)\\), \\((-3,8,1)\\) and \\((6,5,4)\\) are redundant and hence find the relationship between them.\nProblem 15: Show that the feature vectors \\((1,2,-1,0)\\), \\((1,3,1,2)\\), \\((4,2,1,0)\\) and \\((6,1,0,1)\\) are redundant and hence find the relationship between them.\n\n\n\n\n\n\nImportant\n\n\n\nThree Parts of the Fundamental theorem The fundamental theorem of linear algebra relates all four of the fundamental subspaces in a number of different ways. There are main parts to the theorem:\nPart 1:(Rank nullity theorem) The column and row spaces of an \\(m\\times n\\) matrix \\(A\\) both have dimension \\(r\\), the rank of the matrix. The nullspace has dimension \\(n−r\\), and the left nullspace has dimension \\(m−r\\).\nPart 2:(Orthogonal subspaces) The nullspace and row space are orthogonal. The left nullspace and the column space are also orthogonal.\nPart 3:(Matrix decomposition) The final part of the fundamental theorem of linear algebra constructs an orthonormal basis, and demonstrates a singular value decomposition: any matrix \\(M\\) can be written in the form \\(M=U\\Sigma V^T\\) , where \\(U_{m\\times m}\\) and \\(V_{n\\times n}\\) are unitary matrices, \\(\\Sigma_{m\\times n}\\) matrix with nonnegative values on the diagonal.\nThis part of the fundamental theorem allows one to immediately find a basis of the subspace in question. This can be summarized in the following table.\n\n\n\n\n\n\n\n\n\n\nSubspace\nSubspace of\nSymbol\nDimension\nBasis\n\n\n\n\nColumn space\n\\(\\mathbb{R}^m\\)\n\\(\\operatorname{im}(A)\\)\n\\(r\\)\nFirst \\(r\\) columns of \\(U\\)\n\n\nNullspace (kernel)\n\\(\\mathbb{R}^n\\)\n\\(\\ker(A)\\)\n\\(n - r\\)\nLast \\(n - r\\) columns of \\(V\\)\n\n\nRow space\n\\(\\mathbb{R}^n\\)\n\\(\\operatorname{im}(A^T)\\)\n\\(r\\)\nFirst \\(r\\) columns of \\(V\\)\n\n\nLeft nullspace (kernel)\n\\(\\mathbb{R}^m\\)\n\\(\\ker(A^T)\\)\n\\(m - r\\)\nLast \\(m - r\\) columns of \\(U\\)\n\n\n\n\n\n\n\n2.2.5.2 Computational methods to find all the four fundamental subspaces of a matrix\nThere are different approaches to find the four fundamental subspaces of a matrix using Python. Simplest method is just convert our mathematical procedure into Python functions and call them to find respective spaces. This method is illustrated below.\n\n# importing numpy library for numerical computation\nimport numpy as np\n# define the function create the row-reduced Echelon form of given matrix\ndef row_echelon_form(A):\n    \"\"\"Convert matrix A to its row echelon form.\"\"\"\n    A = A.astype(float)\n    rows, cols = A.shape\n    for i in range(min(rows, cols)):\n        # Pivot: find the maximum element in the current column\n        max_row = np.argmax(np.abs(A[i:, i])) + i\n        if A[max_row, i] == 0:\n            continue  # Skip if the column is zero\n        # Swap the current row with the max_row\n        A[[i, max_row]] = A[[max_row, i]]\n        # Eliminate entries below the pivot\n        for j in range(i + 1, rows):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n    return A\n\n# define function to generate null space from the row-reduced echelon form\ndef null_space_of_matrix(A, rtol=1e-5):\n    \"\"\"Compute the null space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    rows, cols = A_reduced.shape\n    # Identify pivot columns\n    pivots = []\n    for i in range(rows):\n        for j in range(cols):\n            if np.abs(A_reduced[i, j]) &gt; rtol:\n                pivots.append(j)\n                break\n    free_vars = set(range(cols)) - set(pivots)\n    \n    null_space = []\n    for free_var in free_vars:\n        null_vector = np.zeros(cols)\n        null_vector[free_var] = 1\n        for pivot, row in zip(pivots, A_reduced[:len(pivots)]):\n            null_vector[pivot] = -row[free_var]\n        null_space.append(null_vector)\n    \n    return np.array(null_space).T\n\n# define the function to generate the row-space of A\n\ndef row_space_of_matrix(A):\n    \"\"\"Compute the row space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    # The non-zero rows of the reduced matrix form the row space\n    non_zero_rows = A_reduced[~np.all(A_reduced == 0, axis=1)]\n    return non_zero_rows\n\n# define the function to generate the column space of A\n\ndef column_space_of_matrix(A):\n    \"\"\"Compute the column space of a matrix A using row reduction.\"\"\"\n    A_reduced = row_echelon_form(A)\n    rows, cols = A_reduced.shape\n    # Identify pivot columns\n    pivots = []\n    for i in range(rows):\n        for j in range(cols):\n            if np.abs(A_reduced[i, j]) &gt; 1e-5:\n                pivots.append(j)\n                break\n    column_space = A[:, pivots]\n    return column_space\n\n\n\n2.2.5.3 Examples:\n\nFind all the fundamental subspaces of \\(A=\\begin{pmatrix}1&2&3\\\\ 4&5&6\\\\7&8&9\\end{pmatrix}\\).\n\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nprint(\"Matrix A:\")\nprint(A)\n\n# Null Space\nnull_space_A = null_space_of_matrix(A)\nprint(\"\\nNull Space of A:\")\nprint(null_space_A)\n\n# Row Space\nrow_space_A = row_space_of_matrix(A)\nprint(\"\\nRow Space of A:\")\nprint(row_space_A)\n\n# Column Space\ncolumn_space_A = column_space_of_matrix(A)\nprint(\"\\nColumn Space of A:\")\nprint(column_space_A)\n\nMatrix A:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nNull Space of A:\n[[-9.        ]\n [-1.71428571]\n [ 1.        ]]\n\nRow Space of A:\n[[7.00000000e+00 8.00000000e+00 9.00000000e+00]\n [0.00000000e+00 8.57142857e-01 1.71428571e+00]\n [0.00000000e+00 5.55111512e-17 1.11022302e-16]]\n\nColumn Space of A:\n[[1 2]\n [4 5]\n [7 8]]\n\n\n\n\n2.2.5.4 Rank and Solution of System of Linear Equations\nIn linear algebra, the rank of a matrix is a crucial concept for understanding the structure of a system of linear equations. It provides insight into the solutions of these systems, helping us determine the number of independent equations and the nature of the solution space.\n\nDefinition 2.2 (Rank and System Consistency) The rank of a matrix \\(A\\) is defined as the maximum number of linearly independent rows or columns. When solving a system of linear equations represented by \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(A\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{b}\\) is a vector, the rank of \\(A\\) plays a crucial role in determining the solution’s existence and uniqueness.\nConsistency of the System\n\nConsistent System: A system of linear equations is consistent if there exists at least one solution. This occurs if the rank of the coefficient matrix \\(A\\) is equal to the rank of the augmented matrix \\([A|\\mathbf{b}]\\). Mathematically, this can be expressed as: \\[\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}])\\] If this condition is met, the system has solutions. The solutions can be:\n\nUnique if the rank equals the number of variables.\nInfinitely many if the rank is less than the number of variables.\n\nInconsistent System: A system is inconsistent if there are no solutions. This occurs when: \\[\\text{rank}(A) \\ne \\text{rank}([A|\\mathbf{b}])\\] In this case, the equations represent parallel or conflicting constraints that cannot be satisfied simultaneously.\n\n\n\n\n\n\n\n\nUse of Null space in creation of general solution from particular solution\n\n\n\nIf the system \\(AX=b\\) has many solutions, then the general solution of the system can be found using a particular solution and the elements in the null space of the coefficient matrix \\(A\\) as\n\\[X=x_p+tX_N\\]\nwhere \\(X\\) is the general solution and \\(t\\) is a free variable (parameter) and \\(X_N\\in N(A)\\).\n\n\n\n\n2.2.5.5 Computational method to solve system of linear equations.\nIf for a system \\(AX=b\\), \\(det(A)\\neq 0\\), then the system has a unique solution and can be found by solve() function from NumPy. If the system is consistant and many solutions, then computationally we will generate the general solution using the \\(N(A)\\). A detailed Python code is given below.\n\nimport numpy as np\n\ndef check_consistency(A, b):\n    \"\"\"\n    Check the consistency of a linear system Ax = b and return the solution if consistent.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix.\n    b (numpy.ndarray): Right-hand side vector.\n    \n    Returns:\n    tuple: A tuple with consistency status, particular solution (if consistent), and null space (if infinite solutions).\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Augment the matrix A with vector b\n    augmented_matrix = np.column_stack((A, b))\n    \n    # Compute ranks\n    rank_A = np.linalg.matrix_rank(A)\n    rank_augmented = np.linalg.matrix_rank(augmented_matrix)\n    \n    # Check for consistency\n    if rank_A == rank_augmented:\n        if rank_A == A.shape[1]:\n            # Unique solution\n            solution = np.linalg.solve(A, b)\n            return \"Consistent and has a unique solution\", solution, None\n        else:\n            # Infinitely many solutions\n            particular_solution = np.linalg.lstsq(A, b, rcond=None)[0]\n            null_space = null_space_of_matrix(A)\n            return \"Consistent but has infinitely many solutions\", particular_solution, null_space\n    else:\n        return \"Inconsistent system (no solution)\", None, None\n\ndef null_space_of_matrix(A):\n    \"\"\"\n    Compute the null space of matrix A, which gives the set of solutions to Ax = 0.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix.\n    \n    Returns:\n    numpy.ndarray: Basis for the null space of A.\n    \"\"\"\n    u, s, vh = np.linalg.svd(A)\n    null_mask = (s &lt;= 1e-10)  # Singular values near zero\n    null_space = np.compress(null_mask, vh, axis=0)\n    return null_space.T\n\n\nExample 1: Solve \\[\\begin{align*}\n2x-y+z&=1\\\\\nx+2y&=3\\\\\n3x+2y+z&=4\n\\end{align*}\\]\n\n\n# Example usage 1: System with a unique solution\nA1 = np.array([[2, -1, 1], [1, 0, 2], [3, 2, 1]])\nb1 = np.array([1, 3, 4])\n\nstatus1, solution1, null_space1 = check_consistency(A1, b1)\nprint(\"Example 1 - Status:\", status1)\n\nif solution1 is not None:\n    print(\"Solution:\", solution1)\nif null_space1 is not None:\n    print(\"Null Space:\", null_space1)\n\nExample 1 - Status: Consistent and has a unique solution\nSolution: [0.27272727 0.90909091 1.36363636]\n\n\n\nExample 2: Solve the system of equations, \\[\\begin{align*}\nx+2y+z&=3\\\\\n2x+4y+2z&=6\\\\\nx+y+z&=2\n\\end{align*}\\]\n\n\n# Example usage 2: System with infinitely many solutions\nA2 = np.array([[1, 2, 1], [2, 4, 2], [1, 1, 1]])\nb2 = np.array([3, 6, 2])\n\nstatus2, solution2, null_space2 = check_consistency(A2, b2)\nprint(\"\\nExample 2 - Status:\", status2)\n\nif solution2 is not None:\n    print(\"Particular Solution:\", solution2)\nif null_space2 is not None:\n    print(\"Null Space (Basis for infinite solutions):\", null_space2)\n\n\nExample 2 - Status: Consistent but has infinitely many solutions\nParticular Solution: [0.5 1.  0.5]\nNull Space (Basis for infinite solutions): [[ 7.07106781e-01]\n [ 1.11022302e-16]\n [-7.07106781e-01]]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_2.html#module-review",
    "href": "module_2.html#module-review",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "2.3 Module review",
    "text": "2.3 Module review\n\nDefine the Hadamard Product in Linear Algebra and Provide a Suitable Example.\n\nHint: Element-wise product of matrices of the same dimensions.\n\nExample: For \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), \\(A \\circ B = \\begin{bmatrix} 5 & 12 \\\\ 21 & 32 \\end{bmatrix}\\).\n\nGive Two Applications of the Hadamard Product in Machine Learning.\n\nHint: Gradient updates and feature scaling in deep learning.\n\nFind the Outer Product of Two Vectors $S_1 = [1, 2, 7] $ and \\(S_2 = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\end{bmatrix}\\).\n\nHint: Compute \\(S_1 \\cdot S_2^T\\).\n\nDefine and Differentiate Between the Dot Product and the Outer Product of Two Vectors.\n\nHint: Dot product results in a scalar; outer product results in a matrix.\n\nWrite a Pseudocode to Compute the Hadamard Product of Two Matrices.\n\nHint: Use nested loops for element-wise multiplication.\n\nCompute the Row Norms of Matrix \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\).\n\nHint: Use \\(\\|A_{i*}\\| = \\sqrt{\\sum_j A_{ij}^2}\\).\n\nFind the Column Norms of Matrix \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\).\n\nHint: Use \\(\\|A_{*j}\\| = \\sqrt{\\sum_i A_{ij}^2}\\).\n\nCompute the Frobenius Norm of Matrix \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\).\n\nHint: \\(\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\\).\n\nProve the Hadamard Product Is Commutative and Associative.\n\nHint: Based on properties of element-wise operations.\n\nExplain the Use of the Hadamard Product in Convolutional Neural Networks.\n\nHint: Used in depth-wise convolutions and attention mechanisms.\n\nFor Vectors \\(u = [1, 2, 3]\\) and \\(v = [4, 5, 6]\\), Compute \\(u \\circ v\\) and \\(u \\cdot v\\).\n\nHint: Hadamard product is element-wise, dot product is summation.\n\nWrite Pseudocode for Outer Product of Two Vectors.\n\nHint: Compute $u_i v_j $ for all \\(i, j\\).\n\nDiscuss the Role of the Outer Product in Tensor Decomposition.\n\nHint: Used to represent rank-1 tensors.\n\nFind Hadamard Product of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 2 & 0 \\\\ 1 & 5 \\end{bmatrix}\\).\n\nHint: Multiply corresponding elements.\n\nUse Python to Compute Outer Product of $x = [1, 2] $ and \\(y = [3, 4]\\).\n\nHint: Use NumPy’s np.outer(x, y).\n\nWrite all the fundamental subspaces of \\(A = \\begin{bmatrix} 1 & 2 & 5 \\\\ 3 & 4 & 10 \\\\ 5 & 6 & 16 \\end{bmatrix}\\).\n\nHint: Determine the column space, null space, row space, and left null space using Gaussian elimination and rank of \\(A\\).\n\nIn a recommendation system, the user preference is \\(u = [4, 3, 5]\\) and the item score is \\(v = [2, 5, 4]\\). Find the user-item interaction score and its Frobenius norm.\n\nHint: Compute the outer product \\(u \\cdot v^T\\) and the Frobenius norm \\(\\|u \\cdot v^T\\|_F = \\sqrt{\\sum u_iv_j^2}\\).\n\nVerify the rank-nullity theorem for \\(A = \\begin{bmatrix} 7 & -3 & 5 \\\\ 9 & 11 & 2 \\\\ 16 & 8 & 7 \\end{bmatrix}\\).\n\nHint: Compute the rank of \\(A\\), the dimension of its null space, and verify \\(\\text{rank}(A) + \\text{nullity}(A) = \\text{number of columns of } A\\).\n\nDefine the Kronecker product of two matrices. Find the Kronecker product of \\(A = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 1 & 0 & 1 & 8 \\\\ 2 & -2 & 2 & -2 \\\\ -4 & 0 & 3 & -1 \\end{bmatrix}\\) in block matrix form.\n\nHint: Use the definition \\(A \\otimes B = \\begin{bmatrix} a_{11}B & a_{12}B \\\\ a_{21}B & a_{22}B \\end{bmatrix}\\).\n\nExplain and compute the rank of the Kronecker product of two matrices $A $ and \\(B\\), where $A $ is \\(2 \\times 2\\) and $B $ is \\(3 \\times 3\\).\n\nHint: Use the property \\(\\text{rank}(A \\otimes B) = \\text{rank}(A) \\cdot \\text{rank}(B)\\).\n\nFind the row space and column space of \\(A = \\begin{bmatrix} 2 & 4 & 6 \\\\ 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix}\\).\n\nHint: Row space is spanned by independent rows; column space is spanned by independent columns.\n\nDetermine if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\\) is invertible. If not, explain why using its fundamental subspaces.\n\nHint: Check if the null space is trivial or if the determinant of \\(A\\) is zero.\n\nWrite a pseudocode to calculate the Frobenius norm of any \\(m \\times n\\) matrix.\n\nHint: Use \\(\\|A\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n A_{ij}^2}\\).\n\nFind the projection of \\(b = \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}\\) onto the column space of \\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}\\).\n\nHint: Use \\(P = A(A^T A)^{-1}A^T b\\).\n\nWrite a pseudocode to compute the outer product of two vectors \\(x\\) and \\(y\\).\n\nHint: Use a nested loop or NumPy’s np.outer() function to compute the product.\n\n\n\n\n\n\nStrang, Gilbert. 2020. Linear Algebra for Everyone. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_2.html#footnotes",
    "href": "module_2.html#footnotes",
    "title": "2  Transforming Linear Algebra to Computational Language",
    "section": "",
    "text": "A regularization techniques in Deep learning. This approach deactivate some selected neurons to control model over-fitting↩︎\nRemember that the covariance of \\(X\\) is defined as \\(Cov(X)=\\dfrac{\\sum (X-\\bar{X})^2}{n-1}\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transforming Linear Algebra to Computational Language</span>"
    ]
  },
  {
    "objectID": "module_3.html",
    "href": "module_3.html",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "",
    "text": "3.1 Introduction to NumPy\nIn this section, we will introduce NumPy, the core library for scientific computing in Python. NumPy provides support for arrays, matrices, and a host of mathematical functions to operate on these structures. This is particularly useful for linear algebra computations, making it an essential tool in computational mathematics. The library also serves as the foundation for many other Python libraries like SciPy, Pandas, and Matplotlib.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#introduction-to-numpy",
    "href": "module_3.html#introduction-to-numpy",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "",
    "text": "3.1.1 Purpose of Using NumPy\nThe primary purpose of NumPy is to enable efficient numerical computations involving large datasets, vectors, and matrices. With NumPy, one can perform mathematical operations on arrays and matrices in a way that is highly optimized for performance, both in terms of memory and computational efficiency (Harris et al. 2020).\nSome key advantages of using NumPy include:\n\nEfficient handling of large datasets: Arrays in NumPy are optimized for performance and consume less memory compared to native Python lists.\nMatrix operations: NumPy provides built-in functions for basic matrix operations, allowing one to perform tasks like matrix multiplication, transpose, and inversion easily.\nLinear algebra: It includes functions for solving systems of equations, finding eigenvalues and eigenvectors, computing matrix factorizations, and more.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#basic-operations-in-numpy",
    "href": "module_3.html#basic-operations-in-numpy",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.2 Basic Operations in NumPy",
    "text": "3.2 Basic Operations in NumPy\nThis section will present several examples of using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. While the types of operations shown here may seem a bit dry and pedantic, they comprise the building blocks of many other examples used throughout the book. Get to know them well!\nWe’ll cover a few categories of basic array manipulations here:\n\nAttributes of arrays: Determining the size, shape, memory consumption, and data types of arrays\nIndexing of arrays: Getting and setting the value of individual array elements\nSlicing of arrays: Getting and setting smaller subarrays within a larger array\nReshaping of arrays: Changing the shape of a given array\nJoining and splitting of arrays: Combining multiple arrays into one, and splitting one array into many\n\n\n\n\n\n\n\nLoading numpy to a python programme\n\n\n\n\nSyntax\n\nimport numpy as \"name of instance\"\neg: import numpy as np\n\n\n\n3.2.0.1 Array Creation\nAt the core of NumPy is the ndarray object, which represents arrays and matrices. Here’s how to create arrays using NumPy:\nimport numpy as np\n\n# Creating a 1D array\narr = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D matrix\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"1D Array: \\n\", arr)\nprint(\"2D Matrix: \\n\", matrix)\n\n\n3.2.1 Define different types of numpy arrays\nAs the first step to understand different types of arrays in NumPy let us consider the following examples.\n\n3.2.1.1 1D Array (Vector)\nIn NumPy, a one-dimensional (1D) array is similar to a list or vector in mathematics. It consists of a single row or column of numbers, making it an ideal structure for storing sequences of values.\n\nimport numpy as np\n\n# Creating a 1D array\narr = np.array([1, 2, 3, 4])\nprint(arr)\n\n[1 2 3 4]\n\n\nHere, np.array() is used to create a 1D array (or vector) containing the values [1, 2, 3, 4]. The array represents a single sequence of numbers, and it is the basic structure of NumPy.\n\n\n\n\n\n\nUse:\n\n\n\nA 1D array can represent many things, such as a vector in linear algebra, a list of numbers, or a single dimension of data in a machine learning model.\n\n\n\n\n3.2.1.2 2D Array (Matrix)\nA two-dimensional (2D) array is equivalent to a matrix in mathematics. It consists of rows and columns and is often used to store tabular data or perform matrix operations.\n\nfrom IPython.display import display, HTML\n# Creating a 2D array (Matrix)\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\ndisplay(matrix)\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nIn this example, the 2D array (or matrix) is created using np.array() by providing a list of lists, where each list represents a row in the matrix. The result is a matrix with two rows and three columns.\n\n\n\n\n\n\nUse:\n\n\n\nMatrices are fundamental structures in linear algebra. They can represent anything from transformation matrices in graphics to coefficients in systems of linear equations.\n\n\n\n\n3.2.1.3 Zero Arrays\nZero arrays are used to initialize matrices or arrays with all elements set to zero. This can be useful when creating placeholder arrays where the values will be computed or updated later.\n\n# Creating an array of zeros\nzero_matrix = np.zeros((3, 3))\nprint(zero_matrix)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\nThe np.zeros() function creates an array filled with zeros. In this example, we create a 3x3 matrix with all elements set to zero.\n\n\n\n\n\n\nUse:\n\n\n\nZero arrays are commonly used in algorithms that require the allocation of memory for arrays that will be updated later.\n\n\n\n\n3.2.1.4 Identity Matrix\nAn identity matrix is a square matrix with ones on the diagonal and zeros elsewhere. It plays a crucial role in linear algebra, especially in solving systems of linear equations and matrix factorizations.\n\n# Creating an identity matrix\nidentity_matrix = np.eye(3)\nprint(identity_matrix)\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\nThe np.eye(n) function creates an identity matrix with the specified size. In this case, we create a 3x3 identity matrix, where all diagonal elements are 1, and off-diagonal elements are 0.\n\n\n3.2.1.5 Arange Function\nThe np.arange() function is used to create an array with evenly spaced values within a given range. It’s similar to Python’s built-in ``range() function but returns a NumPy array instead of a list.\n\n# Creating an array using arange\narr = np.arange(1, 10, 2)\nprint(arr)\n\n[1 3 5 7 9]\n\n\nHere, np.arange(1, 10, 2) generates an array of numbers starting at 1, ending before 10, with a step size of 2. The result is [1, 3, 5, 7, 9].\n\n\n\n\n\n\nUse:\n\n\n\nThis function is useful when creating arrays for loops, data generation, or defining sequences for analysis.\n\n\n\n\n3.2.1.6 Linspace Function\nThe np.linspace() function generates an array of evenly spaced values between a specified start and end, with the number of intervals defined by the user.\n\n# Creating an array using linspace\narr = np.linspace(0, 1, 5)\nprint(arr)\n\n[0.   0.25 0.5  0.75 1.  ]\n\n\nnp.linspace(0, 1, 5) creates an array with 5 evenly spaced values between 0 and 1, including both endpoints. The result is [0. , 0.25, 0.5 , 0.75, 1. ]. :::{.callout-note} ### Use: linspace() is often used when you need a specific number of evenly spaced points within a range, such as for plotting functions or simulating data. :::\n\n\n3.2.1.7 Reshaping Arrays\nThe reshape() function changes the shape of an existing array without changing its data. It’s useful when you need to convert an array to a different shape for computations or visualizations.\n\n# Reshaping an array\narr = np.arange(1, 10)\nreshaped_arr = arr.reshape(3, 3)\nprint(reshaped_arr)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\nIn this example, a 1D array with 9 elements is reshaped into a 3x3 matrix using the reshape() method. The data remains the same but is now structured in a 2D form.\n\n\n\n\n\n\nUse:\n\n\n\nReshaping is critical in linear algebra and machine learning when working with input data of different dimensions.\n\n\n\n\n3.2.1.8 Random Arrays\nNumPy’s random module is used to generate arrays with random values. These arrays are useful in simulations, testing algorithms, and initializing variables in machine learning.\n\n# Creating a random array\nrandom_arr = np.random.rand(3, 3)\nprint(random_arr)\n\n[[0.40396322 0.25081974 0.48257721]\n [0.5186337  0.62526675 0.92920372]\n [0.1719952  0.14067499 0.40751399]]\n\n\nnp.random.rand(3, 3) creates a 3x3 matrix with random values between 0 and 1. The rand() function generates random floats in the range \\([0, 1)\\).\n\n\n\n\n\n\nUse:\n\n\n\nRandom arrays are commonly used for initializing weights in machine learning algorithms, simulating stochastic processes, or for testing purposes.\n\n\n\n\n\n\n\n\nSyntax\n\n\n\n\nOne-Dimensional Array: np.array([list of values])\n\nTwo-Dimensional Array: np.array([[list of values], [list of values]])\n\nZero Array: np.zeros(shape)\n\nshape is a tuple representing the dimensions (e.g., (3, 3) for a 3x3 matrix).\n\nIdentity Matrix: np.eye(n)\n\nn is the size of the matrix.\n\nArrange Function: np.arange(start, stop, step)\n\nstart is the starting value, stop is the end value (exclusive), and step is the increment.\n\nLinspace Function: np.linspace(start, stop, num)\n\nstart and stop define the range, and num is the number of evenly spaced values.\n\nReshaping Arrays: np.reshape(array, new_shape)\n\narray is the existing array, and new_shape is the desired shape (e.g., (3, 4)).\n\nRandom Arrays without Using rand: np.random.randint(low, high, size)\n\nlow and high define the range of values, and size defines the shape of the array.\n\n\n\n\n\n\n\n3.2.2 Review Questions\nQ1: What is the purpose of using np.array() in NumPy?\nAns: np.array() is used to create arrays in NumPy, which can be 1D, 2D, or multi-dimensional arrays.\nQ2: How do you create a 2D array in NumPy?\nAns: A 2D array can be created using np.array([[list of values], [list of values]]).\nQ3: What is the difference between np.zeros() and np.eye()?\nAns: np.zeros() creates an array filled with zeros of a specified shape, while np.eye() creates an identity matrix of size n.\nQ4: What is the syntax to create an evenly spaced array using np.linspace()?\nAns: The syntax is np.linspace(start, stop, num), where num specifies the number of evenly spaced points between start and stop.\nQ5: How can you reshape an array in NumPy?\nAns: Arrays can be reshaped using np.reshape(array, new_shape), where new_shape is the desired shape for the array.\nQ6: How do you create a random integer array in a specific range using NumPy?\nAns: You can use np.random.randint(low, high, size) to generate a random array with integers between low and high, and size defines the shape of the array.\nQ7: What does the function np.arange(start, stop, step) do?\nAns: It generates an array of values from start to stop (exclusive) with a step size of step.\nQ8: What is array broadcasting in NumPy?\nAns: Array broadcasting allows NumPy to perform element-wise operations on arrays of different shapes by automatically expanding the smaller array to match the shape of the larger array.\nQ9: How do you generate a zero matrix of size 4x4 in NumPy?\nAns: A zero matrix of size 4x4 can be generated using np.zeros((4, 4)).\nQ10: What is the difference between np.arange() and np.linspace()?\nAns: np.arange() generates values with a specified step size, while np.linspace() generates evenly spaced values over a specified range and includes the endpoint.\n\n\n\n3.2.3 Tensors in NumPy\nA tensor is a generalized concept of matrices and vectors. In mathematical terms, tensors are multi-dimensional arrays, and their dimensionality (or rank) is what differentiates them from simpler structures like scalars (rank 0), vectors (rank 1), and matrices (rank 2). A tensor with three dimensions or more is often referred to as a higher-order tensor.\nIn practical terms, tensors can be seen as multi-dimensional arrays where each element is addressed by multiple indices. Tensors play a significant role in machine learning and deep learning frameworks, where operations on these multi-dimensional data structures are common.\n\n3.2.3.1 Types of Tensors:\n\nScalar (0-D Tensor): A single number.\n\nExample: 5 Rank: 0 Shape: ()\n\nVector (1-D Tensor): An array of numbers.\n\nExample: [1, 2, 3] Rank: 1 Shape: (3)\n\nMatrix (2-D Tensor): A 2D array (rows and columns).\n\nExample: [[1, 2, 3], [4, 5, 6]] Rank: 2 Shape: (2, 3)\n\n3-D Tensor: An array of matrices.\n\nExample: [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]] Rank: 3 Shape: (2, 2, 3)\n\nN-D Tensor: A tensor with N dimensions, where \\(N &gt; 3\\).\n\nExample: A 4-D tensor could represent data with shape (n_samples, n_channels, height, width) in image processing.\n\n\n3.2.3.2 Creating Tensors Using NumPy\nIn NumPy, tensors are represented as multi-dimensional arrays. You can create tensors in a way similar to how you create arrays, but you extend the dimensions to represent higher-order tensors.\nCreating a 1D Tensor (Vector)\nA 1D tensor is simply a vector. You can create one using np.array():\n\nimport numpy as np\nvector = np.array([1, 2, 3])\nprint(vector)\n\n[1 2 3]\n\n\nCreating a 2D Tensor (Matrix)\nA 2D tensor is a matrix:\n\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\nprint(matrix)\n\n[[1 2 3]\n [4 5 6]]\n\n\nCreating a 3D Tensor\nTo create a 3D tensor (a stack of matrices):\n\ntensor_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nprint(tensor_3d)\n\n[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n\n\nCreating a 4D Tensor\nIn applications like deep learning, a 4D tensor is often used to represent a batch of images, where the dimensions could be (batch_size, channels, height, width):\n\ntensor_4d = np.random.randint(10, size=(2, 3, 4, 5))  # 2 batches, 3 channels, 4x5 images\nprint(tensor_4d)\n\n[[[[1 2 1 3 8]\n   [7 6 6 6 3]\n   [9 6 0 9 3]\n   [9 3 5 3 2]]\n\n  [[4 7 7 4 8]\n   [7 8 9 8 6]\n   [5 3 7 5 8]\n   [8 1 7 7 0]]\n\n  [[3 2 9 7 6]\n   [1 4 9 5 9]\n   [2 6 4 9 4]\n   [5 8 1 5 7]]]\n\n\n [[[4 7 0 6 8]\n   [9 5 7 1 6]\n   [9 7 5 4 7]\n   [3 5 8 8 5]]\n\n  [[5 8 3 1 3]\n   [2 6 6 0 0]\n   [9 6 7 6 5]\n   [8 1 1 8 1]]\n\n  [[3 0 6 6 4]\n   [6 8 7 9 0]\n   [1 2 9 0 2]\n   [0 8 1 1 8]]]]\n\n\n\n\n\n\n\n\nGeneral Syntax for Creating Tensors Using NumPy\n\n\n\nnp.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n\nobject: An array-like object (nested lists) that you want to convert to a tensor.\ndtype: The desired data type for the tensor elements.\ncopy: Whether to copy the data (default True).\norder: Row-major (C) or column-major (F) order.\nndmin: Specifies the minimum number of dimensions for the tensor.\n\n\n\nIn the next section we will discuss the various attributes of the NumPy array.\n\n\n3.2.3.3 Attributes of arrays\nEach array has attributes ndim (the number of dimensions), shape (the size of each dimension), and size (the total size of the array):\nTo illustrate this attributes, consider the following arrays:\n\n#np.random.seed(0)  # seed for reproducibility\n\nx1 = np.random.randint(10, size=6)  # One-dimensional array\nx2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array\nx3 = np.random.randint(10, size=(3, 4, 5))  # Three-dimensional array\n\nThe array attributes of \\(x_3\\) is shown below.\n\nprint(\"x3 ndim: \", x3.ndim)\nprint(\"x3 shape:\", x3.shape)\nprint(\"x3 size: \", x3.size)\n\nx3 ndim:  3\nx3 shape: (3, 4, 5)\nx3 size:  60\n\n\nAnother useful attribute are the dtype which return the data type of the array , itemsize, which lists the size (in bytes) of each array element, and nbytes, which lists the total size (in bytes) of the array:\n\nprint(\"dtype:\", x3.dtype)\nprint(\"itemsize:\", x3.itemsize, \"bytes\")\nprint(\"nbytes:\", x3.nbytes, \"bytes\")\n\ndtype: int32\nitemsize: 4 bytes\nnbytes: 240 bytes\n\n\n\n\n\n3.2.4 Array Indexing: Accessing Single Elements\nIf you are familiar with Python’s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, the \\(i^{th}\\) value (counting from zero) can be accessed by specifying the desired index in square brackets, just as with Python lists:\nTo demonstrate indexing, let us consider the one dimensional array:\n\nx1=np.array([8, 5, 4, 7,4,1])\n\nThe fourth element of x1 can be accessed as\n\nprint(x1[3])\n\n7\n\n\nNow the second element from the end of the the arrray x1 can be accessed as:\n\nprint(x1[-2])\n\n4\n\n\n\n3.2.4.1 Acessing elements in multi-dimensional arrays\nIn a multi-dimensional array, items can be accessed using a comma-separated tuple of indices. An example is shown below.\n\nx2=np.array([[3, 3, 9, 2],\n       [5, 2, 3, 5],\n       [7, 2, 7, 1]])\nprint(x2)# list the 2-D array\n\n[[3 3 9 2]\n [5 2 3 5]\n [7 2 7 1]]\n\n\nNow print the third element in the first row, we will use the following code.\n\nx2[0, 2] ## access the element in first row and thrid column\n\n9\n\n\n\nx2[2, -1] ## access the element in the 3rd row and last column\n\n1\n\n\n\n\n3.2.4.2 Modification of array elements\nValues can also be modified using any of the above index notation. An example is shown below.\n\nx2[2, -1]=20 ## replace the 3rd row last column element of x2 by 20\nprint(x2)\n\n[[ 3  3  9  2]\n [ 5  2  3  5]\n [ 7  2  7 20]]\n\n\n\n\n\n\n\n\nHomogenity of data in NumPy arrays\n\n\n\nKeep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don’t be caught unaware by this behavior!\n\n\n\n\n3.2.4.3 Array Slicing: Accessing Subarrays\nJust as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon (:) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this:\nx[start:stop:step]\nIf any of these are unspecified, they default to the values start=0, stop=size of dimension, step=1.\nWe’ll take a look at accessing sub-arrays in one dimension and in multiple dimensions.\n1. One-dimensional subarrays\n\nx = np.arange(0,10)\nx\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nx[1:6]  # first five elements\n\narray([1, 2, 3, 4, 5])\n\n\n\nx[5:]  # elements after index 5\n\narray([5, 6, 7, 8, 9])\n\n\n\nx[4:7]  # middle sub-array\n\narray([4, 5, 6])\n\n\n\nx[::2]  # every other element with step 2 (alternate elements)\n\narray([0, 2, 4, 6, 8])\n\n\n2. Multi-dimensional subarrays (slicing)\nMulti-dimensional slices work in the same way, with multiple slices separated by commas. For example:\n\n# creating a two dimensional array\nx2=np.array([[1,2,3],[3,4,5],[5,6,7]])\nprint(x2)\n\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\n\n\n# selecting first 3 rows and first two columns from x2\nprint(x2[:3,:2])\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\nprint(x2[:3:2,:3:2]) # slice alternate elements in first three rows and first three columns\n\n[[1 3]\n [5 7]]\n\n\n\n\n\n\n\n\nAccessing array rows and columns\n\n\n\nOne commonly needed routine is accessing of single rows or columns of an array. This can be done by combining indexing and slicing, using an empty slice marked by a single colon (:)\n\n\nFor example all the elements in first column can be accessed as:\n\nprint(x2[:, 0])  # first column of x2\n\n[1 3 5]\n\n\n\n\n3.2.4.4 Creating copies of arrays\nDespite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method.\nThis concept can be illustrated through an example. Consider the array x2 previously defined:\n\nprint(x2)\n\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\n\nNow take a copy of a slice of x2 as follows.\n\n# create a copy of subarray and store it with the new name\nx2_sub_copy = x2[:2, :2].copy()\nprint(x2_sub_copy)\n\n[[1 2]\n [3 4]]\n\n\nNow the changes happend in the copy will not affect the orginal array. For example, replace one element in the copy slice and check how it is refelected in both arrays.\n\nx2_sub_copy[0, 0] = 42\nprint(x2_sub_copy)\n\n[[42  2]\n [ 3  4]]\n\n\n\nprint(x2)\n\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\n\n\n\n3.2.4.5 More on reshaping\nAnother useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape method. There are various approaches in reshaping of arrays. For example, if you want to put the numbers 1 through 9 in a \\(3 \\times 3\\) grid, you can do the following:\n\nnp.arange(1, 10)\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\ngrid = np.arange(1, 10).reshape((9, 1))\nprint(grid)\n\n[[1]\n [2]\n [3]\n [4]\n [5]\n [6]\n [7]\n [8]\n [9]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with non-contiguous memory buffers this is not always the case.\nAnother common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. This can be done with the reshape method, or more easily done by making use of the newaxis keyword within a slice operation:\n\n\nMore Examples\n\nx = np.array([1, 2, 3])\nprint(x)\n\n[1 2 3]\n\n\nNow check the dimension of the array created.\n\nx.shape\n\n(3,)\n\n\nReshaping the array as a matrix.\n\n# row vector via reshape\nx1=x.reshape((1, 3))\nx1.shape\n\n(1, 3)\n\n\nWe can achieve the same using the newaxis function as shown below.\n\n# row vector via newaxis\nprint(x[np.newaxis, :])\n\n[[1 2 3]]\n\n\nSome other similar operations are here.\n\n# column vector via reshape\nx.reshape((3, 1))\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n# column vector via newaxis\nx[:, np.newaxis]\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n\n\n3.2.5 Array Concatenation and Splitting\nAll of the preceding routines worked on single arrays. It’s also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays. We’ll take a look at those operations here.\n\n3.2.5.1 Concatenation of arrays\nConcatenation, or joining of two arrays in NumPy, is primarily accomplished using the routines np.concatenate, np.vstack, and np.hstack. np.concatenate takes a tuple or list of arrays as its first argument, as we can see here:\n\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nnp.concatenate([x, y])\n\narray([1, 2, 3, 3, 2, 1])\n\n\nAnother example is shown here:\n\nnp.concatenate([y, y, y])\n\narray([3, 2, 1, 3, 2, 1, 3, 2, 1])\n\n\nIt can also be used for two-dimensional arrays:\n\ngrid1 = np.array([[1, 2, 3],\n                 [4, 5, 6]])\ngrid2=np.array([[5,5,5],[7,7,7]])\n# concatenate along the first axis\nnm=np.concatenate([grid1, grid2],axis=0)\nnm.shape\nprint(nm)\n\n[[1 2 3]\n [4 5 6]\n [5 5 5]\n [7 7 7]]\n\n\nRow-wise concatenation is showm below.\n\n# concatenate along the second axis (horrizontal) (zero-indexed)\nnp.concatenate([grid1, grid2], axis=1)\n\narray([[1, 2, 3, 5, 5, 5],\n       [4, 5, 6, 7, 7, 7]])\n\n\nFor working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions:\n\nx = np.array([1, 2, 3])\ngrid = np.array([[9, 8, 7],\n                 [6, 5, 4]])\n\n# vertically stack the arrays\ngrid\n\narray([[9, 8, 7],\n       [6, 5, 4]])\n\n\nNow the new vector x has the same number of columns of grid. So we can only vertically stack it grid. For this the numpy function vstack will be used as follows.\n\ngrid2=np.vstack([grid,x])\nprint(grid2)\n\n[[9 8 7]\n [6 5 4]\n [1 2 3]]\n\n\nSimilarly the horrizontal stacking can be shown as follows.\n\n# horizontally stack the arrays\ny = np.array([[99],\n              [99],[3]])\nnp.hstack([grid2, y])\n\narray([[ 9,  8,  7, 99],\n       [ 6,  5,  4, 99],\n       [ 1,  2,  3,  3]])\n\n\n\n\n3.2.5.2 Splitting of arrays\nThe opposite of concatenation is splitting, which is implemented by the functions np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices giving the split points:\nLet’s begin with one dimensional arrays. First we split this array at specified locations and save it into sub arrays.\n\nx = [1, 2, 3, 99, 99, 3, 2, 1]\n\nNow split the list into two sub lists at index 2\n\nx1,x2=np.split(x,[2])\n\nNow see the sub-arrays:\n\nprint(\"the first array is:\", x1)\nprint(\"the second array is:\", x2)\n\nthe first array is: [1 2]\nthe second array is: [ 3 99 99  3  2  1]\n\n\nMore sub arrays can be created by passing the splitting locations as a list as follows.\n\nx1,x2,x3=np.split(x,[2,4])\nprint(x1,\"\\n\",x2,'\\n',x3)\n\n[1 2] \n [ 3 99] \n [99  3  2  1]\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that \\(N\\) split-points, leads to \\(N + 1\\) subarrays. The related functions np.hsplit and np.vsplit are similar:\n\n\nNow use the vsplit and hsplit functions on multi dimensional arrays.\n\ngrid = np.arange(16).reshape((4, 4))\ngrid\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n\n\n\n# vsplit\nupper, lower = np.vsplit(grid, [2])\nprint(upper)\nprint(lower)\n\n[[0 1 2 3]\n [4 5 6 7]]\n[[ 8  9 10 11]\n [12 13 14 15]]\n\n\n\n#hsplit\nleft, right = np.hsplit(grid, [2])\nprint(\"Left array:\\n\",left,\"\\n Right array:\\n\",right)\n\nLeft array:\n [[ 0  1]\n [ 4  5]\n [ 8  9]\n [12 13]] \n Right array:\n [[ 2  3]\n [ 6  7]\n [10 11]\n [14 15]]\n\n\n\n\n\n3.2.6 Review Questions\nShort Answer Questions (SAQ)\nQ1: What is the main purpose of the NumPy library in Python?\nAns: The main purpose of NumPy is to provide support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to perform operations on these arrays efficiently.\nQ2: How can a 1D array be created in NumPy?\nAns: A 1D array can be created using np.array() function, like:\nnp.array([1, 2, 3])\nQ3: How do you access the shape of a NumPy array?\nAns: You can access the shape of a NumPy array using the .shape attribute. For example, array.shape gives the dimensions of the array.\nQ4: What does the np.reshape() function do?\nAns: The np.reshape() function reshapes an array to a new shape without changing its data.\nQ5: Explain the difference between vstack() and hstack() in NumPy.\nAns: vstack() vertically stacks arrays (along rows), while hstack() horizontally stacks arrays (along columns).\nQ6: How does NumPy handle array slicing?\nAns: Array slicing in NumPy is done by specifying the start, stop, and step index like array[start:stop:step], which returns a portion of the array.\nQ7: What is the difference between the np.zeros() and np.ones() functions?\nAns: np.zeros() creates an array filled with zeros, while np.ones() creates an array filled with ones.\nQ8: What is array broadcasting in NumPy?\nAns: Broadcasting in NumPy allows arrays of different shapes to be used in arithmetic operations by stretching the smaller array to match the shape of the larger array.\nQ9: How can you stack arrays along a new axis in NumPy? Ans: You can use np.stack() to join arrays along a new axis.\nQ10: How do you generate a random integer array using NumPy?\nAns: You can generate a random integer array using np.random.randint(low, high, size).\n\nLong Answer Questions (LAQ)\nQ1: Explain how array slicing works in NumPy.\nAns: Array slicing in NumPy is a method to access or modify a subset of elements from a larger array. The syntax for slicing is array[start:stop:step], where:\nstart is the index from which slicing begins (inclusive), stop is the index where slicing ends (exclusive), step is the interval between indices to include in the slice. For example, in a 1D array, arr[1:5:2] will return every second element between the indices 1 and 4.\nQ2: Discuss the difference between the .reshape() function and the .ravel() function in NumPy.\nAns: The .reshape() function changes the shape of an array without modifying its data, allowing a multi-dimensional array to be flattened or reshaped into any compatible shape. On the other hand, .ravel() returns a flattened 1D version of an array, but it tries to avoid copying the data by returning a flattened view where possible. If modifying the flattened array is necessary, ravel() returns a copy instead.\nQ3: Explain how NumPy handles broadcasting during array operations.\nAns: Broadcasting in NumPy is a method to perform element-wise operations on arrays of different shapes. Smaller arrays are “broadcast” across the larger array by repeating their elements to match the shape of the larger array. For example, when adding a scalar to a 2D array, the scalar is added to each element of the array by broadcasting the scalar to match the array’s shape. Similarly, operations between arrays of different shapes follow the broadcasting rules to make them compatible.\nQ4: Describe how you would split an array in NumPy using the np.split() function. Provide an example.\nAns: The np.split() function in NumPy divides an array into multiple sub-arrays based on the indices provided. The syntax is: np.split(array, indices) Here, array is the array to be split, and indices is a list of indices where the split will occur. For example:\narr = np.array([1, 2, 3, 4, 5, 6])  \nnp.split(arr, [2, 4])  \nThis splits the array at indices 2 and 4, resulting in three sub-arrays: [1, 2], [3, 4], and [5, 6].\nQ5: What are the key differences between np.hsplit() and np.vsplit()? Provide examples.\nAns: np.hsplit() horizontally splits an array along its columns, while np.vsplit() vertically splits an array along its rows. For example, if we have a 2D array:\narr = np.array([[1, 2, 3], [4, 5, 6]])  \nnp.hsplit(arr, 3) splits the array into three columns, each with two rows: [[1], [4]], [[2], [5]], [[3], [6]]. np.vsplit(arr, 2) splits the array into two sub-arrays along rows: [[1, 2, 3]] and [[4, 5, 6]].\nQ6: How can you create a 2D array with random integers between 1 and 10 using NumPy? Provide an example.\nAns: A 2D array with random integers between 1 and 10 can be created using np.random.randint(low, high, size). Example:\nnp.random.randint(1, 10, size=(3, 3))\nThis generates a 3x3 array with random integers between 1 and 9.\nQ7: Describe how you would reshape an array from 1D to 2D in NumPy.\nAns: Reshaping an array from 1D to 2D in NumPy can be done using the .reshape() function. For example, given a 1D array:\narr = np.array([1, 2, 3, 4, 5, 6])  \nTo reshape it into a 2D array with 2 rows and 3 columns:\narr.reshape(2, 3)\nThis results in [[1, 2, 3], [4, 5, 6]].\nQ8: Explain the concept of stacking arrays in NumPy using np.stack(). Provide an example.\nAns: np.stack() joins arrays along a new axis, unlike hstack() and vstack(), which concatenate along existing axes. For example:\narr1 = np.array([1, 2, 3])  \narr2 = np.array([4, 5, 6])  \nnp.stack((arr1, arr2), axis=0)\nThis stacks the arrays along a new axis, resulting in [[1, 2, 3], [4, 5, 6]].\nQ9: How does NumPy’s array_split() differ from split()? Provide an example.\nAns: The array_split() function allows unequal splitting of an array, whereas split() requires the splits to result in equal-sized sub-arrays. For example:\narr = np.array([1, 2, 3, 4, 5])  \nnp.array_split(arr, 3)\nThis will split the array into three parts: [1, 2], [3, 4], and [5].\nQ10: How would you flatten a multi-dimensional array into a 1D array in NumPy?\nAns: You can flatten a multi-dimensional array using the .ravel() or .flatten() methods. Example using ravel():\narr = np.array([[1, 2], [3, 4]])  \narr.ravel()\nThis flattens the array into [1, 2, 3, 4].\nQ11: Discuss the importance of NumPy in scientific computing and how it handles large datasets efficiently.\nAns: NumPy is crucial in scientific computing because it provides efficient storage and operations for large datasets through its n-dimensional array objects. It uses continuous memory blocks, making array operations faster than traditional Python lists, and supports a variety of mathematical functions and broadcasting, which simplifies computation.\nNumPy operates efficiently by:\n\nAvoiding type checking at each operation due to its homogeneous data type constraint.\nLeveraging vectorization to reduce the need for explicit loops in operations.\nProviding optimized C and Fortran libraries for core computations.\n\nExample of large dataset handling:\nlarge_array = np.random.rand(1000000)\nsum_large_array = np.sum(large_array)  # Efficient summation\nThis efficiency makes NumPy a foundation for data-driven scientific applications like machine learning, signal processing, and simulations.\nQ12: What is the difference between a view and a copy in NumPy? Why does this matter in array operations?\nAns: A view is a reference to the original array, meaning changes in the view will affect the original array. A copy creates a new, independent array.\nExample:\narr = np.array([1, 2, 3])\nview = arr[:2]  # Creates a view\ncopy = arr[:2].copy()  # Creates a copy\nview[0] = 99  # This will change arr\nViews are more memory-efficient, but changes to them affect the original data, whereas copies do not.\nQ13: How are higher-dimensional arrays handled in NumPy, and how can they be reshaped and indexed? Provide a practical example.\nAns: Higher-dimensional arrays (tensors) in NumPy can be created and manipulated like 1D and 2D arrays. You can reshape tensors using reshape() and index them similarly, using one index for each dimension.\ntensor = np.arange(24).reshape(2, 3, 4)  # 3D tensor with shape (2, 3, 4)\nelement = tensor[1, 2, 3]  # Access element at specified indices\nwe can reshape tensors:\nreshaped_tensor = tensor.reshape(4, 6)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#some-important-numpy-function-for-linear-algrbra",
    "href": "module_3.html#some-important-numpy-function-for-linear-algrbra",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.3 Some important NumPy function for Linear Algrbra",
    "text": "3.3 Some important NumPy function for Linear Algrbra\nLet’s start with some basic matrix operations. Suppose we have two matrices, A and B, and we want to add them together. In NumPy, we can do this with the simple command, A+B. Let’s look into the detailed computational steps.\n\n# Addition and Subtraction\nmatrix1 = np.array([[1, 2], [3, 4]])\nmatrix2 = np.array([[5, 6], [7, 8]])\nprint(matrix1 + matrix2) # prints [[6, 8], [10, 12]]\n\n[[ 6  8]\n [10 12]]\n\n\nSimilary, matrix difference and other matrix operations can be illustrated as follows.\n\nprint(matrix1 - matrix2) # prints [[-4, -4], [-4, -4]]\n\n[[-4 -4]\n [-4 -4]]\n\n\n\n# Scalar Multiplication\nmatrix = np.array([[1, 2], [3, 4]])\nprint(2 * matrix) # prints [[2, 4], [6, 8]]\n\n[[2 4]\n [6 8]]\n\n\n\n# Matrix Multiplication\nmatrix1 = np.array([[1, 2], [3, 4]])\nmatrix2 = np.array([[5, 6], [7, 8]])\nprint(np.dot(matrix1, matrix2)) # prints [[19, 22], [43, 50]]\n\n[[19 22]\n [43 50]]\n\n\n\n# Matrix Hadamards product\nmatrix1 = np.array([[1, 2], [3, 4]])\nmatrix2 = np.array([[5, 6], [7, 8]])\nprint(matrix1*matrix2) \n\n[[ 5 12]\n [21 32]]\n\n\n\n# Transpose\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\nprint(np.transpose(matrix)) # prints [[1, 4], [2, 5], [3, 6]]\n\n[[1 4]\n [2 5]\n [3 6]]\n\n\n\n# inverse of a matrix\na = np.array([[1, 2], [3, 4]])\na_inv = np.linalg.inv(a)\n\nNext, let’s talk about vectors. A vector is simply a matrix with one column. They’re often used to represent things like forces or velocities in physics. In NumPy, we can represent vectors as arrays with one dimension.\n\nvector = np.array([1, 2, 3])\nprint(vector) # prints [1, 2, 3]\n\n[1 2 3]\n\n\nLet’s say we have two vectors, \\(\\vec{u}\\) and \\(\\vec{v}\\), and we want to compute their dot product (i.e., the sum of the products of their corresponding entries). We can do this with the command:\n\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\nprint(np.dot(vector1, vector2)) # prints 32\n\n32\n\n\nLike that there are some other operations too.\n\n# Cross Product\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\nprint(np.cross(vector1, vector2)) # prints [-3, 6, -3]\n\n[-3  6 -3]\n\n\n\n\n\n\n\n\nNote\n\n\n\nNorm: The norm of a vector is a scalar that represents the “length” of the vector. In NumPy, we can compute the norm using the numpy.linalg.norm function. The inner product of two vectors is a matrix that is computed by multiplying the first vector by the transpose of the second vector. In NumPy, we can compute the inner product using the numpy.inner function.\n\n\n\n# finding norm\nvector = np.array([1, 2, 3])\nprint(np.linalg.norm(vector)) # prints 3.74165738677\n\n3.7416573867739413\n\n\n\n#finding inner product\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\nprint(np.inner(vector1, vector2)) # prints 32\n\n32\n\n\nTo handle higher dimensional mutrix multiplication, one can use matmul() function. The np.matmul() function is another way to perform matrix multiplication. Unlike np.dot(), it handles higher-dimensional arrays correctly by broadcasting. The syntax for this operation is np.matmul(a, b).\n\nA = np.array([[1, 0], [0, 1]])\nB = np.array([[4, 1], [2, 2]])\n\n# Matrix multiplication using matmul\nresult = np.matmul(A, B)\nprint(result)\n\n[[4 1]\n [2 2]]\n\n\nThe function np.linalg.inv() computes the inverse of a square matrix. Syntax for this function is np.linalg.inv(a). An example is shown below.\n\nA = np.array([[1, 2], [3, 4]])\n\n# Compute inverse\ninv_A = np.linalg.inv(A)\nprint(inv_A)\n\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\nThe np.linalg.det() function computes the determinant of a square matrix. The determinant is useful for solving linear systems and understanding matrix properties. The syntax is np.linalg.det(a).\n\nA = np.array([[1, 2], [3, 4]])\n\n# Compute determinant\ndet_A = np.linalg.det(A)\nprint(det_A)\n\n-2.0000000000000004\n\n\nThe np.linalg.solve() function solves a linear matrix equation or system of linear scalar equations. It finds the vector x that satisfies \\(Ax = b\\). Syntax for this function is np.linalg.solve(A, b).\n\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])\n\n# Solve system of equations\nx = np.linalg.solve(A, b)\nprint(x)\n\n[2. 3.]\n\n\nThis function computes the QR decomposition of a matrix. QR decomposition is used to solve linear systems, least squares problems, and compute eigenvalues. Syntax for this function is np.linalg.qr().\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# QR decomposition\nQ, R = np.linalg.qr(A)\nprint(\"Q:\", Q)\nprint(\"R:\", R)\n\nQ: [[-0.12309149  0.90453403  0.40824829]\n [-0.49236596  0.30151134 -0.81649658]\n [-0.86164044 -0.30151134  0.40824829]]\nR: [[-8.12403840e+00 -9.60113630e+00 -1.10782342e+01]\n [ 0.00000000e+00  9.04534034e-01  1.80906807e+00]\n [ 0.00000000e+00  0.00000000e+00 -8.88178420e-16]]\n\n\nThe np.linalg.lstsq() function solves a linear least-squares problem, which is useful in regression tasks. The syntax for the function is np.linalg.lstsq(a, b, rcond=None).\n\nA = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\nb = np.array([6, 8, 9, 11])\n\n# Least-squares solution\nx, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\nprint(\"Solution:\", x)\n\nSolution: [2.09090909 2.54545455]\n\n\nThe Kronecker product is a matrix operation used in various applications like tensor products and matrix calculus. Syntax for this function is np.kron(a, b).\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[0, 5], [6, 7]])\n\nkronecker = np.kron(A, B)\nprint(kronecker)\n\n[[ 0  5  0 10]\n [ 6  7 12 14]\n [ 0 15  0 20]\n [18 21 24 28]]\n\n\nCosine similarity is used to find the cosine of the angle between two vectors. The mathematical formula for this operation is \\(\\cos \\theta=\\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}\\cdot\\vec{b}|}\\). The python function to calculate the cosine similarity is shown below.\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\ncosine_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\nprint(cosine_sim)\n\n0.9746318461970762\n\n\n\n3.3.1 Linear Regression using NumPy\nThe np.polyfit() function fits a polynomial of a specified degree to the data, making it useful for regression. Syntax for this function is np.polyfit(x, y, deg). Where \\(x\\) Independent variable (input), \\(y\\) is the dependent variable (output) and deg degree of the fitting polynomial. A simple example is given below.\n\nx = np.array([0, 1, 2, 3, 4])\ny = np.array([1, 3, 5, 7, 9])\n\n# Linear fit (degree 1)\ncoefficients = np.polyfit(x, y, 1)\nprint(\"Coefficients:\", coefficients)\n\nCoefficients: [2. 1.]\n\n\n\n\n3.3.2 Some interesting handy matrix operations using numpy arrays\nIn matrix decomposition, we need the matrix representation, \\(A-\\lambda I\\). For any matrix, we can do this by just A-lambda np.eye(3). This can be deomonstarted here.\n\nA=np.array([[1,2,3],[3,4,5],[7,6,7]])\nlamda=3\nA-lamda*np.eye(3)\n\narray([[-2.,  2.,  3.],\n       [ 3.,  1.,  5.],\n       [ 7.,  6.,  4.]])\n\n\n\nTask: Create a matrix, \\(A\\) using numpy and find the covariance , \\(cov(A)\\) using matrix operation.\n\n\n# creating a random matrix\nimport numpy as np\nA=np.arange(16).reshape(4,4)\nA\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n\n\nNow find \\(A-\\bar{A}\\).\n\nA_bar=np.mean(A,axis=0) # calculating column-wise sum\nA_bar\n\narray([6., 7., 8., 9.])\n\n\n\n# calculating A-A bar with outer product opertation\nprint(A-np.outer(A_bar,np.ones(4)).T)\n\n[[-6. -6. -6. -6.]\n [-2. -2. -2. -2.]\n [ 2.  2.  2.  2.]\n [ 6.  6.  6.  6.]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe same can be done using reshaping method\n\n#calculating A-A_bar using broadcasting\nX=A-np.mean(A,axis=0).reshape(1,4)\nprint(X)\n\n[[-6. -6. -6. -6.]\n [-2. -2. -2. -2.]\n [ 2.  2.  2.  2.]\n [ 6.  6.  6.  6.]]\n\n\n\n\nCalculating the covariance.\n\n#mannualy calculating covariance\nCoV=(1/3)*np.dot(X.T,X)\nCoV\n\narray([[26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667]])\n\n\nWe can verify the same using default function as follows.\n\n#calculating covariance using numpy function\nnp.cov(A, rowvar=False)\n\narray([[26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667],\n       [26.66666667, 26.66666667, 26.66666667, 26.66666667]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is interesting to compare the two ways of flattening an array using reshape().\n\n#comparing two ways of flattening a matrix using numpy\nA.reshape(-1)==A.reshape(16,)\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#basics-of-scipy-library-for-computational-linear-algebra",
    "href": "module_3.html#basics-of-scipy-library-for-computational-linear-algebra",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.4 Basics of SciPy Library for Computational Linear Algebra",
    "text": "3.4 Basics of SciPy Library for Computational Linear Algebra\nFollowing the comprehensive exploration of the NumPy library, which forms the foundation of array operations and basic linear algebra computations, it is essential to expand into more advanced tools for scientific computing. The SciPy library builds upon NumPy, offering a vast collection of functions and utilities specifically designed for higher-level operations in scientific and technical computing. While NumPy provides efficient array handling and basic matrix operations, SciPy extends these capabilities by incorporating advanced functions for optimization, integration, interpolation, and linear algebra, among other tasks (Virtanen et al. 2020).\nFor computational linear algebra, SciPy provides specialized modules like scipy.linalg, which can handle everything from solving linear systems to eigenvalue decompositions and matrix factorizations. This transition from NumPy to SciPy enables the handling of more complex problems efficiently and allows users to leverage optimized algorithms for large-scale numerical computations. By integrating SciPy into the workflow, computations can be carried out more robustly, expanding on the basic linear algebra concepts introduced through NumPy with advanced techniques necessary for practical applications.\n\n3.4.1 Basic Matrix operations\nSciPy builds on the functionality of NumPy, offering more sophisticated and optimized algorithms, particularly suited for numerical computing tasks. While NumPy provides essential operations for linear algebra, SciPy’s scipy.linalg module extends these capabilities with more advanced functions. SciPy functions are often better optimized for large-scale systems, making them highly efficient for computational linear algebra applications.\nAs the first step to use SciPy, we need to import the (only) necessary submodules for our specific tasks. In our discussion, we consider only linear algebra. So we import the linalg submodule as follows\n\n#import scipy\nimport numpy as np # for matrix definition\nfrom scipy import linalg\n#print(scipy.__version__) # check version\n\nNow let’s discuss various SciPy functions for linear algebra with examples.\n\n3.4.1.1 Computing the Determinant\nThe determinant is a scalar value that can be computed from the elements of a square matrix and is often used to determine whether a system of linear equations has a unique solution.\n\nSyntax: scipy.linalg.det(A)\n\n\n# example\nA = np.array([[1, 2], [3, 4]])\ndet_A = linalg.det(A)\nprint(det_A)\n\n-2.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nSimilar functionality is provided by np.linalg.det(). However, scipy.linalg.det() is often preferred when working with very large matrices due to the efficiency of SciPy’s backend implementations.\n\n\n\n\n3.4.1.2 Solving Linear Systems of Equations\nOne of the fundamental tasks in linear algebra is solving a system of linear equations of the form \\(AX = b\\), where \\(A\\) is a matrix and \\(b\\) is a vector or matrix of known values. &gt;Syntax: scipy.linalg.solve(A, b)\n\n#example\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])\nx = linalg.solve(A, b)\nprint(x)\n\n[2. 3.]\n\n\n\n\n\n\n\n\nNote\n\n\n\nNumPy’s np.linalg.solve() also provides this functionality, but SciPy’s version is better suited for larger and more complex matrices because it uses more efficient algorithms for decomposing the matrix.\n\n\n\n\n3.4.1.3 Matrix Inversion\nMatrix inversion is a critical operation in many linear algebra problems, particularly in solving systems of linear equations. &gt;Syntax: scipy.linalg.inv(A)\n\nA = np.array([[1, 2], [3, 4]])\ninv_A = linalg.inv(A)\nprint(inv_A)\n\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n3.4.1.4 Kronecker Product\nThe Kronecker product is used in various applications, including constructing block matrices and expanding the dimensionality of matrices.\n\nSyntax: scipy.linalg.kron(A, B)\n\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[0, 5], [6, 7]])\nkron_product = linalg.kron(A, B)\nprint(kron_product)\n\n[[ 0  5  0 10]\n [ 6  7 12 14]\n [ 0 15  0 20]\n [18 21 24 28]]\n\n\n\n\n3.4.1.5 Eigenvalues and Eigenvectors\nEigenvalues and eigenvectors are fundamental in many areas of linear algebra, including solving systems of differential equations and performing dimensionality reduction in machine learning.\n\nSyntax: scipy.linalg.eig(A)\n\n\nA = np.array([[3, 2], [4, 1]])\neigenvalues, eigenvectors = linalg.eig(A)\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\", eigenvectors)\n\nEigenvalues: [ 5.+0.j -1.+0.j]\nEigenvectors: [[ 0.70710678 -0.4472136 ]\n [ 0.70710678  0.89442719]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#sparse-matrices",
    "href": "module_3.html#sparse-matrices",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.5 Sparse Matrices",
    "text": "3.5 Sparse Matrices\nSparse matrices are often useful in numerical simulations dealing with large systems, if the problem can be described in matrix form where the matrices or vectors mostly contains zeros. Scipy has a good support for sparse matrices, with basic linear algebra operations (such as equation solving, eigenvalue calculations, etc.).\nThere are many possible strategies for storing sparse matrices in an efficient way. Some of the most common are the so-called coordinate form (COO), list of list (LIL) form, and compressed-sparse column CSC (and row, CSR). Each format has some advantages and disadvantages. Most computational algorithms (equation solving, matrix-matrix multiplication, etc.) can be efficiently implemented using CSR or CSC formats, but they are not so intuitive and not so easy to initialize. So often a sparse matrix is initially created in COO or LIL format (where we can efficiently add elements to the sparse matrix data), and then converted to CSC or CSR before used in real calculations.\nFor more information about these sparse formats, see e.g. http://en.wikipedia.org/wiki/Sparse_matrix\n\n3.5.1 Sparse Matrix operations in SciPy\nSparse matrices are a key feature of SciPy, providing an efficient way to store and manipulate large matrices with a significant number of zero elements. SciPy offers a variety of sparse matrix formats and supports operations like matrix multiplication, addition, transposition, and solving systems of equations.\nHere is a guide to working with sparse matrices in SciPy.\nTypes of Sparse Matrices in SciPy\nSciPy provides different types of sparse matrices depending on the use case:\n\nCSR (Compressed Sparse Row) Matrix: Efficient for row slicing and matrix-vector products.\nCSC (Compressed Sparse Column) Matrix: Efficient for column slicing and fast arithmetic operations.\nCOO (Coordinate) Matrix: Suitable for constructing sparse matrices by specifying individual entries.\nDIA (Diagonal) Matrix: For matrices where non-zero elements are primarily on the diagonals.\nLIL (List of Lists) Matrix: Good for constructing matrices incrementally.\n\n\n3.5.1.1 Importing Sparse Matrices\n\nfrom scipy.sparse import csr_matrix, csc_matrix, coo_matrix\nimport numpy as np\n\nCreating Sparse Matrices\n\n# Create a dense matrix\ndense_matrix = np.array([[0, 0, 3], [4, 0, 0], [0, 5, 6]])\n\n# Convert dense matrix to CSR format\ncsr = csr_matrix(dense_matrix)\n\n# Display CSR matrix\nprint(csr)\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int32'\n    with 4 stored elements and shape (3, 3)&gt;\n  Coords    Values\n  (0, 2)    3\n  (1, 0)    4\n  (2, 1)    5\n  (2, 2)    6\n\n\n\n# creating sparse matrix in COO format\n# Define row indices, column indices, and values\nrow = np.array([0, 1, 2, 2])\ncol = np.array([2, 0, 1, 2])\ndata = np.array([3, 4, 5, 6])\n\n# Create COO sparse matrix\ncoo = coo_matrix((data, (row, col)), shape=(3, 3))\n\nprint(coo)\n\n&lt;COOrdinate sparse matrix of dtype 'int32'\n    with 4 stored elements and shape (3, 3)&gt;\n  Coords    Values\n  (0, 2)    3\n  (1, 0)    4\n  (2, 1)    5\n  (2, 2)    6\n\n\nBasic Operations with Sparse Matrices\nThe basic matrix operations can be performed on the sparse matrix too. The difference is that in the case of sparse matices, the respective operations will be done only on non-zero enties. Now look into the basic matrix operations thorugh following examples.\nMatrix Multiplication:\n\nA = csr_matrix([[1, 0, 0], [0, 0, 1], [0, 2, 0]])\nB = csr_matrix([[4, 5], [0, 0], [7, 8]])\n\n# Matrix multiplication (dot product)\nresult = A.dot(B)\nprint(result.toarray())  # Convert to dense array for display\n\n[[4 5]\n [7 8]\n [0 0]]\n\n\nTransposition:\n\n# Transpose the matrix\ntransposed = A.transpose()\n\nprint(transposed.toarray())\n\n[[1 0 0]\n [0 0 2]\n [0 1 0]]\n\n\nAddition:\n\n# Adding two sparse matrices\nC = csr_matrix([[0, 1, 2], [3, 0, 0], [0, 0, 5]])\nD = csr_matrix([[0, 1, 0], [0, 0, 0], [2, 0, 5]])\n\nsum_matrix = C + D\n\nprint(sum_matrix.toarray())\n\n[[ 0  2  2]\n [ 3  0  0]\n [ 2  0 10]]\n\n\nSolving Sparse Linear Systems\nWe can solve systems of linear equations using sparse matrices with the spsolve() function:\n\nfrom scipy.sparse.linalg import spsolve\n\n# Create a sparse matrix (A) and a dense vector (b)\nA = csr_matrix([[3, 1, 0], [1, 2, 0], [0, 0, 1]])\nb = np.array([5, 5, 1])\n\n# Solve the system Ax = b\nx = spsolve(A, b)\n\nprint(\"Solution x:\", x)\n\nSolution x: [1. 2. 1.]\n\n\n\n\n3.5.1.2 Conversion from one sparse matrix system to another\nWe can convert between different sparse matrix formats using the .tocsc(),.tocoo(), .todia(), and similar methods:\n\n# Convert CSR to COO format\ncoo = A.tocoo()\nprint(coo)\nprint(\"The matrix is :\\n\",coo.toarray())\n\n&lt;COOrdinate sparse matrix of dtype 'int32'\n    with 5 stored elements and shape (3, 3)&gt;\n  Coords    Values\n  (0, 0)    3\n  (0, 1)    1\n  (1, 0)    1\n  (1, 1)    2\n  (2, 2)    1\nThe matrix is :\n [[3 1 0]\n [1 2 0]\n [0 0 1]]\n\n\n\n\n\n\n\n\nSummary of sparse matrix operations\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ncsr_matrix()\nCompressed Sparse Row matrix.\n\n\ncsc_matrix()\nCompressed Sparse Column matrix.\n\n\ncoo_matrix()\nCoordinate format matrix.\n\n\nspsolve()\nSolves sparse linear systems.\n\n\nspdiags()\nExtracts or constructs diagonal sparse matrices.\n\n\nlil_matrix()\nList of lists sparse matrix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#visualization-libraries",
    "href": "module_3.html#visualization-libraries",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.6 Visualization Libraries",
    "text": "3.6 Visualization Libraries\nData visualization libraries in Python empower developers and data scientists to create compelling visual representations of data. Popular libraries include Matplotlib, which offers versatile 2-D plotting capabilities, Seaborn for statistical graphics, Bokeh for interactive web applications, Altair for declarative visualizations, and Plotly for web-based interactive charts and dashboards.\n\n3.6.1 Matplotlib: A Comprehensive Data Visualization Library in Python\nMatplotlib is a powerful Python library that enables developers and data scientists to create a wide range of static, animated, and interactive visualizations. Whether we’re exploring data, presenting insights, or building scientific plots, Matplotlib has we covered. Let’s delve into its features:\n\nPublication-Quality Plots: Matplotlib allows us to create professional-quality plots suitable for research papers, presentations, and publications. Customize colors, fonts, and styles to match our requirements.\nVersatility: With Matplotlib, one can create line plots, scatter plots, bar charts, histograms, pie charts, and more. It supports 2-D plotting and can handle complex visualizations.\nInteractive Figures: While it’s known for static plots, Matplotlib also offers interactivity. We can zoom, pan, and explore data points within the plot.\nCustomization: Fine-tune every aspect of our plot, from axis labels and titles to grid lines and legends. Annotations and text can be added seamlessly.\nIntegration with Jupyter: Matplotlib integrates well with Jupyter notebooks, making it a favorite among data scientists and analysts.\n\n\n\n\n\n\n\nBackground\n\n\n\nMatplotlib is a multi-platform data visualization library built on NumPy arrays, and designed to work with the broader SciPy stack. It was conceived by John Hunter in 2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting via gnuplot from the IPython command line. IPython’s creator, Fernando Perez, was at the time scrambling to finish his PhD, and let John know he wouldn’t have time to review the patch for several months. John took this as a cue to set out on his own, and the Matplotlib package was born, with version 0.1 released in 2003. It received an early boost when it was adopted as the plotting package of choice of the Space Telescope Science Institute (the folks behind the Hubble Telescope), which financially supported Matplotlib’s development and greatly expanded its capabilities.\nOne of Matplotlib’s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large user base, which in turn has led to an active developer base and Matplotlib’s powerful tools and ubiquity within the scientific Python world.\nIn recent years, however, the interface and style of Matplotlib have begun to show their age. Newer tools like ggplot and ggvis in the R language, along with web visualization toolkits based on D3js and HTML5 canvas, often make Matplotlib feel clunky and old-fashioned. Still, I’m of the opinion that we cannot ignore Matplotlib’s strength as a well-tested, cross-platform graphics engine. Recent Matplotlib versions make it relatively easy to set new global plotting styles , and people have been developing new packages that build on its powerful internals to drive Matplotlib via cleaner, more modern APIs—for example, Seaborn , ggpy, HoloViews, Altair, and even Pandas itself can be used as wrappers around Matplotlib’s API. Even with wrappers like these, it is still often useful to dive into Matplotlib’s syntax to adjust the final plot output. For this reason, I believe that Matplotlib itself will remain a vital piece of the data visualization stack, even if new tools mean the community gradually moves away from using the Matplotlib API directly.\n\n\n\n3.6.1.1 General Matplotlib Tips\nBefore we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package.\n\nImporting the matplotlib module\n\nJust as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports:\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\n\n\n\n\n\n\nNote\n\n\n\nThe plt interface is what we will use most often, as we shall see throughout this chapter.\n\n\n\n\n3.6.1.2 Setting Styles\nWe will use the plt.style directive to choose appropriate aesthetic styles for our figures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style:\nplt.style.use('classic')\nplt.style.use('default')\nplt.style.use('seaborn')\nThroughout this section, we will adjust this style as needed. Note that the stylesheets used here are supported as of Matplotlib version 1.5; if you are using an earlier version of Matplotlib, only the default style is available.\nA simple example of loading the matplotlib module and setting theme is shown below.\n\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n\n\n\n\n3.6.2 How to Display Your Plots?\nA visualization you can’t see won’t be of much use, but just how you view your Matplotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in a Jupyter notebook.\n\n\n3.6.3 Plotting from a Jupyter Notebook\nThe Jupyter notebook is a browser-based interactive data analysis tool that can combine narrative, code, graphics, HTML elements, and much more into a single executable document.\nIf you are using Matplotlib from within a script, the function plt.show is your friend. plt.show starts an event loop, looks for all currently active Figure objects, and opens one or more interactive windows that display your figure or figures.\nThe plt.show command does a lot under the hood, as it must interact with your system’s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you.\nOne thing to be aware of: the plt.show command should be used only once per Python session, and is most often seen at the very end of the script. Multiple show commands can lead to unpredictable backend-dependent behavior, and should mostly be avoided.\n\n\n\n\n\n\nNote\n\n\n\nUsing plt.show in IPython’s Matplotlib mode is not required.\n\n\nPlotting interactively within a Jupyter notebook can be done with the %matplotlib command, and works in a similar way to the IPython shell. You also have the option of embedding graphics directly in the notebook, with two possible options:\n\n%matplotlib inline will lead to static images of your plot embedded in the notebook.\n%matplotlib notebook will lead to interactive plots embedded within the notebook.\n\nFor this discussion, we will generally stick with the default, with figures rendered as static images (see the following figure for the result of this basic plotting example):\n\n%matplotlib inline\n\n\nplt.plot([1,2,3,4])\nplt.show()\n\n\n\n\n\n\n\n\n\n3.6.3.1 Adding titles, axis labels, and a legend\nLet’s redraw this plot but now with a title, axis labels, and a legend:\n\nx_vals = [1,2,3,4]\nplt.plot(x_vals, label=\"An awesome line\")\nplt.ylabel('The y-axis label!')\nplt.xlabel('The x-axis label!')\nplt.title(\"The title of the graph!\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.6.3.2 Adding both x and y data\nYou may be wondering why the x-axis ranges from 0-3 and the y-axis from 1-4. If you provide a single list or array to the plot() command, matplotlib assumes it is a sequence of y values, and automatically generates the x values for you.\nplot() is a versatile command, and will take an arbitrary number of arguments. For example, to plot x versus y, you can issue the command:\n\nx_vals = [1,2,3,4]\ny_vals = [1, 4, 9, 16]\nplt.plot(x_vals, y_vals)\nplt.show()\n\n\n\n\n\n\n\n\nMore explicit examples are shown below.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()  # You must call plt.show() to make graphics appear.\n\n\n\n\n\n\n\n\n\nimport numpy as np\nx = np.linspace(0, 10, 100)\n\nfig = plt.figure()\nplt.plot(x, np.sin(x), '-')\nplt.plot(x, np.cos(x), '--');\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s change the theme to seaborn and create more plots with additional features.\n\nplt.style.use('seaborn-v0_8')\n\n\nimport numpy as np\nfig=plt.figure()\nx = np.linspace(0, 10, 100)\nplt.plot(x, np.sin(x), 'r-', label=r'$\\sin(x)$') # r stands for colour and r in label stands for row text\nplt.plot(x, np.cos(x), 'c--', label=r'$\\cos(x)$')\nplt.title(r'Plots of $\\sin(x)$ and $\\cos(x)$' )\nplt.axis('tight')\nplt.legend(frameon=True, loc='upper right', ncol=1,framealpha=.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving Figures to File\n\n\n\nOne nice feature of Matplotlib is the ability to save figures in a wide variety of formats. Saving a figure can be done using the savefig() command. In savefig(), the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. The list of supported file types can be found for your system by using the following method of the figure canvas object. Following function return all supported formats.\n\nfig.canvas.get_supported_filetypes()\n\n{'eps': 'Encapsulated Postscript',\n 'jpg': 'Joint Photographic Experts Group',\n 'jpeg': 'Joint Photographic Experts Group',\n 'pdf': 'Portable Document Format',\n 'pgf': 'PGF code for LaTeX',\n 'png': 'Portable Network Graphics',\n 'ps': 'Postscript',\n 'raw': 'Raw RGBA bitmap',\n 'rgba': 'Raw RGBA bitmap',\n 'svg': 'Scalable Vector Graphics',\n 'svgz': 'Scalable Vector Graphics',\n 'tif': 'Tagged Image File Format',\n 'tiff': 'Tagged Image File Format',\n 'webp': 'WebP Image Format'}\n\n\nNote that when saving your figure, it’s not necessary to use plt.show() or related commands discussed earlier.\n\n\nFor example, to save the previous figure as a PNG file, you can run this:\n\nfig.savefig('my_figure.png')\n\nTo confirm that it contains what we think it contains, let’s use the IPython Image object to display the contents of this file:\n\nfrom IPython.display import Image\nImage('my_figure.png')\n\n\n\n\n\n\n\n\n\n\n3.6.3.3 MATLAB-style Interface\nMatplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot (plt) interface. For example, the following code will probably look quite familiar to MATLAB users:\n\n\n3.6.3.4 Plotting multiple charts\nYou can create multiple plots within the same figure by using subplot\nLet’s consider an example of two plots on same canvas.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some fake data.\nx1 = np.linspace(0.0, 5.0)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\nx2 = np.linspace(0.0, 2.0)\ny2 = np.cos(2 * np.pi * x2)\nplt.plot(x1,y1,label=\"first graph\")\nplt.plot(x2,y2,label=\"second graph\")\nplt.axis('tight')\nplt.legend(frameon=True, loc='upper right', ncol=1,framealpha=.7)\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s represent these two plots in seperate subplots as shown below.\n\nfig, (ax1, ax2) = plt.subplots(2, 1)\nfig.suptitle('A tale of 2 subplots')\n\nax1.plot(x1, y1, 'o-')\nax1.set_ylabel('Damped oscillation')\n\nax2.plot(x2, y2, '.-')\nax2.set_xlabel('time (s)')\nax2.set_ylabel('Undamped')\n\nplt.show()\n\n\n\n\n\n\n\n\nAnother approach is shown below.\n\nplt.subplot(2, 1, 1)\nplt.plot(x1, y1, 'o-')\nplt.title('A tale of 2 subplots')\nplt.ylabel('Damped oscillation')\n\nplt.subplot(2, 1, 2)\nplt.plot(x2, y2, '.-')\nplt.xlabel('time (s)')\nplt.ylabel('Undamped')\n\nplt.show()\n\n\n\n\n\n\n\n\nAnother example is shown below.\n\n# First create a grid of plots\n# ax will be an array of two Axes objects\nplt.style.use('default')\nfig, ax = plt.subplots(2)\n\n# Call plot() method on the appropriate object\nax[0].plot(x, np.sin(x),'r--',label=r'$\\sin x$')\nax[0].set_title(r'$\\sin $ graph')\nax[1].set_xlabel(\"x values\")\nax[0].set_ylabel(r'$y=\\sin x$')\nax[1].plot(x, np.cos(x),'g-',label=r'$\\cos x$')\nax[1].set_ylabel(r'$y=\\cos x$')\nplt.axis('tight')\nplt.legend(frameon=True, loc='upper right', ncol=1,framealpha=.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.6.3.5 Simple Scatter Plots\nAnother commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. We’ll start by setting up the notebook for plotting and importing the functions we will use:\n\nimport numpy as np\nx = np.linspace(0, 10, 100)\nfig = plt.figure()\nplt.plot(x, np.sin(x), '-o', label=r'$\\sin(x)$',)\nplt.plot(x, np.cos(x), 'p', label=r'$\\cos(x)$')\nplt.title('Plots of $\\sin(x)$ and $\\cos(x)$' )\nplt.axis('tight')\nplt.legend(frameon=True, loc='upper right', ncol=1,framealpha=.7)\nplt.show()\n\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_2100\\3327894231.py:6: SyntaxWarning: invalid escape sequence '\\s'\n  plt.title('Plots of $\\sin(x)$ and $\\cos(x)$' )\n\n\n\n\n\n\n\n\n\n\n\n3.6.3.6 Scatter Plots with plt.scatter\nA second, more powerful method of creating scatter plots is the plt.scatter function, which can be used very similarly to the plt.plot function:\n\n\n\n\n\n\nNote\n\n\n\nThe primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data.\n\n\nLet’s show this by creating a random scatter plot with points of many colors and sizes. In order to better see the overlapping results, we’ll also use the alpha keyword to adjust the transparency level:\n\nrng = np.random.RandomState(42)\nx = rng.randn(100)\ny = rng.randn(100)\ncolors = rng.rand(100)\nsizes = 1000 * rng.rand(100)\n\nplt.scatter(x, y, c=colors, s=sizes, alpha=0.3,\n            cmap='viridis',label=\" Random Y values\")\nplt.xlabel('Random x values')\nplt.ylabel('Random y values')\nplt.title('Bubble plot' )\nplt.colorbar();  # show color scale\n\n\n\n\n\n\n\n\n\n\n\n3.6.4 Histograms, Binnings, and Density\nA simple histogram can be a great first step in understanding a dataset.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8')\ndata = np.random.randn(1000)\nplt.hist(data)\n\n(array([ 17.,  59., 157., 225., 209., 170., 104.,  44.,  13.,   2.]),\n array([-2.63815139, -2.02458463, -1.41101787, -0.79745111, -0.18388435,\n         0.42968241,  1.04324918,  1.65681594,  2.2703827 ,  2.88394946,\n         3.49751622]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n3.6.4.1 Customizing Histograms\nThe hist() function has many options to tune both the calculation and the display; here’s an example of a more customized histogram:\n\nplt.hist(data, bins=30, density=True, alpha=0.5,\n         histtype='stepfilled', color='steelblue',\n         edgecolor='none');\n\n\n\n\n\n\n\n\nThe plt.hist docstring has more information on other customization options available. This combination of histtype=‘stepfilled’ along with some transparency alpha to be very useful when comparing histograms of several distributions:\n\nw1 = np.random.normal(0, 0.8, 1000)\nw2 = np.random.normal(-2, 1, 1000)\nw3 = np.random.normal(3, 1.2, 1000)\n\nkwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n\nplt.hist(w1, **kwargs,label='w1')\nplt.hist(w2, **kwargs,label='w2')\nplt.hist(w3, **kwargs,label='w3')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n3.6.5 Working with datafiles\nConsider the pokemon dataset for this job. The main featurs of this dataset are:\nDefense: This column represents the base damage resistance against normal attacks. Higher values indicate that the Pokémon can withstand more physical damage.\nSp. Atk (Special Attack): This column shows the base modifier for special attacks. Pokémon with higher Special Attack values can deal more damage with special moves.\nSp. Def (Special Defense): This column indicates the base damage resistance against special attacks. Higher values mean the Pokémon can better resist damage from special moves.\nSpeed: This column determines which Pokémon attacks first in each round. Pokémon with higher Speed values will generally attack before those with lower values.\nStage: This column represents the evolutionary stage of the Pokémon. It typically ranges from 1 to 3, with 1 being the base form and 3 being the final evolved form. Some Pokémon may have additional stages, such as Mega Evolutions or Gigantamax forms.\nLegendary: This is a boolean column that identifies whether the Pokémon is legendary. It is marked as True for legendary Pokémon and False for non-legendary ones.\nThese columns provide valuable insights into the strengths and characteristics of each Pokémon, helping players strategize and build their teams effectively. 🌟\nNow, let’s read our data into a Pandas dataframe. We will relax the limit on display columns and rows using the set_option() method in Pandas:\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/sijuswamy/PyWorks/main/Pokemon.csv\",encoding = 'utf_8')\ndf.head()\n\n\n\n\n\n\n\n\nName\nType 1\nType 2\nTotal\nHP\nAttack\nDefense\nSp.Atk\nSp.Def\nSpeed\nStage\nLegendary\n\n\n\n\n0\nBulbasaur\nGrass\nPoison\n318\n45\n49\n49\n65\n65\n45\n1\nFalse\n\n\n1\nIvysaur\nGrass\nPoison\n405\n60\n62\n63\n80\n80\n60\n2\nFalse\n\n\n2\nVenusaur\nGrass\nPoison\n525\n80\n82\n83\n100\n100\n80\n3\nFalse\n\n\n3\nCharmander\nFire\nPoison\n309\n39\n52\n43\n60\n50\n65\n1\nFalse\n\n\n4\nCharmeleon\nFire\nNaN\n405\n58\n64\n58\n80\n65\n80\n2\nFalse\n\n\n\n\n\n\n\nSince the Legendary feature contains the string True and False. But they are part of the logical data type in python. So let’s replace these values with TRUE and FALSE strings as follows.\n\nbooleanDictionary = {True: 'TRUE', False: 'FALSE'}\ndf = df.replace(booleanDictionary)\ndf.head()\n\n\n\n\n\n\n\n\nName\nType 1\nType 2\nTotal\nHP\nAttack\nDefense\nSp.Atk\nSp.Def\nSpeed\nStage\nLegendary\n\n\n\n\n0\nBulbasaur\nGrass\nPoison\n318\n45\n49\n49\n65\n65\n45\n1\nFALSE\n\n\n1\nIvysaur\nGrass\nPoison\n405\n60\n62\n63\n80\n80\n60\n2\nFALSE\n\n\n2\nVenusaur\nGrass\nPoison\n525\n80\n82\n83\n100\n100\n80\n3\nFALSE\n\n\n3\nCharmander\nFire\nPoison\n309\n39\n52\n43\n60\n50\n65\n1\nFALSE\n\n\n4\nCharmeleon\nFire\nNaN\n405\n58\n64\n58\n80\n65\n80\n2\nFALSE\n\n\n\n\n\n\n\nCreating a histogram\nWe can generate a histogram for any of the numerical columns by calling the hist() method on the plt object and passing in the selected column in the data frame. Let’s do this for the speed column, which corresponds to speed of the player.\n\nplt.hist(df['Speed'])\nplt.xlabel('Speed in minutes')\nplt.ylabel('Frequency')\nplt.title('Histogram of player speed')\nplt.show()\n\n\n\n\n\n\n\n\nScatterplot of Attack vs HP\nTo generate a scatter plot in Matplotlib, we simply use the scatter() method on the plt object. Let’s also label the axes and give our plot a title:\n\nplt.scatter(df['Attack'], df['HP'])\nplt.title('Attack vs. HP')\nplt.show()\n\n\n\n\n\n\n\n\nBarchart\nBar charts are another useful visualization tool for analyzing categories in data. To visualize categorical columns, we first should count the values. We can use the counter method from the collections modules to generate a dictionary of count values for each category in a categorical column. Let’s do this for the Legendary column.\n\nfrom collections import Counter \n\nprint(Counter(df[('Legendary')]))\n\nCounter({'FALSE': 147, 'TRUE': 4})\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can filter this dictionary using the most_common method. Let’s look at the 10 most common nationality values (you can also use the least_common method to analyze infrequent nationality values)\n\n\n\nLegendary_dict = dict(Counter(df[('Legendary')]).most_common(2))\nplt.bar(Legendary_dict.keys(), Legendary_dict.values())\nplt.xlabel('Legendary')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Ten Most Common Legendary')\n#plt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nGenerating Pie Charts With Matplotlib\nPie charts are a useful way to visualize proportions in your data. So first wee need to create the dictionary of propotion then feed it to the pie chart.\n\nprop = dict(Counter(df['Legendary']))\n\nfor key, values in prop.items():\n\n    prop[key] = (values)/len(df)*100\n\nprint(prop)\n\n{'FALSE': 97.35099337748345, 'TRUE': 2.6490066225165565}\n\n\n\nfig1, ax1 = plt.subplots()\n\nax1.pie(prop.values(), labels=prop.keys(), autopct='%1.1f%%',\n\n        shadow=True, startangle=0)\n\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n\n\n\n\n\n\n\n\nBox plots Box plots are helpful in visualizing the statistical summaries. The following code demonstrate the way of creating the box plot.\n\n plt.title(\"Five point summary of Speed\")\nplt.boxplot(df['Speed'],patch_artist=True, notch=True,labels=['Speed'])\nplt.show()\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_2100\\1029903557.py:2: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot(df['Speed'],patch_artist=True, notch=True,labels=['Speed'])\n\n\n\n\n\n\n\n\n\n\n\n3.6.6 Data Visualization With Seaborn\nSeaborn is a library built on top of Matplotlib that enables more sophisticated visualization and aesthetic plot formatting. Once you’ve mastered Matplotlib, you may want to move up to Seaborn for more complex visualizations.\nFor example, simply using the Seaborn set() method can dramatically improve the appearance of your Matplotlib plots. Let’s take a look.\nFirst, import Seaborn as sns and reformat all of the figures we generated. At the top of your script, write the following code and rerun:\n\nimport seaborn as sns\nsns.set()\nplt.show()\n\n\n\n3.6.7 Histograms With Seaborn\nTo regenerate our histogram of the overall column, we use the histplot method on the Seaborn object:\n\nsns.histplot(df['Speed'],kde=True)\nplt.xlabel('Speed in minutes')\nplt.ylabel('Frequency')\nplt.title('Histogram of player speed')\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s modify the histogtram by including the feature Legendary.\n\nsns.histplot(x='Speed',hue='Legendary',kde=True,data=df)\nplt.xlabel('Speed in minutes')\nplt.ylabel('Frequency')\nplt.title('Histogram of player speed')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.6.8 Scatter Plots With Seaborn\nSeaborn also makes generating scatter plots straightforward. Let’s recreate the scatter plot from earlier:\n\nsns.scatterplot(x='Attack', y='HP',hue='Legendary',data=df)\nplt.title('Attack vs. HP')\nplt.show()\n\n\n\n\n\n\n\n\nIn the similar way, let’s compare the Attack and Defense stats for our Pokémon over Stages\n\n# Plot using Seaborn\nsns.lmplot(x='Attack', y='Defense', data=df,\n           fit_reg=False, legend=False,\n           hue='Stage')\n \n# Tweak using Matplotlib\nplt.ylim(0, 200)\nplt.xlim(0, 160)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s witness the power of seaboran in creating boxplots of all numerical features in single line of code!\n\n# Boxplot\nplt.figure(figsize=(8,6)) # Set plot dimensions\nsns.boxplot(data=df)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_3.html#module-review",
    "href": "module_3.html#module-review",
    "title": "3  Python Libraries for Computational Linear Algebra",
    "section": "3.7 Module review",
    "text": "3.7 Module review\n\n3.7.1 Advanced Linear Algebra Questions\n\nWrite the NumPy methods for (a) matrix multiplication and (b) element-wise multiplication with suitable examples.\n\nHint: Use numpy.matmul(A, B) or A @ B for matrix multiplication and numpy.multiply(A, B) for element-wise multiplication.\n\nWhat is the determinant of a matrix? Write the Python code to compute the determinant of \\(A = \\begin{bmatrix} 3 & 2 \\\\ 5 & 7 \\end{bmatrix}\\) using NumPy.\n\nHint: Use numpy.linalg.det(A) to find the determinant.\n\nExplain the difference between numpy.dot() and numpy.matmul() with suitable examples.\n\nHint: Discuss their usage for both vectors and matrices.\n\nWhat is the condition number of a matrix? Write Python code to compute the condition number of \\(A = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\).\n\nHint: Use numpy.linalg.cond(A) to calculate the condition number.\n\nDescribe the process of normalizing a matrix. Write Python code to normalize the rows of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\).\n\nHint: Use numpy.linalg.norm and divide each row by its norm.\n\nWhat is a transpose of a matrix? Write Python code to compute the transpose of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\).\n\nHint: Use numpy.transpose(A) or A.T.\n\nWrite Python code to compute the inverse of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) using NumPy. Also, verify the result by multiplying \\(A\\) with its inverse.\n\nHint: Use numpy.linalg.inv(A) for the inverse and numpy.matmul() to check the identity property.\n\nExplain the role of the trace of a matrix in linear algebra. Write Python code to compute the trace of \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\).\n\nHint: Use numpy.trace(A) to compute the trace.\n\nWhat is matrix slicing? Demonstrate how to extract the first two rows and last two columns from \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\).\n\n\n\nHint: Use NumPy slicing syntax: A[:2, -2:].\n\n\nExplain the difference between solving \\(Ax = b\\) using numpy.linalg.solve() and directly computing \\(A^{-1}b\\). Which method is computationally more efficient?\n- Hint: Discuss how numpy.linalg.solve(A, b) avoids explicitly calculating the inverse of \\(A\\), making it more efficient.\nWrite the NumPy methods for (a) outer product and (b) inner product with suitable examples.\n- Hint: Use numpy.outer(a, b) for the outer product and numpy.inner(a, b) for the inner product. Example: \\(a = [1, 2]\\) and \\(b = [3, 4]\\).\nWhat is the pseudo-inverse of a matrix? Write the Python code chunk to solve a system of linear equations \\(Ax = b\\), where \\(A = \\begin{bmatrix} 2 & 3 \\\\ -1 & 3 \\\\ 2 & 4 \\end{bmatrix}\\) and \\(b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\), using the pseudo-inverse. Justify the reason for using the pseudo-inverse instead of \\(A^{-1}\\).\n- Hint: The pseudo-inverse is computed using the Moore-Penrose method (numpy.linalg.pinv). It is useful for non-square or rank-deficient matrices.\nExplain various array concatenation routines from the NumPy library with suitable examples.\n- Hint: Use numpy.concatenate, numpy.vstack, numpy.hstack, and numpy.dstack to merge arrays along different axes.\nDescribe the major differences between NumPy and SymPy in advanced linear algebra operations.\n- Hint: NumPy focuses on numerical computations with high performance, while SymPy provides symbolic computation and exact results.\nDemonstrate the use of numpy.linalg.norm to compute different norms of a matrix \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\).\n- Hint: Use ord='fro' for the Frobenius norm, ord=1 for the 1-norm, and ord=np.inf for the infinity norm.\nWhat is the Kronecker product of two matrices? Find the Kronecker product of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 0 & 5 \\\\ 6 & 7 \\end{bmatrix}\\) using NumPy.\n- Hint: Use numpy.kron(A, B) to compute the Kronecker product.\nExplain how sparse matrices are represented in SciPy. Provide examples of common formats such as CSR, COO, and CSC.\n- Hint: Use scipy.sparse.csr_matrix, scipy.sparse.coo_matrix, and scipy.sparse.csc_matrix for different formats.\nWrite Python code to compute the determinant and rank of a matrix \\(A = \\begin{bmatrix} 5 & 2 \\\\ 1 & 4 \\end{bmatrix}\\) using NumPy.\n- Hint: Use numpy.linalg.det(A) for determinant and numpy.linalg.matrix_rank(A) for rank.\nWhat are broadcasting rules in NumPy? Explain with examples how broadcasting simplifies matrix operations.\n\n- **Hint**: Broadcasting allows element-wise operations on arrays of different shapes. Example: Adding a scalar to a matrix.\n\nIllustrate the computation of eigenvalues and eigenvectors of \\(A = \\begin{bmatrix} 6 & 2 \\\\ 2 & 3 \\end{bmatrix}\\) using NumPy.\n\nHint: Use numpy.linalg.eig(A) to compute eigenvalues and eigenvectors.\n\n\n\n\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.” Nature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Libraries for Computational Linear Algebra</span>"
    ]
  },
  {
    "objectID": "module_4.html",
    "href": "module_4.html",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "",
    "text": "4.1 Introduction\nMatrix decomposition plays a pivotal role in computational linear algebra, forming the backbone of numerous modern applications in fields such as data science, machine learning, computer vision, and signal processing. The core idea behind matrix decomposition is to break down complex matrices into simpler, structured components that allow for more efficient computation. Techniques such as LU, QR, Singular Value Decomposition (SVD), and Eigenvalue decompositions not only reduce computational complexity but also provide deep insights into the geometry and structure of data. These methods are essential in solving systems of linear equations, performing dimensionality reduction, and extracting meaningful features from data. For instance, LU decomposition is widely used to solve large linear systems, while QR decomposition plays a key role in solving least squares problems—a fundamental task in machine learning models.\nIn emerging fields like big data analytics and artificial intelligence, matrix decomposition techniques are indispensable for processing and analyzing high-dimensional datasets. SVD and Principal Component Analysis (PCA), for example, are extensively used for data compression and noise reduction, making machine learning algorithms more efficient by reducing the number of variables while retaining key information. Additionally, sparse matrix decompositions allow for the handling of enormous datasets where most entries are zero, optimizing memory usage and computation time. As data science and machine learning continue to evolve, mastering these matrix decomposition techniques provides not only a computational advantage but also deeper insights into the structure and relationships within data, enhancing the performance of algorithms in real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#lu-decomposition",
    "href": "module_4.html#lu-decomposition",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "4.2 LU Decomposition",
    "text": "4.2 LU Decomposition\nLU decomposition is a powerful tool in linear algebra that elegantly unravels the complexity of solving systems of linear equations. At its core, LU decomposition expresses a matrix \\(A\\) as the product of two distinct matrices: \\(L\\) (a lower triangular matrix with ones on the diagonal) and \\(U\\) (an upper triangular matrix). This decomposition transforms the problem of solving \\(Ax = b\\) into a two-step process: first, solving \\(Ly = b\\) for \\(y\\), followed by \\(Ux = y\\) for \\(x\\). This systematic approach not only simplifies computations but also provides insightful perspectives on the relationships between the equations involved.\nThe magic of LU decomposition lies in its utilization of elementary transformations—operations that allow us to manipulate the rows of a matrix to achieve a row-reduced echelon form. These transformations include row swaps, scaling, and adding multiples of one row to another. By applying these operations, we can gradually transform the original matrix \\(A\\) into the upper triangular matrix \\(U\\), while simultaneously capturing the essence of these transformations in the lower triangular matrix \\(L\\). This interplay of \\(L\\) and \\(U\\) not only enhances computational efficiency but also unveils the deeper structural relationships within the matrix.\nMoreover, the beauty of matrix multiplication shines through in LU decomposition. The product \\(A = LU\\) showcases how two simpler matrices can combine to reconstruct a more complex one, demonstrating the power of linear combinations in solving equations. As we delve into LU decomposition, we embark on a journey that highlights the synergy between algebraic manipulation and geometric interpretation, empowering us to tackle intricate problems with grace and precision. Given a square matrix \\(A\\), the LU decomposition expresses \\(A\\) as a product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\): \\[A = LU\\]\nWhere: - \\(L\\) is a lower triangular matrix with 1’s on the diagonal and other elements like \\(l_{21}, l_{31}, l_{32}, \\dots\\), - \\(U\\) is an upper triangular matrix with elements \\(u_{11}, u_{12}, u_{13}, u_{22}, u_{23}, u_{33}, \\dots\\).\n\n4.2.1 Step-by-Step Procedure\nLet’s assume \\(A\\) is a \\(3 \\times 3\\) matrix for simplicity: \\[A = \\begin{pmatrix}a_{11} & a_{12} & a_{13} \\\\a_{21} & a_{22} & a_{23} \\\\a_{31} & a_{32} & a_{33}\\end{pmatrix}\\]\nWe need to find matrices \\(L\\) and \\(U\\), where:\n\n\\(L = \\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\\)\n\\(U = \\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}\\)\n\nThe product of \\(L\\) and \\(U\\) gives: \\[LU = \\begin{pmatrix} 1 & 0 & 0 \\\\l_{21} & 1 & 0 \\\\l_{31} & l_{32} & 1\\end{pmatrix}\\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\0 & u_{22} & u_{23} \\\\0 & 0 & u_{33}\\end{pmatrix}=\\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\l_{21}u_{11} & l_{21}u_{12} + u_{22} & l_{21}u_{13} + u_{23} \\\\l_{31}u_{11} & l_{31}u_{12} + l_{32}u_{22} & l_{31}u_{13} + l_{32}u_{23} + u_{33}\\end{pmatrix}\\]\nBy equating this with \\(A\\), we can set up a system of equations to solve for \\(l_{ij}\\) and \\(u_{ij}\\).\n\nStep 1: Solve for \\(u_{11}, u_{12}, u_{13}\\)\n\nFrom the first row of \\(A = LU\\), we have: \\[u_{11} = a_{11}\\] \\[u_{12} = a_{12}\\] \\[u_{13} = a_{13}\\]\n\nStep 2: Solve for \\(l_{21}\\) and \\(u_{22}, u_{23}\\)\n\nFrom the second row, we get: \\[l_{21}u_{11} = a_{21} \\quad \\Rightarrow \\quad l_{21} = \\frac{a_{21}}{u_{11}}\\] \\[l_{21}u_{12} + u_{22} = a_{22} \\quad \\Rightarrow \\quad u_{22} = a_{22} - l_{21}u_{12}\\] \\[l_{21}u_{13} + u_{23} = a_{23} \\quad \\Rightarrow \\quad u_{23} = a_{23} - l_{21}u_{13}\\]\n\nStep 3: Solve for \\(l_{31}, l_{32}\\) and \\(u_{33}\\)\n\nFrom the third row, we get: \\[l_{31}u_{11} = a_{31} \\quad \\Rightarrow \\quad l_{31} = \\frac{a_{31}}{u_{11}}\\] \\[l_{31}u_{12} + l_{32}u_{22} = a_{32} \\quad \\Rightarrow \\quad l_{32} = \\frac{a_{32} - l_{31}u_{12}}{u_{22}}\\] \\[l_{31}u_{13} + l_{32}u_{23} + u_{33} = a_{33} \\quad \\Rightarrow \\quad u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}\\]\nFinal Result\nThus, the LU decomposition is given by the matrices: - \\(L = \\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\\) - \\(U = \\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}\\)\nWhere: - \\(u_{11} = a_{11}, u_{12} = a_{12}, u_{13} = a_{13}\\) - \\(l_{21} = \\frac{a_{21}}{u_{11}}, u_{22} = a_{22} - l_{21}u_{12}, u_{23} = a_{23} - l_{21}u_{13}\\) - \\(l_{31} = \\frac{a_{31}}{u_{11}}, l_{32} = \\frac{a_{32} - l_{31}u_{12}}{u_{22}}, u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}\\)\n\n\n4.2.2 Example\nLet’s decompose the following matrix: \\[A = \\begin{pmatrix} 4 & 3 & 2 \\\\6 & 3 & 1 \\\\2 & 1 & 3\\end{pmatrix}\\]\nFollowing the steps outlined above:\n\n\\(u_{11} = 4, u_{12} = 3, u_{13} = 2\\)\n\\(l_{21} = \\frac{6}{4} = 1.5\\), so:\n\n\\(u_{22} = 3 - 1.5 \\times 3 = -1.5\\)\n\\(u_{23} = 1 - 1.5 \\times 2 = -2\\)\n\n\\(l_{31} = \\frac{2}{4} = 0.5\\), so:\n\n\\(l_{32} = \\frac{1 - 0.5 \\times 3}{-1.5} = 0.67\\)\n\\(u_{33} = 3 - 0.5 \\times 2 - 0.67 \\times (-2) = 2.67\\)\n\n\nThus, the decomposition is: - \\(L = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1.5 & 1 & 0 \\\\\n0.5 & 0.67 & 1\n\\end{pmatrix}\\) - \\(U = \\begin{pmatrix}\n4 & 3 & 2 \\\\\n0 & -1.5 & -2 \\\\\n0 & 0 & 2.67\n\\end{pmatrix}\\)\n\n\n4.2.3 Python Implementation\n\nimport numpy as np\nfrom scipy.linalg import lu\n\n# Define matrix A\nA = np.array([[4, 3, 2],\n              [6, 3, 1],\n              [2, 1, 3]])\n\n# Perform LU decomposition\nP, L, U = lu(A)\n\n# Print the results\nprint(\"L = \\n\", L)\nprint(\"U = \\n\", U)\n\nL = \n [[1.         0.         0.        ]\n [0.66666667 1.         0.        ]\n [0.33333333 0.         1.        ]]\nU = \n [[6.         3.         1.        ]\n [0.         1.         1.33333333]\n [0.         0.         2.66666667]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince there are many row transformations that reduce a given matrix into row echelon form. So the LU decomposition is not unique.\n\n\n\n\n4.2.4 LU Decomposition Practice Problems with Solutions\nProblem 1: Decompose the matrix \\[ A = \\begin{pmatrix} 4 & 3 \\\\ 6 & 3 \\end{pmatrix} \\] into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\).\nSolution:\nLet \\[ L = \\begin{pmatrix} 1 & 0 \\\\ l_{21} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{pmatrix}. \\]\nWe have:\n\nFrom the first row: \\(u_{11} = 4\\) and \\(u_{12} = 3\\).\nFrom the second row: \\(6 = l_{21} \\cdot 4\\) gives \\(l_{21} = \\frac{6}{4} = 1.5\\).\nFinally, \\(3 = 1.5 \\cdot 3 + u_{22}\\) gives \\(u_{22} = 3 - 4.5 = -1.5\\).\n\nThus, we have: \\[ L = \\begin{pmatrix} 1 & 0 \\\\ 1.5 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 4 & 3 \\\\ 0 & -1.5 \\end{pmatrix}. \\]\nProblem 2: Given the matrix \\[ A = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 5 & 8 \\\\ 4 & 5 & 6 \\end{pmatrix}, \\] perform LU decomposition to find matrices \\(L\\) and \\(U\\).\nSolution:\nLet \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. \\]\nWe have:\n\nFrom Row 1: \\(u_{11} = 1, u_{12} = 2, u_{13} = 3\\).\nFrom Row 2: \\(2 = l_{21} \\cdot 1\\) gives \\(l_{21} = 2\\).\n\nFor Row 2: \\(5 = l_{21} \\cdot 2 + u_{22}\\) gives \\(5 = 4 + u_{22} \\Rightarrow u_{22} = 1\\).\n\\(8 = l_{21} \\cdot 3 + u_{23} \\Rightarrow 8 = 6 + u_{23} \\Rightarrow u_{23} = 2\\).\n\nFrom Row 3: \\(4 = l_{31} \\cdot 1 \\Rightarrow l_{31} = 4\\).\n\n\\(5 = l_{31} \\cdot 2 + l_{32} \\cdot 1 \\Rightarrow 5 = 8 + l_{32} \\Rightarrow l_{32} = -3\\).\nFinally, \\(6 = l_{31} \\cdot 3 + l_{32} \\cdot 2 + u_{33} \\Rightarrow 6 = 12 - 6 + u_{33} \\Rightarrow u_{33} = 0\\).\n\n\nThus, \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 4 & -3 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix}. \\]\nProblem 3: Perform LU decomposition of the matrix \\[ A = \\begin{pmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{pmatrix}, \\] and verify the decomposition by checking \\(A = LU\\).\nSolution:\nLet \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. \\]\nWe have:\n\nFrom Row 1: \\(u_{11} = 2, u_{12} = 1, u_{13} = 1\\).\nFrom Row 2: \\(4 = l_{21} \\cdot 2 \\Rightarrow l_{21} = 2\\).\n\n\\(-6 = 2 \\cdot 1 + u_{22} \\Rightarrow u_{22} = -8\\).\n\\(0 = 2 \\cdot 1 + u_{23} \\Rightarrow u_{23} = -2\\).\n\nFrom Row 3: \\(-2 = l_{31} \\cdot 2 \\Rightarrow l_{31} = -1\\).\n\n\\(7 = -1 \\cdot 1 + l_{32} \\cdot -8 \\Rightarrow 7 = -1 - 8l_{32} \\Rightarrow l_{32} = -1\\).\nFinally, \\(2 = -1 \\cdot 1 + -1 \\cdot -2 + u_{33} \\Rightarrow 2 = 1 + u_{33} \\Rightarrow u_{33} = 1\\).\n\n\nThus, \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{pmatrix}. \\]\nProblem 4: For the matrix \\[ A = \\begin{pmatrix} 3 & 1 & 6 \\\\ 2 & 1 & 1 \\\\ 1 & 2 & 2 \\end{pmatrix}, \\] find the LU decomposition and use it to solve the system \\(Ax = b\\) where \\(b = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix}\\).\nSolution:\nLet \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. \\]\nWe have:\n\nFrom Row 1: \\(u_{11} = 3, u_{12} = 1, u_{13} = 6\\).\nFrom Row 2: \\(2 = l_{21} \\cdot 3 \\Rightarrow l_{21} = \\frac{2}{3}\\).\n\n\\(1 = \\frac{2}{3} \\cdot 1 + u_{22} \\Rightarrow 1 = \\frac{2}{3} + u_{22} \\Rightarrow u_{22} = \\frac{1}{3}\\).\n\\(1 = \\frac{2}{3} \\cdot 6 + u_{23} \\Rightarrow 1 = 4 + u_{23} \\Rightarrow u_{23} = -3\\).\n\nFrom Row 3: \\(1 = l_{31} \\cdot 3 \\Rightarrow l_{31} = \\frac{1}{3}\\).\n\n\\(2 = \\frac{1}{3} \\cdot 1 + l_{32} \\cdot \\frac{1}{3} \\Rightarrow 2 = \\frac{1}{3} + \\frac{1}{3} l_{32} \\Rightarrow l_{32} = 6\\).\nFinally, \\(2 = \\frac{1}{3} \\cdot 6 + 6 \\cdot -3 + u_{33} \\Rightarrow 2 = 2 - 18 + u_{33} \\Rightarrow u_{33} = 18\\).\n\n\nThus, \\[ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{2}{3} & 1 & 0 \\\\ \\frac{1}{3} & 6 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 3 & 1 & 6 \\\\ 0 & \\frac{1}{3} & -3 \\\\ 0 & 0 & 18 \\end{pmatrix}. \\]\nNow, to solve \\(Ax = b\\), we first solve \\(Ly = b\\): \\[ \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{2}{3} & 1 & 0 \\\\ \\frac{1}{3} & 6 & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix} \\]\nSolving this gives: 1. \\(y_1 = 9\\) 2. \\(\\frac{2}{3} \\cdot 9 + y_2 = 5 \\Rightarrow 6 + y_2 = 5 \\Rightarrow y_2 = -1\\) 3. \\(\\frac{1}{3} \\cdot 9 + 6 \\cdot -1 + y_3 = 4 \\Rightarrow 3 - 6 + y_3 = 4 \\Rightarrow y_3 = 7\\)\nNext, solve \\(Ux = y\\): \\[ \\begin{pmatrix} 3 & 1 & 6 \\\\ 0 & \\frac{1}{3} & -3 \\\\ 0 & 0 & 18 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ -1 \\\\ 7 \\end{pmatrix} \\]\n\nFrom Row 3: \\(18x_3 = 7 \\Rightarrow x_3 = \\frac{7}{18}\\)\nFrom Row 2: \\(\\frac{1}{3}x_2 - 3x_3 = -1 \\Rightarrow \\frac{1}{3}x_2 - \\frac{21}{18} = -1 \\Rightarrow \\frac{1}{3}x_2 = -\\frac{18}{18} + \\frac{21}{18} = \\frac{3}{18} \\Rightarrow x_2 = \\frac{1}{3}\\)\nFrom Row 1: \\(3x_1 + x_2 + 6x_3 = 9 \\Rightarrow 3x_1 + \\frac{1}{3} + \\frac{42}{18} = 9 \\Rightarrow 3x_1 + \\frac{1}{3} + \\frac{7}{3} = 9 \\Rightarrow 3x_1 = 9 - \\frac{8}{3} = \\frac{27 - 8}{3} = \\frac{19}{3} \\Rightarrow x_1 = \\frac{19}{9}\\)\n\nThus, the solution to \\(Ax = b\\) is \\[ x = \\begin{pmatrix} \\frac{19}{9} \\\\ \\frac{1}{3} \\\\ \\frac{7}{18} \\end{pmatrix}. \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#lu-decomposition-practice-problems",
    "href": "module_4.html#lu-decomposition-practice-problems",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "4.3 LU Decomposition Practice Problems",
    "text": "4.3 LU Decomposition Practice Problems\nProblem 1: Decompose the matrix \\[ A = \\begin{pmatrix} 4 & 3 \\\\ 6 & 3 \\end{pmatrix} \\] into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\).\nProblem 2: Given the matrix \\[ A = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 5 & 8 \\\\ 4 & 5 & 6 \\end{pmatrix}, \\] perform LU decomposition to find matrices \\(L\\) and \\(U\\).\nProblem 3: Perform LU decomposition of the matrix \\[ A = \\begin{pmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{pmatrix}, \\] and verify the decomposition by checking \\(A = LU\\).\nProblem 4: For the matrix \\[ A = \\begin{pmatrix} 3 & 1 & 6 \\\\ 2 & 1 & 1 \\\\ 1 & 2 & 2 \\end{pmatrix}, \\] find the LU decomposition and use it to solve the system \\(Ax = b\\) where \\(b = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix}\\).\nProblem 5: Decompose the matrix \\[ A = \\begin{pmatrix} 1 & 3 & 1 \\\\ 2 & 6 & 1 \\\\ 1 & 1 & 4 \\end{pmatrix} \\] into \\(L\\) and \\(U\\), and solve the system \\(Ax = \\begin{pmatrix} 5 \\\\ 9 \\\\ 6 \\end{pmatrix}\\).\nProblem 6: Given the matrix \\[ A = \\begin{pmatrix} 7 & 3 \\\\ 2 & 5 \\end{pmatrix}, \\] perform LU decomposition and use the result to solve \\(Ax = b\\) for \\(b = \\begin{pmatrix} 10 \\\\ 7 \\end{pmatrix}\\).\nProblem 7: Find the LU decomposition of the matrix \\[ A = \\begin{pmatrix} 2 & -1 & 1 \\\\ -2 & 2 & -1 \\\\ 4 & -1 & 3 \\end{pmatrix}, \\] and use it to solve \\(Ax = b\\) where \\(b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 7 \\end{pmatrix}\\).\nProblem 8: Perform LU decomposition of the matrix \\[ A = \\begin{pmatrix} 5 & 2 & 1 \\\\ 10 & 4 & 3 \\\\ 15 & 8 & 6 \\end{pmatrix}. \\]\nProblem 9: Use LU decomposition to find the solution to the system \\(Ax = b\\) where \\[ A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 2 & 3 & 5 \\\\ 4 & 6 & 8 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 15 \\\\ 30 \\end{pmatrix}. \\]\nProblem 10: Decompose the matrix \\[ A = \\begin{pmatrix} 6 & -2 & 2 \\\\ 12 & -8 & 6 \\\\ -6 & 3 & -3 \\end{pmatrix} \\] into \\(L\\) and \\(U\\), and verify that \\(A = LU\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#matrix-approach-to-create-lu-decomposition",
    "href": "module_4.html#matrix-approach-to-create-lu-decomposition",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "4.4 Matrix Approach to Create LU Decomposition",
    "text": "4.4 Matrix Approach to Create LU Decomposition\nLU decomposition can be performed using elementary matrix operations. In this method, we iteratively apply elementary matrices to reduce the given matrix \\(A\\) into an upper triangular matrix \\(U\\), while keeping track of the transformations to form the lower triangular matrix \\(L\\).\nThe LU decomposition can be written as: \\[A = LU\\]\nwhere: - \\(L\\) is the product of the inverses of the elementary matrices. - \\(U\\) is the upper triangular matrix obtained after applying the row operations.\nExample: LU Decomposition of a 3x3 Matrix\nGiven the matrix: \\[\nA = \\begin{pmatrix}\n2 & 1 & 1 \\\\\n4 & -6 & 0 \\\\\n-2 & 7 & 2\n\\end{pmatrix}\n\\]\nWe will decompose \\(A\\) into \\(L\\) and \\(U\\) using elementary row operations.\n\nStep 1: Applying Elementary Matrices\n\nWe want to perform row operations to reduce \\(A\\) into upper triangular form.\n\nStep 1.1: Eliminate the \\(a_{21}\\) entry (below the pivot in column 1)\n\nTo eliminate the \\(4\\) in position \\(a_{21}\\), perform the operation: \\[R_2 \\rightarrow R_2 - 2R_1\\]\nThis corresponds to multiplying \\(A\\) by the elementary matrix: \\[E_1 = \\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1\\end{pmatrix}\\]\nAfter this row operation, the matrix becomes: \\[\nE_1 A = \\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n-2 & 7 & 2\n\\end{pmatrix}\n\\]\n\nStep 1.2: Eliminate the \\(a_{31}\\) entry\n\nTo eliminate the \\(-2\\) in position \\(a_{31}\\), perform the operation: \\[R_3 \\rightarrow R_3 + R_1\\]\nThis corresponds to multiplying the matrix by another elementary matrix: \\[\nE_2 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n\\]\nNow, the matrix becomes: \\[\nE_2 E_1 A = \\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 8 & 3\n\\end{pmatrix}\n\\]\n\nStep 1.3: Eliminate the \\(a_{32}\\) entry\n\nFinally, to eliminate the \\(8\\) in position \\(a_{32}\\), perform the operation: \\[\nR_3 \\rightarrow R_3 + R_2\n\\]\nThis corresponds to multiplying the matrix by the third elementary matrix:\n\\[\nE_3 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\]\nAfter applying this operation, the matrix becomes: \\[\nE_3 E_2 E_1 A = \\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\]\nThis is the upper triangular matrix \\(U\\).\n\nStep 2: Construct the Lower Triangular Matrix \\(L\\)\n\nThe lower triangular matrix \\(L\\) is formed by taking the inverses of the elementary matrices \\(E_1, E_2, E_3\\). Each inverse corresponds to the inverse of the row operations we applied.\n\n\\(E_1^{-1}\\) corresponds to adding back \\(2R_1\\) to \\(R_2\\), so: \\[\nE_1^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\]\n\\(E_2^{-1}\\) corresponds to subtracting \\(R_1\\) from \\(R_3\\), so: \\[\nE_2^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n-1 & 0 & 1\n\\end{pmatrix}\n\\]\n\\(E_3^{-1}\\) corresponds to subtracting \\(R_2\\) from \\(R_3\\), so: \\[\nE_3^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & -1 & 1\n\\end{pmatrix}\n\\]\n\nNow, the lower triangular matrix \\(L\\) is obtained by multiplying these inverses in reverse order: \\[\nL = E_3^{-1} E_2^{-1} E_1^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{pmatrix}\n\\]\nThus, the LU decomposition of \\(A\\) is: \\[\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{pmatrix},\n\\quad U = \\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\]\nVerification\nNow, we check if \\(A = LU\\).\nMultiply \\(L\\) and \\(U\\):\n\nimport numpy as np\n\nL = np.array([[1, 0, 0],\n              [2, 1, 0],\n              [-1, -1, 1]])\n\nU = np.array([[2, 1, 1],\n              [0, -8, -2],\n              [0, 0, 1]])\n\nA = L @ U\nA\n\narray([[ 2,  1,  1],\n       [ 4, -6,  0],\n       [-2,  7,  2]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#background",
    "href": "module_4.html#background",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "5.1 Background",
    "text": "5.1 Background\nImagine encountering a low-resolution image of a familiar scene. The human brain excels at recognizing familiar objects by relying on essential features, often extracting the most significant details while discarding the less important information. This cognitive process mirrors the power of eigenvalue decomposition, where eigenvectors represent the ``nectar’’ of a matrix, capturing its most important characteristics.\nAs an example, try to identify this image. If you can do it, then your brain know this place!\n\nBefore proceeding further just compare the size of its’ original clean image and the low-quality image shown in Figure\nOriginal image size: 985.69 KB\nReconstructed image size: 1.12 KB\nThe reconstructed image is just 0.2% of the original in size! This is the core principle of optimizing image storage of CCTV system. This resizing can be done and execute with optimal scaling with the help of Linear Algebra. This module mainly focuses on such engineering applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#introduction-1",
    "href": "module_4.html#introduction-1",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nSpectral decomposition, also known as eigenvalue decomposition, is a powerful tool in computational linear algebra that breaks down a matrix into its eigenvalues and eigenvectors. This technique allows matrices to be represented in terms of their fundamental components, making it easier to analyze and manipulate them. It is especially useful for symmetric matrices, which are common in various applications. Spectral decomposition facilitates solving systems of equations, optimizing functions, and performing transformations in a simplified, structured manner, as it allows operations to be performed on the eigenvalues, which often leads to more efficient computations.\nThe importance of spectral decomposition extends across a wide range of fields, including computer science, engineering, and data science. In machine learning, for instance, it forms the backbone of algorithms like Principal Component Analysis (PCA), which is used for dimensionality reduction. It also plays a vital role in numerical stability when dealing with large matrices and is central to many optimization problems, such as those found in machine learning and physics. Spectral decomposition not only provides a deeper understanding of the properties of matrices but also offers practical benefits in improving the efficiency and accuracy of numerical algorithms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#spectral-decomposition-detailed-concepts",
    "href": "module_4.html#spectral-decomposition-detailed-concepts",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "5.3 Spectral Decomposition: Detailed Concepts",
    "text": "5.3 Spectral Decomposition: Detailed Concepts\n\n5.3.1 Eigenvalues and Eigenvectors\nThe core idea behind spectral decomposition is that it expresses a matrix in terms of its eigenvalues and eigenvectors. For a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), an eigenvalue \\(\\lambda \\in \\mathbb{R}\\) and an eigenvector \\(v \\in \\mathbb{R}^{n}\\) satisfy the following equation:\n\\[\nA v = \\lambda v\n\\]\nThis implies that when the matrix \\(A\\) acts on the vector \\(v\\), it only scales the vector by \\(\\lambda\\), but does not change its direction. The eigenvector \\(v\\) represents the direction of this scaling, while the eigenvalue \\(\\lambda\\) represents the magnitude of the scaling.\n\n\n\n\n\n\nProperties of Eigen values\n\n\n\n\nIf \\(\\lambda\\) is an eigenvalue of \\(A\\), then it satisfies the characteristic polynomial:\n\\[\np(\\lambda) = \\text{det}(A - \\lambda I) = 0.\n\\]\nThe sum of the eigenvalues (counted with algebraic multiplicity) is equal to the trace of the matrix:\n\\[\n\\sum_{i=1}^{n} \\lambda_i = \\text{trace}(A).\n\\]\nThe product of the eigenvalues (counted with algebraic multiplicity) is equal to the determinant of the matrix:\n\\[\n\\prod_{i=1}^{n} \\lambda_i = \\text{det}(A).\n\\]\nIf\\(A\\) is symmetric, then:\n\nAll eigenvalues \\(\\lambda\\) are real.\nIf \\(\\lambda_i\\) and \\(\\lambda_j\\) are distinct eigenvalues, then their corresponding eigenvectors \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\) satisfy:\n\n\\[\n\\mathbf{v}_i^T \\mathbf{v}_j = 0.\n\\]\nIf \\(A\\) is a scalar multiple of \\(k\\), then:\n\\[\n\\lambda_i \\text{ of } kA = k \\cdot \\lambda_i \\text{ of } A.\n\\]\nIf \\(A\\) is invertible, then:\n\\[\n\\lambda_i \\text{ of } A^{-1} = \\frac{1}{\\lambda_i \\text{ of } A}.\n\\]\nIf \\(A\\) and \\(B\\) are similar, then:\n\\[\nB = P^{-1} A P \\implies \\lambda_i \\text{ of } B = \\lambda_i \\text{ of } A.\n\\]\nIf \\(\\lambda\\) is an eigenvalue, it has:\n\nAlgebraic Multiplicity: The number of times \\(\\lambda\\) appears as a root of \\(p(\\lambda)\\).\nGeometric Multiplicity: The dimension of the eigenspace \\(E_{\\lambda} = \\{\\mathbf{v} : A\\mathbf{v} = \\lambda \\mathbf{v}\\}\\).\n\nIf\\(A\\) is symmetric and all eigenvalues \\(\\lambda\\) are positive, then \\(A\\) is positive definite:\n\\[\n\\lambda_i &gt; 0 \\implies A \\text{ is positive definite.}\n\\]\nA square matrix \\(A\\) has an eigenvalue \\(\\lambda = 0\\) if and only if \\(A\\) is singular:\n\\[\n\\text{det}(A) = 0 \\iff \\lambda = 0.\n\\]\n\n\n\n\n\n\n\n\n\nEigen Vectors\n\n\n\nEigen vectors are the non-trivial solutions of \\(det(A-\\lambda I)=0\\) for distinct \\(\\lambda\\).\n\n\n\n\n\n\n\n\nProperties of Eigen vectors\n\n\n\n\nIf \\(\\mathbf{v}\\) is an eigenvector of a square matrix \\(A\\) corresponding to the eigenvalue \\(\\lambda\\), then:\n\\[\nA\\mathbf{v} = \\lambda \\mathbf{v}.\n\\]\nEigenvectors corresponding to distinct eigenvalues are linearly independent. If \\(\\lambda_1\\) and \\(\\lambda_2\\) are distinct eigenvalues of \\(A\\), with corresponding eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\), then:\n\\[\nc_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 = \\mathbf{0} \\implies c_1 = 0 \\text{ and } c_2 = 0.\n\\]\nIf \\(\\mathbf{v}\\) is an eigenvector corresponding to the eigenvalue \\(\\lambda\\), then any non-zero scalar multiple of \\(\\mathbf{v}\\) is also an eigenvector corresponding to \\(\\lambda\\):\n\\[\n\\text{If } \\mathbf{v} \\text{ is an eigenvector, then } c\\mathbf{v} \\text{ is an eigenvector for any non-zero scalar } c.\n\\]\nThe eigenspace \\(E_{\\lambda}\\) associated with an eigenvalue \\(\\lambda\\) is defined as:\n\\[\nE_{\\lambda} = \\{ \\mathbf{v} : A\\mathbf{v} = \\lambda \\mathbf{v} \\} = \\text{Null}(A - \\lambda I).\n\\]\nThe dimension of the eigenspace \\(E_{\\lambda}\\) is equal to the geometric multiplicity of the eigenvalue \\(\\lambda\\).\nIf \\(A\\) is a symmetric matrix, then eigenvectors corresponding to distinct eigenvalues are orthogonal:\n\\[\n\\mathbf{v}_i^T \\mathbf{v}_j = 0 \\text{ for distinct eigenvalues } \\lambda_i \\text{ and } \\lambda_j.\n\\]\nFor any square matrix \\(A\\), if \\(\\lambda = 0\\) is an eigenvalue, the eigenvectors corresponding to this eigenvalue form the null space of \\(A\\):\n\\[\nE_{0} = \\{ \\mathbf{v} : A\\mathbf{v} = \\mathbf{0} \\} = \\text{Null}(A).\n\\]\nIf\\(A\\) is invertible, then \\(A\\) has no eigenvalue equal to zero, meaning all eigenvectors correspond to non-zero eigenvalues.\nFor\\(A\\) as a scalar multiple of \\(k\\):\n\\[\nA\\mathbf{v} = k \\lambda \\mathbf{v} \\text{ for eigenvalue } \\lambda.\n\\]\n\n\n\n\n\n5.3.2 Eigenvalue Decomposition (Spectral Decomposition)\nFor matrices that are diagonalizable (including symmetric matrices), spectral decomposition expresses the matrix as a combination of its eigenvalues and eigenvectors. Specifically, for a matrix \\(A\\), spectral decomposition is represented as:\n\\[\nA = V \\Lambda V^{-1}\n\\]\nwhere: - \\(V\\) is the matrix of eigenvectors of \\(A\\), - \\(\\Lambda\\) is a diagonal matrix of eigenvalues of \\(A\\), - \\(V^{-1}\\) is the inverse of the matrix of eigenvectors (if \\(V\\) is invertible).\nFor symmetric matrices \\(A\\), the decomposition becomes simpler:\n\\[\nA = Q \\Lambda Q^\\top\n\\]\nHere, \\(Q\\) is an orthogonal matrix of eigenvectors (i.e., \\(Q^\\top Q = I\\)), and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.\n\n\n5.3.3 Geometric Interpretation\nEigenvalues and eigenvectors provide insights into the geometry of linear transformations represented by matrices. Eigenvectors represent directions that remain invariant under the transformation, while eigenvalues indicate how these directions are stretched or compressed.\nFor example, in the case of a transformation matrix that scales or rotates data points, eigenvalues show the magnitude of scaling along the principal axes (directions defined by eigenvectors).\n\n\n5.3.4 Importance of Diagonalization\nThe key advantage of spectral decomposition is that it simplifies matrix operations. When a matrix is diagonalized as \\(A = Q \\Lambda Q^\\top\\), any function of the matrix \\(A\\) (such as powers, exponentials, or inverses) can be easily computed by operating on the diagonal matrix \\(\\Lambda\\). For example:\n\\[\nA^k = Q \\Lambda^k Q^\\top\n\\]\nSince \\(\\Lambda\\) is diagonal, raising \\(\\Lambda\\) to any power \\(k\\) is straightforward, involving only raising each eigenvalue to the power \\(k\\).\n\n\n5.3.5 Properties of Symmetric Matrices\nSpectral decomposition applies particularly well to symmetric matrices, which satisfy \\(A = A^\\top\\). Symmetric matrices have the following key properties:\n\nReal eigenvalues: The eigenvalues of a symmetric matrix are always real numbers.\nOrthogonal eigenvectors: The eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal to each other.\nDiagonalizability: Every symmetric matrix can be diagonalized by an orthogonal matrix.\n\nThese properties make symmetric matrices highly desirable in computational applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#mathematical-requirements-for-spectral-decomposition",
    "href": "module_4.html#mathematical-requirements-for-spectral-decomposition",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "5.4 Mathematical Requirements for Spectral Decomposition",
    "text": "5.4 Mathematical Requirements for Spectral Decomposition\n\n5.4.1 Determining Eigenvalues and Eigenvectors\nThe eigenvalues of a matrix \\(A\\) are the solutions to the characteristic equation:\n\\[\n\\text{det}(A - \\lambda I) = 0\n\\]\nHere, \\(I\\) is the identity matrix, and \\(\\lambda\\) represents the eigenvalues. Solving this polynomial equation provides the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\). Once the eigenvalues are determined, the eigenvectors can be computed by solving the equation \\((A - \\lambda I)v = 0\\) for each eigenvalue.\n\n\n5.4.2 Characteristic Polynomial of \\(2 \\times 2\\) Matrices\nFor a \\(2 \\times 2\\) matrix: \\[\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\] the characteristic polynomial is derived from the determinant of \\(A - \\lambda I\\), where \\(I\\) is the identity matrix:\n\\[\n\\det(A - \\lambda I) = 0\n\\]\nThis leads to: \\[\n\\det\\begin{pmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{pmatrix} = (a - \\lambda)(d - \\lambda) - bc = 0\n\\]\n The characteristic polynomial can be simplified to: \\[\n\\lambda^2 - (a + d)\\lambda + (ad - bc) = 0\n\\]\nThis polynomial can be solved using the quadratic formula: \\[\n\\lambda = \\frac{(a + d) \\pm \\sqrt{(a + d)^2 - 4(ad - bc)}}{2}\n\\]\n\n\n\n\n\n\nShortcut to write Characteristic polynomial of a \\(2\\times 2\\) matrix\n\n\n\nIf \\(A=\\begin{bmatrix} a & b\\\\ c& d\\end{bmatrix}\\), then the characteristic polynomial is \\[\\lambda^2-(\\text{Trace}(A))\\lambda+det(A)=0\\]\nEigen vectors can be found by using the formula: \\[\\begin{equation*}\nEV(\\lambda=\\lambda_1)=\\begin{bmatrix}\\lambda_1-d\\\\ c\\end{bmatrix}\n\\end{equation*}\\]\n\n\n\n\n5.4.3 Problems\nExample 1: Find Eigenvalues and Eigenvectors of the matrix, \\[A = \\begin{pmatrix} 3 & 2 \\\\ 4 & 1 \\end{pmatrix}\\]\nSolution:\nThe characteristic equation is given by \\[det(A-\\lambda I)=0\\]\n\\[\\begin{align*}\n\\lambda^2 - 4\\lambda - 5 &= 0\\\\\n(\\lambda-5)(\\lambda+1)&=0\\\\\n\\end{align*}\\]\nHence the eigen values are \\(\\lambda_1=5,\\quad \\lambda_2=-1\\).\nSo the eigen vectors are: \\[\\begin{align*}\nEV(\\lambda=\\lambda_1)&=\\begin{bmatrix}\\lambda_1-d\\\\ c\\end{bmatrix}\\\\\n\\therefore EV(\\lambda=5)&=\\begin{bmatrix}4\\\\ 4\\end{bmatrix}=\\begin{bmatrix}1\\\\ 1\\end{bmatrix}\\\\\n\\therefore EV(\\lambda=-1)&=\\begin{bmatrix}-2\\\\ 4\\end{bmatrix}=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}\n\\end{align*}\\]\nProblem 2: Calculate the eigenvalues and eigenvectors of the matrix: \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\)\nSolution:\nTo find the eigenvalues and eigenvectors of a \\(2 \\times 2\\) matrix, we can use the shortcut formula for the characteristic polynomial:\n\\[\n\\lambda^2 - \\text{trace}(A)\\lambda + \\det(A) = 0,\n\\]\nwhere \\(A\\) is the matrix. Let’s apply this to the matrix\n\\[\nA = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n\\]\nFirst, we calculate the trace and determinant of \\(A\\):\n\nThe trace is the sum of the diagonal elements:\n\n\\[\n\\text{trace}(A) = 2 + 2 = 4.\n\\]\n\nThe determinant is calculated as follows:\n\n\\[\n\\det(A) = (2)(2) - (1)(1) = 4 - 1 = 3.\n\\]\nNext, substituting the trace and determinant into the characteristic polynomial gives:\n\\[\n\\lambda^2 - (4)\\lambda + 3 = 0,\n\\]\nwhich simplifies to:\n\\[\n\\lambda^2 - 4\\lambda + 3 = 0.\n\\]\nWe can factor this quadratic equation:\n\\[\n(\\lambda - 1)(\\lambda - 3) = 0.\n\\]\nSetting each factor to zero gives the eigenvalues:\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 3.\n\\]\nTo find the eigenvectors corresponding to each eigenvalue, we use the shortcut for the eigenvector of a \\(2 \\times 2\\) matrix \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\):\n\\[\nEV(\\lambda) = \\begin{pmatrix} \\lambda - d \\\\ c \\end{pmatrix}.\n\\]\nFor the eigenvalue \\(\\lambda_1 = 1\\):\n\\[\nEV(1) = \\begin{pmatrix} 1 - 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}.\n\\]\nThis eigenvector can be simplified (up to a scalar multiple) to:\n\\[\n\\mathbf{v_1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n\\]\nFor the eigenvalue \\(\\lambda_2 = 3\\):\n\\[\nEV(3) = \\begin{pmatrix} 3 - 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n\\]\nThis eigenvector is already in a simple form:\n\\[\n\\mathbf{v_2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n\\]\nProblem 3: For the matrix: \\(A = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}\\), find the eigenvalues and eigenvectors.\nSolution:\nWe are given the matrix \\[\nA = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}\n\\]\nand we aim to find its eigenvalues using the characteristic polynomial.\nThe shortcut formula for the characteristic polynomial of a \\(3 \\times 3\\) matrix is given by: \\[\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors of } A)\\lambda - \\det(A) = 0.\n\\]\nThe trace of a matrix is the sum of its diagonal elements. For matrix \\(A\\), we have: \\[\n\\text{tr}(A) = 1 + 1 + 1 = 3.\n\\]\nThe principal minors are the determinants of the \\(2 \\times 2\\) submatrices obtained by deleting one row and one column of \\(A\\).\nThe first minor is obtained by deleting the third row and third column: \\[\n\\det\\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = (1)(1) - (2)(0) = 1.\n\\]\nThe second minor is obtained by deleting the second row and second column: \\[\n\\det\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = (1)(1) - (1)(1) = 0.\n\\]\nThe third minor is obtained by deleting the first row and first column: \\[\n\\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = (1)(1) - (0)(0) = 1.\n\\]\nThus, the sum of the principal minors is: \\[\n1 + 0 + 1 = 2.\n\\]\nThe determinant of \\(A\\) can be calculated using cofactor expansion along the first row: \\[\n\\det(A) = 1 \\cdot \\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix} + 1 \\cdot \\det\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n\\] \\[\n= 1 \\cdot (1) - 2 \\cdot (0) + 1 \\cdot (-1) = 1 - 0 - 1 = 0.\n\\]\nNow, we substitute these values into the characteristic polynomial formula: \\[\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0\n\\] \\[\n\\lambda^3 - 3\\lambda^2 + 2\\lambda - 0 = 0.\n\\]\nWe now solve the equation: \\[\n\\lambda^3 - 3\\lambda^2 + 2\\lambda = 0.\n\\] Factoring out \\(\\lambda\\) and apply factor theorem, we get:\n\\[\\begin{align*}\n   \\lambda(\\lambda^2 - 3\\lambda + 2) &= 0\\\\\n   \\lambda(\\lambda-2)(\\lambda-1)&=0\n\\end{align*}\\]\nThis gives one eigenvalue: \\[\n\\lambda_1 = 0;\\quad \\lambda_2=2;\\quad \\lambda_3=1\n\\]\nNow we find the eigenvectors corresponding to each eigenvalue.\nFor \\(\\lambda_1 = 0\\), solve \\((A - 0I)\\mathbf{v} = 0\\): \\[\n\\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n\\] This gives the system: \\[\nx + 2y + z = 0, \\quad y = 0, \\quad x + z = 0.\n\\] Thus, \\(x = -z\\), and the eigenvector is: \\[\n\\mathbf{v}_1 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n\\]\nFor \\(\\lambda_2 = 2\\), solve \\((A - 2I)\\mathbf{v} = 0\\): \\[\n\\begin{pmatrix} -1 & 2 & 1 \\\\ 0 & -1 & 0 \\\\ 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n\\] This gives the system: \\[\n-x + 2y + z = 0, \\quad -y = 0, \\quad x - z = 0.\n\\] Thus, \\(x = z\\), and the eigenvector is: \\[\n\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n\\]\nFor \\(\\lambda_3 = 1\\), solve \\((A - I)\\mathbf{v} = 0\\): \\[\n\\begin{pmatrix} 0 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n\\] This gives the system: \\[\n2y + z = 0, \\quad x = 0.\n\\] Thus, \\(z = -2y\\), and the eigenvector is: \\[\n\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix}.\n\\]\nProblem 3: If \\(A=\\begin{bmatrix}1&2&4\\\\ 0&3&4\\\\ 1&-1&-1 \\end{bmatrix}\\), compute the eigen values and eigen vectors and left eigen vectors of \\(A\\).\nSolution:\nWe are given the matrix \\[\nA = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix}\n\\]\nand need to find its eigenvalues and eigenvectors.\nThe characteristic polynomial for a \\(3 \\times 3\\) matrix is given by: \\[\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0.\n\\]\nThe trace is the sum of the diagonal elements: \\[\n\\text{tr}(A) = 1 + 3 + (-1) = 3.\n\\]\nWe now compute the \\(2 \\times 2\\) principal minors:\n\nMinor by removing the third row and third column: \\[\n\\det\\begin{pmatrix} 1 & 2 \\\\ 0 & 3 \\end{pmatrix} = (1)(3) - (2)(0) = 3.\n\\]\nMinor by removing the second row and second column: \\[\n\\det\\begin{pmatrix} 1 & 4 \\\\ 1 & -1 \\end{pmatrix} = (1)(-1) - (4)(1) = -1 - 4 = -5.\n\\]\nMinor by removing the first row and first column: \\[\n\\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} = (3)(-1) - (4)(-1) = -3 + 4 = 1.\n\\]\n\nThus, the sum of the principal minors is: \\[\n3 + (-5) + 1 = -1.\n\\]\nWe calculate the determinant of \\(A\\) by cofactor expansion along the first row: \\[\n\\det(A) = 1 \\cdot \\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 0 & 4 \\\\ 1 & -1 \\end{pmatrix} + 4 \\cdot \\det\\begin{pmatrix} 0 & 3 \\\\ 1 & -1 \\end{pmatrix}.\n\\] The \\(2 \\times 2\\) determinants are: \\[\n\\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} = -3 + 4 = 1, \\quad \\det\\begin{pmatrix} 0 & 4 \\\\ 1 & -1 \\end{pmatrix} = -4,\n\\] \\[\n\\det\\begin{pmatrix} 0 & 3 \\\\ 1 & -1 \\end{pmatrix} = -3.\n\\]\nThus: \\[\n\\det(A) = 1 \\cdot 1 - 2 \\cdot (-4) + 4 \\cdot (-3) = 1 + 8 - 12 = -3.\n\\]\nSubstituting into the characteristic polynomial: \\[\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0,\n\\]\nwe get: \\[\n\\lambda^3 - 3\\lambda^2 - \\lambda + 3 = 0.\n\\]\nWe now solve the cubic equation: \\[\\begin{align*}\n   \\lambda^3 - 3\\lambda^2 - \\lambda + 3& = 0. \\\\\n   (\\lambda-1)(\\lambda+1)(\\lambda -3)&=0\n\\end{align*}\\]\n\\[\\lambda_1 = 1, \\quad \\lambda_2 = -1, \\quad \\lambda_3 = 3.\\]\nTo find the eigenvector corresponding to \\(\\lambda_1 = 3\\), solve \\((A - 3I)\\mathbf{v} = 0\\): \\[\nA - 3I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} - 3\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -2 & 2 & 4 \\\\ 0 & 0 & 4 \\\\ 1 & -1 & -4 \\end{pmatrix}.\n\\]\nSolving this system gives the eigenvector: \\[\n\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n\\]\nFor \\(\\lambda_2 = -1\\), solve \\((A +I)\\mathbf{v} = 0\\): \\[\nA +I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} +\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 4 \\\\ 0 & 4 & 4 \\\\ 1 & -1 & 0 \\end{pmatrix}.\n\\]\nNote that the third row is depending on first and second rows. So by finding the cross product of first two rows,\n\\[\n\\mathbf{v}_2 = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n\\]\nFor \\(\\lambda_3 = 1\\), solve \\((A -I)\\mathbf{v} = 0\\): \\[\nA - I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} -\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 2 & 4 \\\\ 0 & 2 & 4 \\\\ 1 & -1 & -2\\end{pmatrix}.\n\\]\nNote that the second row is same as first row. So by finding the cross product of first and third rows, \\[\n\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ -2 \\\\ 1 \\end{pmatrix}.\n\\]\nThus, the eigenvalues of the matrix are: \\[\n\\lambda_1 = 3, \\quad \\lambda_2 = -1, \\quad \\lambda_3 = 1\n\\]\nwith corresponding eigenvectors \\(\\mathbf{v}_1=\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), \\(\\mathbf{v}_2=\\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{v}_3=\\begin{pmatrix} 0 \\\\ -2 \\\\ 1 \\end{pmatrix}\\).\nLeft eigen vectors of the matrix \\(A\\) are eigen vectors of \\(A^T\\).\nHere \\(A^T=\\begin{bmatrix}\n    1&0&1\\\\ 2&3&-1\\\\ 4&4&-1\n\\end{bmatrix}\\).\nSince \\(A\\) and \\(A^T\\) have same eigen values, it is enough to find corresponding eigen vectors. When \\(\\lambda=3\\), the coefficient matrix of \\((A-\\lambda I)X=0\\) reduced into \\(\\begin{bmatrix}\n    -2&0&1\\\\ 2&0&-1\\\\ 4&4&-4\n\\end{bmatrix}\\)\nHere the only independent rows are first and last. So the eigen vector can be found as the cross product of these two rows. \\(\\therefore v_1=\\begin{bmatrix}\n    1\\\\1\\\\2\n\\end{bmatrix}\\).\nWhen \\(\\lambda=-1\\), the coefficient matrix of \\((A-\\lambda I)X=0\\) reduced into \\(\\begin{bmatrix}\n    2&0&1\\\\ 2&4&-1\\\\ 4&4&0\n\\end{bmatrix}\\)\nHere the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows. \\(\\therefore v_2=\\begin{bmatrix}\n    -1\\\\1\\\\2\n\\end{bmatrix}\\). When \\(\\lambda=1\\), the coefficient matrix of \\((A-\\lambda I)X=0\\) reduced into \\(\\begin{bmatrix}\n    0&0&1\\\\ 2&2&-1\\\\ 4&4&-2\n\\end{bmatrix}\\)\nHere the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows. \\(\\therefore v_2=\\begin{bmatrix}\n    -1\\\\1\\\\0\n\\end{bmatrix}\\).\n\n\n5.4.4 Python code to find eigen values and eigen vectors\n\nFind eigen values and eigen vectors of \\(A=\\begin{bmatrix} 2&1\\\\ 1&2\\end{bmatrix}\\).\n\n\nimport numpy as np\nfrom scipy.linalg import null_space\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Find eigenvalues\neigenvalues, _ = np.linalg.eig(A)\n\n# Define identity matrix I\nI = np.eye(A.shape[0])\n\n# Iterate over eigenvalues to find corresponding eigenvectors\nfor i, eigenvalue in enumerate(eigenvalues):\n    # Compute A - lambda * I\n    A_lambda_I = A - eigenvalue * I\n    \n    # Find the null space (which gives the eigenvector)\n    eig_vector = null_space(A_lambda_I)\n    \n    print(f\"Eigenvalue {i+1}: {eigenvalue}\")\n    print(f\"Eigenvector {i+1}:\\n{eig_vector}\\n\")\n\nEigenvalue 1: 3.0\nEigenvector 1:\n[[0.70710678]\n [0.70710678]]\n\nEigenvalue 2: 1.0\nEigenvector 2:\n[[-0.70710678]\n [ 0.70710678]]\n\n\n\nSame can be done using direct approach. Code for this task is given below.\n\nimport numpy as np\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Find eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Display the results\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\\n\", eigenvectors)\n\nEigenvalues: [3. 1.]\nEigenvectors:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\n\n\n\n5.4.5 Diagonalization of Symmetric Matrices\nFor a symmetric matrix \\(A\\), the process of diagonalization can be summarized as follows:\n\nCompute eigenvalues: Solve the characteristic equation \\(\\text{det}(A - \\lambda I) = 0\\) to find the eigenvalues.\nFind eigenvectors: For each eigenvalue \\(\\lambda_i\\), solve \\((A - \\lambda_i I)v_i = 0\\) to find the corresponding eigenvector \\(v_i\\).\nForm the eigenvector matrix: Arrange the eigenvectors into a matrix \\(Q\\), with each eigenvector as a column.\nForm the diagonal matrix of eigenvalues: Construct \\(\\Lambda\\) by placing the eigenvalues along the diagonal of the matrix.\n\nThus, the matrix can be expressed as \\(A = Q \\Lambda Q^\\top\\).\n\nDiagonalize the matrix \\(A=\\begin{bmatrix} 2&1\\\\ 1&2\\end{bmatrix}\\).\n\nPython code for this task is given below.\n\nimport numpy as np\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Step 1: Find eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Step 2: Construct the diagonal matrix D (eigenvalues)\nD = np.diag(eigenvalues)\n\n# Step 3: Construct the matrix P (eigenvectors)\nP = eigenvectors\n\n# Step 4: Calculate the inverse of P\nP_inv = np.linalg.inv(P)\n\n# Verify the diagonalization: A = P D P_inv\nA_reconstructed = P @ D @ P_inv\n\nprint(\"Matrix A:\")\nprint(A)\n\nprint(\"\\nEigenvalues (Diagonal matrix D):\")\nprint(D)\n\nprint(\"\\nEigenvectors (Matrix P):\")\nprint(P)\n\nprint(\"\\nInverse of P:\")\nprint(P_inv)\n\nprint(\"\\nReconstructed matrix A (P D P^(-1)):\")\nprint(A_reconstructed)\n\nMatrix A:\n[[2 1]\n [1 2]]\n\nEigenvalues (Diagonal matrix D):\n[[3. 0.]\n [0. 1.]]\n\nEigenvectors (Matrix P):\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\nInverse of P:\n[[ 0.70710678  0.70710678]\n [-0.70710678  0.70710678]]\n\nReconstructed matrix A (P D P^(-1)):\n[[2. 1.]\n [1. 2.]]\n\n\n\n\n5.4.6 Matrix Functions and Spectral Theorem\nOnce a matrix is diagonalized, various matrix functions become easier to compute. For a function \\(f(A)\\), such as the exponential of a matrix or any power, the function can be applied to the diagonal matrix of eigenvalues:\n\\[\nf(A) = Q f(\\Lambda) Q^\\top\n\\]\nwhere \\(f(\\Lambda)\\) is the function applied element-wise to the eigenvalues in the diagonal matrix \\(\\Lambda\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#overdetermined-systems",
    "href": "module_4.html#overdetermined-systems",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "6.1 Overdetermined Systems",
    "text": "6.1 Overdetermined Systems\nAn overdetermined system of linear equations is a system in which there are more equations than unknowns. Mathematically, if we have a matrix \\(A\\) of size \\(m \\times n\\) where \\(m &gt; n\\), the system can be represented as:\n\\[\nA \\mathbf{x} = \\mathbf{b}\n\\]\nwhere: - \\(A\\) is the coefficient matrix, - \\(\\mathbf{x}\\) is the vector of unknowns (with size \\(n\\)), - \\(\\mathbf{b}\\) is the vector of constants (with size \\(m\\)).\n\n6.1.1 Example of an Overdetermined System\nConsider the following system of equations:\n\\[\\begin{align*}\n2x_1 + 3x_2 &= 5 \\\\\n4x_1 + 6x_2 &= 10 \\\\\n1x_1 + 2x_2 &= 3 \\\\\n\\end{align*}\\]\nHere, we have three equations but only two unknowns (\\(x_1\\) and \\(x_2\\)). This system is overdetermined.\n\n\n6.1.2 Challenges in Solving Overdetermined Systems\n\nNo Exact Solutions: In most cases, an overdetermined system does not have an exact solution because the equations may be inconsistent. For example, if one equation contradicts another, no value of \\(\\mathbf{x}\\) can satisfy all equations simultaneously.\nFinding Best Approximation: When the system is consistent, it may still be that no single solution satisfies all equations perfectly. Therefore, the goal is often to find an approximate solution that minimizes the error.\n\n\n\n6.1.3 Why We Need QR Decomposition\nQR decomposition is particularly useful for solving overdetermined systems for the following reasons:\n\nLeast Squares Solution:\n\nThe primary goal in solving an overdetermined system is to find the least squares solution, which minimizes the sum of the squared residuals (the differences between the left and right sides of the equations). QR decomposition allows us to efficiently compute this solution.\n\nOrthogonality:\n\nThe QR decomposition expresses the matrix \\(A\\) as the product of an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\): \\[\nA = QR\n\\]\nThe orthogonality of \\(Q\\) ensures numerical stability and helps in reducing the problem to solving triangular systems.\n\nStability:\n\nQR decomposition is more stable than other methods, such as Gaussian elimination, especially when dealing with ill-conditioned matrices. This is crucial in applications where precision is important.\n\nComputational Efficiency:\n\nThe process of obtaining the QR decomposition can be performed using efficient algorithms, such as Gram-Schmidt orthogonalization or Householder reflections, which makes it suitable for large systems.\n\n\n\n\n6.1.4 Solving an Overdetermined System using QR Decomposition\nGiven an overdetermined system represented as \\(A \\mathbf{x} = \\mathbf{b}\\), the steps to find the least squares solution using QR decomposition are as follows:\n\nCompute QR Decomposition:\n\nDecompose the matrix \\(A\\) into \\(Q\\) and \\(R\\).\n\nFormulate the Normal Equations:\n\nThe least squares solution can be found from the equation: \\[\nR \\mathbf{x} = Q^T \\mathbf{b}\n\\]\n\nSolve the Triangular System:\n\nSolve for \\(\\mathbf{x}\\) using back substitution, as \\(R\\) is an upper triangular matrix.\n\n\nPython code for solving the above system of equations is given below.\n\nimport numpy as np\n\n# Define the coefficient matrix A and the constant vector b\nA = np.array([[2, 3],\n              [4, 6],\n              [1, 2]])\n\nb = np.array([5, 10, 3])\n\n# Perform QR decomposition\nQ, R = np.linalg.qr(A)\n\n# Calculate the least squares solution\n# Solve the equation R * x = Q^T * b\nQ_b = np.dot(Q.T, b)\nx = np.linalg.solve(R, Q_b)\n\nprint(\"The least squares solution is:\")\nprint(x)\n\nThe least squares solution is:\n[1. 1.]\n\n\n\n\n6.1.5 Problems\n\nProblem 1: Simple Overdetermined System\n\nProblem Statement:\nSolve the overdetermined system given by the equations:\n\\[\\begin{align*}\n2x + 3y &= 5 \\\\\n4x + 6y &= 10 \\\\\n1x + 2y &= 2\n\\end{align*}\\]\n\n# Problem 1: Simple Overdetermined System\n\nimport numpy as np\n\n# Define the coefficient matrix A and the vector b\nA1 = np.array([[2, 3],\n               [4, 6],\n               [1, 2]])\n\nb1 = np.array([5, 10, 2])\n\n# QR decomposition\nQ1, R1 = np.linalg.qr(A1)\nx_qr1 = np.linalg.solve(R1, Q1.T @ b1)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols1, residuals1, rank1, s1 = np.linalg.lstsq(A1, b1, rcond=None)\n\nprint(\"Problem 1 - QR Decomposition Solution:\", x_qr1)\nprint(\"Problem 1 - Ordinary Least Squares Solution:\", x_ols1)\n\nProblem 1 - QR Decomposition Solution: [ 4. -1.]\nProblem 1 - Ordinary Least Squares Solution: [ 4. -1.]\n\n\n\nProblem 2: Overdetermined System with No Exact Solution\n\nProblem Statement:\nSolve the following system:\n\\[\\begin{align*}\nx + 2y &= 3 \\\\\n2x + 4y &= 6 \\\\\n3x + 1y &= 5\n\\end{align*}\\]\n\n# Problem 2: Overdetermined System with No Exact Solution\n\n# Define the coefficient matrix A and the vector b\nA2 = np.array([[1, 2],\n               [2, 4],\n               [3, 1]])\n\nb2 = np.array([3, 6, 5])\n\n# QR decomposition\nQ2, R2 = np.linalg.qr(A2)\nx_qr2 = np.linalg.solve(R2, Q2.T @ b2)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols2, residuals2, rank2, s2 = np.linalg.lstsq(A2, b2, rcond=None)\n\nprint(\"Problem 2 - QR Decomposition Solution:\", x_qr2)\nprint(\"Problem 2 - Ordinary Least Squares Solution:\", x_ols2)\n\nProblem 2 - QR Decomposition Solution: [1.4 0.8]\nProblem 2 - Ordinary Least Squares Solution: [1.4 0.8]\n\n\n\nProblem 3: Overdetermined System with Random Data\n\nProblem Statement:\nGenerate a random overdetermined system and solve it:\n\\[\nAx = b\n\\]\nWhere \\(A\\) is a random \\(6 \\times 3\\) matrix and \\(b\\) is generated accordingly.\n\n# Problem 3: Overdetermined System with Random Data\n\n# Generate a random overdetermined system\nnp.random.seed(0)  # For reproducibility\nA3 = np.random.rand(6, 3)\nx_true = np.array([1, 2, 3])  # True solution\nb3 = A3 @ x_true + np.random.normal(0, 0.1, 6)  # Adding some noise\n\n# QR decomposition\nQ3, R3 = np.linalg.qr(A3)\nx_qr3 = np.linalg.solve(R3, Q3.T @ b3)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols3, residuals3, rank3, s3 = np.linalg.lstsq(A3, b3, rcond=None)\n\nprint(\"Problem 3 - QR Decomposition Solution:\", x_qr3)\nprint(\"Problem 3 - Ordinary Least Squares Solution:\", x_ols3)\n\nProblem 3 - QR Decomposition Solution: [0.94791379 2.10331498 2.98999875]\nProblem 3 - Ordinary Least Squares Solution: [0.94791379 2.10331498 2.98999875]\n\n\n\nProblem 4: Real-World Data Fitting\n\nProblem Statement:\nFit a linear model to the following data points:\n\\[\\begin{align*}\n(1, 2), (2, 3), (3, 5), (4, 7), (5, 11)\n\\end{align*}\\]\n\n# Problem 4: Real-World Data Fitting\n\n# Data points\nx_data = np.array([1, 2, 3, 4, 5])\ny_data = np.array([2, 3, 5, 7, 11])\n\n# Create the design matrix A\nA4 = np.vstack([x_data, np.ones(len(x_data))]).T  # Add intercept\n\n# QR decomposition\nQ4, R4 = np.linalg.qr(A4)\nx_qr4 = np.linalg.solve(R4, Q4.T @ y_data)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols4, residuals4, rank4, s4 = np.linalg.lstsq(A4, y_data, rcond=None)\n\nprint(\"Problem 4 - QR Decomposition Solution:\", x_qr4)\nprint(\"Problem 4 - Ordinary Least Squares Solution:\", x_ols4)\n\nProblem 4 - QR Decomposition Solution: [ 2.2 -1. ]\nProblem 4 - Ordinary Least Squares Solution: [ 2.2 -1. ]\n\n\n\nProblem 5: Polynomial Fit (Higher Degree)\n\nProblem Statement:\nFit a quadratic polynomial to the following data points:\n\\[\\begin{align*}\n(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)\n\\end{align*}\\]\n\n# Problem 5: Polynomial Fit (Higher Degree)\n\n# Data points for polynomial fitting\nx_data_poly = np.array([1, 2, 3, 4, 5])\ny_data_poly = np.array([1, 4, 9, 16, 25])\n\n# Create the design matrix for a quadratic polynomial\nA5 = np.vstack([x_data_poly**2, x_data_poly, np.ones(len(x_data_poly))]).T\n\n# QR decomposition\nQ5, R5 = np.linalg.qr(A5)\nx_qr5 = np.linalg.solve(R5, Q5.T @ y_data_poly)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols5, residuals5, rank5, s5 = np.linalg.lstsq(A5, y_data_poly, rcond=None)\n\nprint(\"Problem 5 - QR Decomposition Solution:\", x_qr5)\nprint(\"Problem 5 - Ordinary Least Squares Solution:\", x_ols5)\n\nProblem 5 - QR Decomposition Solution: [ 1.00000000e+00 -2.73316113e-15  3.80986098e-15]\nProblem 5 - Ordinary Least Squares Solution: [ 1.00000000e+00 -6.77505297e-15  1.06049542e-14]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_4.html#module-review",
    "href": "module_4.html#module-review",
    "title": "4  Linear Algebra for Advanced Applications",
    "section": "6.2 Module review",
    "text": "6.2 Module review\n\nDiagonalize the matrix \\(A = \\begin{bmatrix} 1 & 1 & 3 \\\\ 1 & 5 & 1 \\\\ 3 & 1 & 1 \\end{bmatrix}\\) and write the spectral decomposition of \\(A\\).\n\nHint:\n- To diagonalize a matrix, find the eigenvalues and eigenvectors of the matrix. - Compute the eigenvectors of \\(A\\) to form the matrix \\(P\\), and diagonalize \\(A\\) using the formula \\(A = P D P^{-1}\\) where \\(D\\) is the diagonal matrix of eigenvalues. - Spectral decomposition expresses the matrix as a sum of eigenvalue-weighted outer products of eigenvectors.\n\n\nFind the eigenvalues and eigenvectors of the matrix \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) and write the spectral decomposition.\n\nHint:\n- To find the eigenvalues, solve the characteristic equation \\(det(A - \\lambda I) = 0\\). - Once you have the eigenvalues \\(\\lambda\\), find the eigenvectors by solving \\((A - \\lambda I)v = 0\\) for each eigenvalue. - Express \\(A\\) as a sum of eigenvalue-weighted outer products of its eigenvectors.\n\n\nCompute the QR decomposition of the matrix \\(A = \\begin{bmatrix} 4 & 3 \\\\ 3 & 2 \\end{bmatrix}\\).\n\nHint:\n- Use Gram-Schmidt process or the numpy.linalg.qr() function to compute the QR decomposition. - The matrix \\(A\\) is factored into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\) such that \\(A = QR\\).\n\n\nDerive the LU decomposition of the matrix \\(A = \\begin{bmatrix} 4 & 3 & 1 \\\\ 6 & 3 & 2 \\\\ 3 & 1 & 2 \\end{bmatrix}\\).\n\nHint:\n- Use Gaussian elimination to decompose the matrix into lower and upper triangular matrices. - The matrix \\(A\\) is written as \\(A = LU\\), where \\(L\\) is a lower triangular matrix with 1s on the diagonal, and \\(U\\) is an upper triangular matrix.\n\n\nVerify the rank-nullity theorem for the matrix \\(A = \\begin{bmatrix} 7 & -3 & 5 \\\\ 9 & 11 & 2 \\\\ 16 & 8 & 7 \\end{bmatrix}\\).\n\nHint:\n- The rank-nullity theorem states that the sum of the rank and nullity (dimension of the null space) of a matrix equals the number of its columns. - Compute the rank of the matrix \\(A\\) by finding the number of linearly independent rows or columns, and use this to determine the nullity.\n\n\nWhat are the five important matrix decompositions in linear algebra? Compare them in terms of application.\n\nHint:\n- The five important matrix decompositions include: - LU Decomposition: Used for solving systems of linear equations, especially in computational methods. - QR Decomposition: Primarily used in solving least squares problems and orthogonalization. - Eigenvalue Decomposition: Useful in spectral analysis and principal component analysis (PCA). - Singular Value Decomposition (SVD): Crucial for data compression, noise reduction, and dimensionality reduction. - Cholesky Decomposition: Applied in numerical optimization, especially for solving linear systems with symmetric, positive-definite matrices. - Compare their efficiency, use cases, and computational costs in different applications such as image processing, machine learning, etc.\n\n\nIf \\(A = \\begin{bmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{bmatrix}\\), find the eigenvalues and eigenvectors of \\(A\\).\n\nHint:\n- To find the eigenvalues of \\(A\\), solve the characteristic equation \\(det(A - \\lambda I) = 0\\). - After finding the eigenvalues \\(\\lambda\\), substitute them back into the equation \\((A - \\lambda I) v = 0\\) to find the corresponding eigenvectors. - Eigenvectors should be normalized if required.\n\n\nPerform the QR decomposition of the matrix \\(A = \\begin{bmatrix} 4 & 1 \\\\ 3 & 2 \\end{bmatrix}\\).\n\nHint:\n- Use Gram-Schmidt process or the numpy.linalg.qr() function to compute the QR decomposition. - The matrix \\(A\\) is decomposed into \\(A = QR\\), where \\(Q\\) is an orthogonal matrix and \\(R\\) is an upper triangular matrix.\n\n\nCompute the LU decomposition of the matrix \\(A = \\begin{bmatrix} 2 & 3 & 1 \\\\ 3 & 4 & 2 \\\\ 1 & 2 & 3 \\end{bmatrix}\\).\n\nHint:\n- Use Gaussian elimination or the scipy.linalg.lu() function to find the LU decomposition. - The matrix \\(A\\) is decomposed as \\(A = LU\\), where \\(L\\) is a lower triangular matrix with 1s on the diagonal and \\(U\\) is an upper triangular matrix.\n\n\nFind the Singular Value Decomposition (SVD) of the matrix \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\).\n\nHint:\n- Use the numpy.linalg.svd() function to compute the SVD of matrix \\(A\\). - The SVD decomposes \\(A\\) into \\(A = U \\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is a diagonal matrix containing the singular values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra for Advanced Applications</span>"
    ]
  },
  {
    "objectID": "module_5.html",
    "href": "module_5.html",
    "title": "5  Practical Uses Cases",
    "section": "",
    "text": "5.1 Singular Value Decomposition (SVD) – An Intuitive and Mathematical Approach\nSingular Value Decomposition (SVD) is one of the most powerful matrix factorization tools in linear algebra, extensively used in areas like data compression, signal processing, machine learning, and more. SVD generalizes the concept of diagonalization to non-square matrices, decomposing any \\(m \\times n\\) matrix \\(A\\) into three matrices with well-defined geometric interpretations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#the-svd-theorem",
    "href": "module_5.html#the-svd-theorem",
    "title": "5  Practical Uses Cases",
    "section": "5.2 The SVD Theorem",
    "text": "5.2 The SVD Theorem\nFor any real or complex \\(m \\times n\\) matrix \\(A\\), SVD states that:\n\\[\nA = U \\Sigma V^T\n\\]\nWhere: - \\(U\\) is an \\(m \\times m\\) orthogonal matrix (or unitary in the complex case), - \\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix, with non-negative real numbers (the singular values of \\(A\\)) on the diagonal, - \\(V^T\\) is the transpose (or conjugate transpose in the complex case) of an \\(n \\times n\\) orthogonal matrix \\(V\\).\nThese are range and null spaces for both the column and the row spaces.\n\\[\\begin{align}\n%\n  \\mathbf{C}^{n} &=\n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} \\oplus\n    \\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} \\\\\n%\n  \\mathbf{C}^{m} &=\n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} \\oplus\n    \\color{red} {\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)}\n%\n\\end{align}\\]\nThe singular value decomposition provides an orthonormal basis for the four fundamental subspaces.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#intuition-behind-svd",
    "href": "module_5.html#intuition-behind-svd",
    "title": "5  Practical Uses Cases",
    "section": "5.3 Intuition Behind SVD",
    "text": "5.3 Intuition Behind SVD\nThe SVD can be understood geometrically: - The columns of \\(V\\) form an orthonormal basis of the input space. - The matrix \\(\\Sigma\\) scales and transforms this space along the principal axes. - The columns of \\(U\\) form an orthonormal basis of the output space, representing how the transformed vectors in the input space map to the output space.\nSVD essentially performs three steps on any vector \\(x\\): 1. Rotation: \\(V^T\\) aligns \\(x\\) with the principal axes.\n\nScaling: \\(\\Sigma\\) scales along these axes.\nRotation: \\(U\\) maps the result back to the output space.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#spectral-decomposition-vs.-svd",
    "href": "module_5.html#spectral-decomposition-vs.-svd",
    "title": "5  Practical Uses Cases",
    "section": "5.4 Spectral Decomposition vs. SVD",
    "text": "5.4 Spectral Decomposition vs. SVD\n\nSpectral Decomposition (also known as Eigendecomposition) applies to square matrices and decomposes a matrix \\(A\\) into \\(A = Q \\Lambda Q^{-1}\\), where \\(Q\\) is an orthogonal matrix of eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.\nSVD, on the other hand, applies to any matrix (square or rectangular) and generalizes this idea by using singular values (which are always non-negative) instead of eigenvalues.\n\n\n5.4.1 Comparison:\n\nEigenvectors and Eigenvalues: Spectral decomposition only works if \\(A\\) is square and diagonalizable. It gives insight into the properties of a matrix (e.g., whether it is invertible).\nSingular Vectors and Singular Values: SVD works for any matrix and provides a more general and stable decomposition, useful even for non-square matrices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#steps-to-find-u-sigma-and-vt",
    "href": "module_5.html#steps-to-find-u-sigma-and-vt",
    "title": "5  Practical Uses Cases",
    "section": "5.5 Steps to Find \\(U\\), \\(\\Sigma\\), and \\(V^T\\)",
    "text": "5.5 Steps to Find \\(U\\), \\(\\Sigma\\), and \\(V^T\\)\nGiven a matrix \\(A\\), the SVD factors \\(U\\), \\(\\Sigma\\), and \\(V^T\\) can be computed as follows:\n\nCompute \\(A^T A\\) and find the eigenvalues and eigenvectors:\n\nThe matrix \\(V\\) is formed from the eigenvectors of \\(A^T A\\).\nThe singular values \\(\\sigma_i\\) are the square roots of the eigenvalues of \\(A^T A\\).\n\nConstruct \\(\\Sigma\\):\n\n\\(\\Sigma\\) is a diagonal matrix where the non-zero entries are the singular values \\(\\sigma_1, \\sigma_2, \\dots\\), arranged in decreasing order.\n\nCompute \\(A A^T\\) and find the eigenvectors:\n\nThe matrix \\(U\\) is formed from the eigenvectors of \\(A A^T\\).\n\nTranspose \\(V\\):\n\nThe matrix \\(V^T\\) is simply the transpose of \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#example",
    "href": "module_5.html#example",
    "title": "5  Practical Uses Cases",
    "section": "5.6 Example",
    "text": "5.6 Example\nLet’s consider a simple example where \\(A\\) is a \\(2 \\times 2\\) matrix:\n\\[\nA = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}\n\\]\n\n5.6.1 Step 1: Compute \\(A^T A\\)\n\\[\nA^T A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n\\]\nFind the eigenvalues of \\(A^T A\\):\n\\[\n\\det(A^T A - \\lambda I) = \\det\\begin{pmatrix} 10 - \\lambda & 6 \\\\ 6 & 10 - \\lambda \\end{pmatrix} = 0\n\\]\n\\[\n(10 - \\lambda)^2 - 36 = 0 \\quad \\Rightarrow \\quad \\lambda = 16, \\lambda = 4\n\\]\nThe eigenvalues of \\(A^T A\\) are \\(16\\) and \\(4\\), so the singular values of \\(A\\) are \\(\\sigma_1 = 4\\) and \\(\\sigma_2 = 2\\).\n\n\n5.6.2 Step 2: Find \\(V\\) from the eigenvectors of \\(A^T A\\)\nSolve \\((A^T A - \\lambda I)v = 0\\) for each eigenvalue:\n\nFor \\(\\lambda = 16\\), the eigenvector is \\(v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\),\nFor \\(\\lambda = 4\\), the eigenvector is \\(v_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\\).\n\nThus,\n\\[\nV = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n\\]\n\n\n5.6.3 Step 3: Construct \\(\\Sigma\\)\nThe singular values \\(\\sigma_1 = 4\\) and \\(\\sigma_2 = 2\\), so:\n\\[\n\\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}\n\\]\n\n\n5.6.4 Step 4: Find \\(U\\) from the eigenvectors of \\(A A^T\\)\nSimilarly, compute \\(A A^T\\):\n\\[\nA A^T = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n\\]\nSolve for the eigenvectors of \\(A A^T\\) (same as \\(A^T A\\)):\nThe eigenvectors are \\(u_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}\\) and \\(u_2 = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}\\).\nThus,\n\\[\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n\\]\n\n\n5.6.5 Step 5: Final SVD\nWe can now write the SVD of \\(A\\) as:\n\\[\nA = U \\Sigma V^T\n\\]\nWhere:\n\\[\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad V^T = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n\\]\nPython code to find SVD of this example is given below.\n\nimport numpy as np\n\n# Define the matrix A\nA = np.array([[3, 1],\n              [1, 3]])\n\n# Perform SVD decomposition\nU, Sigma, VT = np.linalg.svd(A)\n\n# Create Sigma matrix from singular values\nSigma_matrix = np.zeros((A.shape[0], A.shape[1]))\nnp.fill_diagonal(Sigma_matrix, Sigma)\n\n# Print results\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nU matrix:\")\nprint(U)\nprint(\"\\nSigma matrix:\")\nprint(Sigma_matrix)\nprint(\"\\nV^T matrix:\")\nprint(VT)\n\n# Verify the decomposition A = U * Sigma * V^T\nA_reconstructed = U @ Sigma_matrix @ VT\nprint(\"\\nReconstructed A (U * Sigma * V^T):\")\nprint(A_reconstructed)\n\nMatrix A:\n[[3 1]\n [1 3]]\n\nU matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nSigma matrix:\n[[4. 0.]\n [0. 2.]]\n\nV^T matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nReconstructed A (U * Sigma * V^T):\n[[3. 1.]\n [1. 3.]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#reconstructing-matrix-a-using-svd",
    "href": "module_5.html#reconstructing-matrix-a-using-svd",
    "title": "5  Practical Uses Cases",
    "section": "5.7 Reconstructing Matrix \\(A\\) Using SVD",
    "text": "5.7 Reconstructing Matrix \\(A\\) Using SVD\nGiven the Singular Value Decomposition (SVD) of a matrix \\(A\\), the matrix can be reconstructed as a linear combination of low-rank matrices using the left singular vectors \\(u_i\\), singular values \\(\\sigma_i\\), and the right singular vectors \\(v_i^T\\).\nThe formula to reconstruct the matrix \\(A\\) is:\n\\[\nA = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n\\]\nwhere: - \\(r\\) is the rank of the matrix \\(A\\) (i.e., the number of non-zero singular values), - \\(\\sigma_i\\) is the \\(i\\)-th singular value from the diagonal matrix \\(\\Sigma\\), - \\(u_i\\) is the \\(i\\)-th column of the matrix \\(U\\) (left singular vectors), - \\(v_i^T\\) is the transpose of the \\(i\\)-th row of the matrix \\(V^T\\) (right singular vectors).\n\n5.7.1 Breakdown of Terms:\n\n\\(u_i \\in \\mathbb{R}^m\\) is a column vector from the matrix \\(U\\) (size \\(m \\times 1\\)),\n\\(v_i^T \\in \\mathbb{R}^n\\) is a row vector from the matrix \\(V^T\\) (size \\(1 \\times n\\)),\n\\(\\sigma_i \\in \\mathbb{R}\\) is a scalar representing the \\(i\\)-th singular value.\n\nEach term \\(\\sigma_i u_i v_i^T\\) represents a rank-1 matrix (the outer product of two vectors). The sum of these rank-1 matrices reconstructs the original matrix \\(A\\).\n\n\n5.7.2 Example:\nFor a matrix \\(A\\), its SVD is represented as:\n\\[\nA = U \\Sigma V^T = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n\\]\nIf the rank of \\(A\\) is 2, then the reconstructed form of \\(A\\) would be:\n\\[\nA = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T\n\\]\nEach term \\(\\sigma_i u_i v_i^T\\) corresponds to a low-rank approximation that contributes to the final matrix. By summing these terms, the full matrix \\(A\\) is obtained.\nPython code demonstrating reconstruction is given below:\n\nimport numpy as np\n\n# Define the matrix A and convert it to float64\nA = np.array([[3, 1], \n              [1, 3]], dtype=np.float64)\n\n# Perform SVD\nU, Sigma, VT = np.linalg.svd(A)\n\n# Reconstruct A using the singular values and singular vectors\nA_reconstructed = np.zeros_like(A)  # This will be float64 now\nfor i in range(len(Sigma)-1):\n    A_reconstructed += Sigma[i] * np.outer(U[:, i], VT[i, :])\n\nprint(\"Original matrix A:\")\nprint(A)\n\nprint(\"\\nReconstructed A from rank-1 matrices:\")\nprint(A_reconstructed)\n\nOriginal matrix A:\n[[3. 1.]\n [1. 3.]]\n\nReconstructed A from rank-1 matrices:\n[[2. 2.]\n [2. 2.]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#singular-value-decomposition-in-image-processing",
    "href": "module_5.html#singular-value-decomposition-in-image-processing",
    "title": "5  Practical Uses Cases",
    "section": "5.8 Singular Value Decomposition in Image Processing",
    "text": "5.8 Singular Value Decomposition in Image Processing\n\n5.8.1 Image Compression\nSVD is widely used for compressing images. By approximating an image with a lower rank matrix, significant amounts of data can be reduced without a substantial loss in quality. The largest singular values and their corresponding singular vectors are retained, allowing for effective storage and transmission.\n\n\n5.8.2 Noise Reduction\nSVD helps in denoising images by separating noise from the original image data. By reconstructing the image using only the most significant singular values and vectors, the impact of noise (often associated with smaller singular values) can be minimized, resulting in a clearer image.\n\n\n5.8.3 Image Reconstruction\nIn applications where parts of an image are missing or corrupted, SVD can facilitate reconstruction. By analyzing the singular values and vectors, missing data can be inferred and filled in, preserving the structural integrity of the image.\n\n\n5.8.4 Facial Recognition\nSVD is employed in facial recognition systems as a means to extract features. By decomposing facial images into their constituent parts, SVD helps identify key features that distinguish different faces, enhancing recognition accuracy.\n\n\n5.8.5 Image Segmentation\nIn image segmentation, SVD can aid in clustering pixels based on their attributes. By reducing dimensionality, it helps identify distinct regions in an image, facilitating the separation of objects and backgrounds.\n\n\n5.8.6 Color Image Processing\nSVD can be applied to color images by treating each color channel separately. This allows for efficient manipulation, compression, and analysis of color images, improving overall processing performance.\n\n\n5.8.7 Pattern Recognition\nSVD is utilized in pattern recognition tasks where it helps to identify and classify patterns within images. By simplifying the data representation, SVD enhances the efficiency and accuracy of recognition algorithms.\n\n\n5.8.8 Example\n\nfrom PIL import Image\nimport urllib.request\nimport matplotlib.pyplot as plt\nurllib.request.urlretrieve(\n  'http://lenna.org/len_top.jpg',\n   \"input.jpg\")\n\nimg = Image.open(\"input.jpg\")\n\n\n# convert to grayscale\nimggray = img.convert('LA')\nplt.figure(figsize=(8,6))\nplt.imshow(imggray);\n\n\n\n\n\n\n\n\n\n# creating image histogram\nimport pandas as pd\nimport numpy as np\nimgmat = np.array(list(imggray.getdata(band=0)), float)\nA=pd.Series(imgmat)\nA.hist(bins=20)\n\n\n\n\n\n\n\n\n\n# printing the pixel values\nprint(imgmat)\n\n[ 80.  80.  79. ... 100.  94.  99.]\n\n\n\n# dimension of the gray scale image matrix\nimgmat.shape\n\n(90000,)\n\n\n\n##loading an image and show it using matrices of pixel values\nfrom skimage import io\nf = \"http://lenna.org/len_top.jpg\" #url of the image\na = io.imread(f) # read the image to a tensor\nc1=a[:,:,0] # channel 1\nc2=a[:,:,1] # channel 2\nc3=a[:,:,2] # channel 3\nprint(c1)\n# dimension of channel-1\nc1.shape\n\n[[109 109 108 ...  54  60  67]\n [112 111 107 ...  52  55  61]\n [111 110 107 ...  51  54  60]\n ...\n [130 129 133 ... 122 119 125]\n [128 127 132 ... 125 119 123]\n [133 130 127 ... 139 133 140]]\n\n\n(225, 400)\n\n\n\nfig = plt.figure(figsize=(12, 3))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nax1.imshow(c1, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax2.imshow(c2, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax3.imshow(c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\nc1_array=np.array(list(c1)).reshape(-1)\npd.Series(c1_array).hist()\n\n\n\n\n\n\n\n\n\n## an application of matrix addition\nplt.imshow(0.34*c1-0.2*c2-0.01*c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\n#converting a grayscale image to numpy array\nimgmat = np.array(list(imggray.getdata(band=0)), float)\nimgmat.shape = (imggray.size[1], imggray.size[0])\nimgmat = np.matrix(imgmat)\nplt.figure(figsize=(8,6))\nplt.imshow(imgmat, cmap='gray');\n\n\n\n\n\n\n\n\nAs promised, one line of command is enough to get the singular value decomposition. \\(U\\) and \\(V\\) are the left-hand side and the right-hand side matrices, respectively. ‘sigma’ is a vector containing the diagonal entries of the matrix \\(\\Sigma\\) The other two lines reconstruct the matrix using the first singular value only. You can already guess the rough shape of the original image.\n\nU, sigma, V = np.linalg.svd(imgmat)\n\nreconstimg = np.matrix(U[:, :1]) * np.diag(sigma[:1]) * np.matrix(V[:1, :])\nplt.imshow(reconstimg, cmap='gray');\n\n\n\n\n\n\n\n\nLet’s see what we get when we use the second, third and fourth singular value as well.\n\nfor i in range(2, 4):\n    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n    plt.imshow(reconstimg, cmap='gray')\n    title = \"n = %s\" % i\n    plt.title(title)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we let \\(i\\) run from 5 to 51, using a step width of 5. For \\(i=50\\), we already get a pretty good image!\n\nfor i in range(5, 51, 5):\n    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n    plt.imshow(reconstimg, cmap='gray')\n    title = \"n = %s\" % i\n    plt.title(title)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut how many singular values do we have after all? The following command gives us the number of entries in sigma. As it is the diagonal matrix, it is stored as a vector and we do not save the zero entries. We now output the number of singular values (the length of the vector sigma, containing the diagonal entries), as well as the size of the matrices \\(U\\) and \\(V\\).\n\nprint(\"We have %d singular values.\" % sigma.shape)\nprint(\"U is of size\", U.shape, \".\")\nprint(\"V is of size\", V.shape, \".\")\nprint(\"The last, or smallest entry in sigma is\", sigma[224])\n\nWe have 225 singular values.\nU is of size (225, 225) .\nV is of size (400, 400) .\nThe last, or smallest entry in sigma is 9.637679189276597\n\n\nAs Python stores the whole singular value decomposition, we do not really save space. But as you saw in the first theoretical exercise of the 10th series, we do not have to compute the whole matrices \\(U\\) and \\(V\\) if we know that we only want to reconstruct the rank \\(k\\) approximation. How many numbers do you have to store for the initial matrix of the picture? How many numbers do you have to store if you want to reconstruct the rank \\(k\\) approximation only?\nUse the following Cell to find an \\(i\\) large enough that you are satisfied with the quality of the image. Check, how much percent of the initial size you have to store. If your picture has a different resolution, you will have to correct the terms.\n\ni = 10\nreconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\nplt.imshow(reconstimg, cmap='gray')\ntitle = \"n = %s\" % i\nplt.title(title)\nplt.show()\n\nnumbers = 400*i + i + 225* i\nprint(\"For this quality, we have to store %d numbers.\" % numbers)\n\n\n\n\n\n\n\n\nFor this quality, we have to store 6260 numbers.\n\n\nIf you really want to have a good quality, say you want to reconstruct using \\(r - 1\\) singular values, where \\(r\\) is the total number of singular values, is it still a good idea to use the singular value decomposition?\n\ni = 30\nreconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\nplt.imshow(reconstimg, cmap='gray')\ntitle = \"n = %s\" % i\nplt.title(title)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#takeaway",
    "href": "module_5.html#takeaway",
    "title": "5  Practical Uses Cases",
    "section": "5.9 Takeaway",
    "text": "5.9 Takeaway\nSingular Value Decomposition provides a general framework for decomposing any matrix into orthogonal components, revealing the underlying structure of the matrix. SVD has numerous applications in machine learning, signal processing, and more. The method to find the matrices \\(U\\), \\(\\Sigma\\), and \\(V^T\\) involves using the eigenvalues and eigenvectors of \\(A^T A\\) and \\(A A^T\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#principal-component-analysis",
    "href": "module_5.html#principal-component-analysis",
    "title": "5  Practical Uses Cases",
    "section": "5.10 Principal Component Analysis",
    "text": "5.10 Principal Component Analysis\nPrincipal Component Analysis (PCA) is fundamentally a technique used for dimensionality reduction, feature extraction, and data visualization. While it’s widely used in various fields, its mathematical foundation lies in Singular Value Decomposition (SVD). This connection between PCA and SVD helps in understanding the underlying mechanics and simplifies PCA computations in high-dimensional spaces. In this section, we will explore PCA as a special case of SVD, present the mathematical derivations, and conclude with an advanced illustration involving image compression.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#principal-component-analysis-as-a-special-case-of-svd",
    "href": "module_5.html#principal-component-analysis-as-a-special-case-of-svd",
    "title": "5  Practical Uses Cases",
    "section": "5.11 Principal Component Analysis as a Special Case of SVD",
    "text": "5.11 Principal Component Analysis as a Special Case of SVD\nPrincipal Component Analysis (PCA) is a widely used technique for dimensionality reduction, feature extraction, and data compression. It is based on the mathematical foundations of linear algebra, particularly eigenvalue decomposition and singular value decomposition (SVD).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#problem-setting-for-pca",
    "href": "module_5.html#problem-setting-for-pca",
    "title": "5  Practical Uses Cases",
    "section": "5.12 Problem Setting for PCA",
    "text": "5.12 Problem Setting for PCA\nLet \\(X \\in \\mathbb{R}^{m \\times n}\\) represent the data matrix, where: - \\(m\\) is the number of data samples, - \\(n\\) is the number of features for each sample.\nTo begin, we center the data by subtracting the mean of each feature from the dataset:\n\\[\nX_{\\text{centered}} = X - \\bar{X}\n\\]\nwhere \\(\\bar{X}\\) is the mean vector for the columns of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#covariance-matrix",
    "href": "module_5.html#covariance-matrix",
    "title": "5  Practical Uses Cases",
    "section": "5.13 Covariance Matrix",
    "text": "5.13 Covariance Matrix\nThe covariance matrix \\(C\\) of the centered data \\(X_{\\text{centered}}\\) is given by:\n\\[\nC = \\frac{1}{m-1} X_{\\text{centered}}^T X_{\\text{centered}} \\in \\mathbb{R}^{n \\times n}\n\\]\nThe covariance matrix captures the relationships between the features of the data. PCA identifies the directions (principal components) that correspond to the maximum variance in the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#eigenvalue-decomposition",
    "href": "module_5.html#eigenvalue-decomposition",
    "title": "5  Practical Uses Cases",
    "section": "5.14 Eigenvalue Decomposition",
    "text": "5.14 Eigenvalue Decomposition\nPCA reduces to finding the eigenvalue decomposition of the covariance matrix \\(C\\):\n\\[\nC v_i = \\lambda_i v_i\n\\]\nwhere: - \\(v_i\\) is the \\(i\\)-th eigenvector, - \\(\\lambda_i\\) is the \\(i\\)-th eigenvalue associated with \\(v_i\\).\nThe eigenvectors \\(v_i\\) represent the principal components, and the eigenvalues \\(\\lambda_i\\) represent the variance captured by each principal component.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#singular-value-decomposition-svd",
    "href": "module_5.html#singular-value-decomposition-svd",
    "title": "5  Practical Uses Cases",
    "section": "5.15 Singular Value Decomposition (SVD)",
    "text": "5.15 Singular Value Decomposition (SVD)\nPCA can be interpreted as a special case of Singular Value Decomposition (SVD). The SVD of the centered data matrix \\(X_{\\text{centered}}\\) is:\n\\[\nX_{\\text{centered}} = U \\Sigma V^T\n\\]\nwhere: - \\(U \\in \\mathbb{R}^{m \\times m}\\) contains the left singular vectors, - \\(\\Sigma \\in \\mathbb{R}^{m \\times n}\\) is the diagonal matrix of singular values, - \\(V \\in \\mathbb{R}^{n \\times n}\\) contains the right singular vectors.\nThe right singular vectors \\(V\\) are equivalent to the eigenvectors of the covariance matrix \\(C\\), and the singular values \\(\\sigma_i\\) are related to the eigenvalues \\(\\lambda_i\\) by:\n\\[\n\\lambda_i = \\sigma_i^2\n\\]\n\n\n\n\n\n\nPCA Derivation Summary\n\n\n\nTo summarize:\n\nCenter the data by subtracting the mean of each feature.\nCompute the covariance matrix \\(C = \\frac{1}{m-1} X_{\\text{centered}}^T X_{\\text{centered}}\\).\nPerform eigenvalue decomposition of \\(C\\) to find the eigenvectors and eigenvalues.\nAlternatively, perform SVD of \\(X_{\\text{centered}}\\). The right singular vectors are the principal components, and the singular values are related to the variance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#applications-of-pca",
    "href": "module_5.html#applications-of-pca",
    "title": "5  Practical Uses Cases",
    "section": "5.16 Applications of PCA",
    "text": "5.16 Applications of PCA\nPCA has several practical applications:\n\nDimensionality Reduction: PCA reduces the number of dimensions in a dataset while retaining the most important information.\nImage Compression: PCA can compress images by keeping only the principal components that capture the most variance.\nFeature Extraction: PCA is used in machine learning to extract the most significant features from high-dimensional datasets, speeding up computations and preventing overfitting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#image-compression-using-pca",
    "href": "module_5.html#image-compression-using-pca",
    "title": "5  Practical Uses Cases",
    "section": "5.17 Image Compression using PCA",
    "text": "5.17 Image Compression using PCA\nConsider an image represented as a matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), where each entry is a pixel intensity. To apply PCA for image compression:\n\nFlatten the image into a matrix where each row represents a pixel.\nCompute the covariance matrix of the image.\nPerform eigenvalue decomposition or SVD on the covariance matrix.\nSelect the top \\(k\\) eigenvectors and project the image onto these eigenvectors.\nReconstruct the image using the reduced data.\n\n\n5.17.1 Sample problems\nConsider the following dataset:\n\\[\nX = \\begin{pmatrix}\n2 & 3 \\\\\n3 & 5 \\\\\n5 & 7\n\\end{pmatrix}\n\\]\n\n\n5.17.2 Step 1: Center the Data\nCalculate the mean of each feature (column):\n\\[\n\\text{Mean} = \\begin{pmatrix}\n\\frac{2+3+5}{3} \\\\\n\\frac{3+5+7}{3}\n\\end{pmatrix} = \\begin{pmatrix}\n3.33 \\\\\n5\n\\end{pmatrix}\n\\]\nSubtract the mean from each data point to center the data:\n\\[\nX_{\\text{centered}} = X - \\text{Mean} = \\begin{pmatrix}\n2 - 3.33 & 3 - 5 \\\\\n3 - 3.33 & 5 - 5 \\\\\n5 - 3.33 & 7 - 5\n\\end{pmatrix} = \\begin{pmatrix}\n-1.33 & -2 \\\\\n-0.33 & 0 \\\\\n1.67 & 2\n\\end{pmatrix}\n\\]\n\n\n5.17.3 Step 2: Calculate the Covariance Matrix\nUsing the formula:\n\\[\nC = \\frac{1}{n-1} X_{\\text{centered}}^T X_{\\text{centered}}\n\\]\nwhere \\(n = 3\\) (the number of data points).\n\nTranspose of Centered Data:\n\n\\[\nX_{\\text{centered}}^T = \\begin{pmatrix}\n-1.33 & -0.33 & 1.67 \\\\\n-2 & 0 & 2\n\\end{pmatrix}\n\\]\n\nMultiply \\(X_{\\text{centered}}^T\\) by \\(X_{\\text{centered}}\\):\n\n\\[\nX_{\\text{centered}}^T X_{\\text{centered}} = \\begin{pmatrix}\n(-1.33)(-1.33) + (-0.33)(-0.33) + (1.67)(1.67) & (-1.33)(-2) + (-0.33)(0) + (1.67)(2) \\\\\n(-2)(-1.33) + (0)(-0.33) + (2)(1.67) & (-2)(-2) + (0)(0) + (2)(2)\n\\end{pmatrix}\n\\]\nCalculating each entry:\n\nFirst entry: \\[\n(-1.33)^2 + (-0.33)^2 + (1.67)^2 = 1.7689 + 0.1089 + 2.7889 = 4.6667\n\\]\nSecond entry: \\[\n(-1.33)(-2) + (1.67)(2) = 2.66 + 3.34 = 6\n\\]\n\nThus, \\[\nX_{\\text{centered}}^T X_{\\text{centered}} = \\begin{pmatrix} 4.67 & 6 \\\\ 6 & 8 \\end{pmatrix}\n\\]\n\nCalculate the Covariance Matrix:\n\n\\[\nC = \\frac{1}{3-1} \\begin{pmatrix} 4.67 & 6 \\\\ 6 & 8 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4.67 & 6 \\\\ 6 & 8 \\end{pmatrix} = \\begin{pmatrix} 2.335 & 3 \\\\ 3 & 4 \\end{pmatrix}\n\\]\n\n\n5.17.4 Step 3: Calculate Eigenvalues and Eigenvectors\n\nEigenvalue Equation:\nThe characteristic polynomial is given by:\n\\[\n\\text{det}(C - \\lambda I) = 0\n\\]\nwhere \\(I\\) is the identity matrix. So:\n\\[\nC - \\lambda I = \\begin{pmatrix} 2.335 - \\lambda & 3 \\\\ 3 & 4 - \\lambda \\end{pmatrix}\n\\]\nThe determinant is:\n\\[\n\\text{det}(C - \\lambda I) = (2.335 - \\lambda)(4 - \\lambda) - (3)(3)\n\\]\nExpanding this, we get:\n\\[\n\\text{det}(C - \\lambda I) = (2.335 \\cdot 4 - 2.335 \\lambda - 4 \\lambda + \\lambda^2 - 9) = \\lambda^2 - (6.335)\\lambda + (9.34 - 9) = \\lambda^2 - 6.335\\lambda + 0.34\n\\]\nFinding Eigenvalues:\nSolving the characteristic equation:\n\\[\n\\lambda^2 - 6.335\\lambda + 0.34 = 0\n\\]\nUsing the quadratic formula:\n\\[\n\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{6.335 \\pm \\sqrt{(-6.335)^2 - 4 \\cdot 1 \\cdot 0.34}}{2}\n\\]\nCalculate \\(b^2 - 4ac\\):\n\\[\n(-6.335)^2 - 4 \\cdot 1 \\cdot 0.34 \\approx 40.096225 - 1.36 = 38.736225\n\\]\nThus, the eigenvalues are:\n\\[\n\\lambda_1 = \\frac{6.335 + \\sqrt{38.736225}}{2}, \\quad \\lambda_2 = \\frac{6.335 - \\sqrt{38.736225}}{2}\n\\]\nFinding Eigenvectors:\n\nFor each eigenvalue \\(\\lambda\\), solve:\n\\[\n(C - \\lambda I) \\mathbf{v} = 0\n\\]\nLet’s denote the eigenvalues we find as \\(\\lambda_1\\) and \\(\\lambda_2\\). For each \\(\\lambda\\):\n\nSubstitute \\(\\lambda\\) into \\((C - \\lambda I)\\) and set up the equation to find the eigenvectors.\n\n\n\n5.17.5 Python Implementation\nHere’s the Python code to perform PCA step by step, corresponding to the tasks outlined above:\n\nimport numpy as np\n\n# Given dataset\nX = np.array([[2, 3],\n              [3, 5],\n              [5, 7]])\n\n# Step 1: Center the data\nmean = np.mean(X, axis=0)\nX_centered = X - mean\n\n# Step 2: Calculate the covariance matrix\ncov_matrix = np.cov(X_centered, rowvar=False)\n\n# Step 3: Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# Display the results\nprint(\"Mean:\\n\", mean)\nprint(\"Centered Data:\\n\", X_centered)\nprint(\"Covariance Matrix:\\n\", cov_matrix)\nprint(\"Eigenvalues:\\n\", eigenvalues)\nprint(\"Eigenvectors:\\n\", eigenvectors)\n\n# Step 4: Project data onto principal components\nX_pca = X_centered @ eigenvectors\n\nprint(\"PCA Result (Projected Data):\\n\", X_pca)\n\nMean:\n [3.33333333 5.        ]\nCentered Data:\n [[-1.33333333 -2.        ]\n [-0.33333333  0.        ]\n [ 1.66666667  2.        ]]\nCovariance Matrix:\n [[2.33333333 3.        ]\n [3.         4.        ]]\nEigenvalues:\n [0.05307638 6.28025695]\nEigenvectors:\n [[-0.79612934 -0.60512649]\n [ 0.60512649 -0.79612934]]\nPCA Result (Projected Data):\n [[-0.14874719  2.39909401]\n [ 0.26537645  0.20170883]\n [-0.11662926 -2.60080284]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#principal-component-analysis-pca-for-image-reconstruction",
    "href": "module_5.html#principal-component-analysis-pca-for-image-reconstruction",
    "title": "5  Practical Uses Cases",
    "section": "5.18 Principal Component Analysis (PCA) for Image Reconstruction",
    "text": "5.18 Principal Component Analysis (PCA) for Image Reconstruction\nIn this example, we will use PCA to reduce the dimensionality of an image and then reconstruct it using only a fraction of the principal components. We will demonstrate this with the famous Lena image.\nStep 1: Load and Display the Image\nFirst, let’s load the image using the PIL library and display it.\n\nfrom PIL import Image\nimport urllib.request\nimport matplotlib.pyplot as plt\n\n# Load the image from URL\nurllib.request.urlretrieve('http://lenna.org/len_top.jpg', \"input.jpg\")\n\n# Open and display the image\nimg = Image.open(\"input.jpg\")\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nStep 2: Convert the Image to Grayscale\nTo simplify the PCA process, we will work with the grayscale version of the image.\n\n# Convert image to grayscale\nimg_gray = img.convert('L')\nplt.imshow(img_gray, cmap='gray')\nplt.title(\"Grayscale Image\")\nplt.axis('off')\nplt.show()\n\n# Convert to NumPy array\nimg_array = np.array(img_gray)\nprint(\"Image Shape:\", img_array.shape)\n\n\n\n\n\n\n\n\nImage Shape: (225, 400)\n\n\nStep 3: Apply PCA to the Grayscale Image\nWe will apply PCA to compress the image by reducing the number of principal components used for reconstruction.\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Flatten the image into 2D (pixels x features)\nimg_flattened = img_array / 255.0  # Normalize pixel values\npca = PCA(n_components=50)  # Choose 50 components\n\n# Fit PCA on the image and transform\nimg_transformed = pca.fit_transform(img_flattened)\n\n# Print the explained variance ratio\nprint(\"Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_))\n\nExplained Variance Ratio: 0.9480357612223579\n\n\nStep 4: Reconstruct the Image using the Reduced Components\nNow we use the transformed PCA components to reconstruct the image and compare it with the original.\n\n# Reconstruct the image from PCA components\nimg_reconstructed = pca.inverse_transform(img_transformed)\n\n# Rescale the image back to original pixel values (0-255)\nimg_reconstructed = (img_reconstructed * 255).astype(np.uint8)\n\n# Plot the reconstructed image\nplt.imshow(img_reconstructed, cmap='gray')\nplt.title(\"Reconstructed Image with 50 Components\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nStep 5: Compare the Original and Reconstructed Images\nFinally, let’s compare the original grayscale image with the PCA-reconstructed image to see how well it retains essential details.\n\n# Plot original and reconstructed side by side\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Original Grayscale Image\nax[0].imshow(img_array, cmap='gray')\nax[0].set_title(\"Original Grayscale Image\")\nax[0].axis('off')\n\n# Reconstructed Image\nax[1].imshow(img_reconstructed, cmap='gray')\nax[1].set_title(\"Reconstructed Image (50 Components)\")\nax[1].axis('off')\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#micro-projects",
    "href": "module_5.html#micro-projects",
    "title": "5  Practical Uses Cases",
    "section": "5.19 Micro Projects",
    "text": "5.19 Micro Projects\n\nLinear Regression\n\nLink to GitHub\nhttps://github.com/sijuswamy/AIML_Files",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#sample-questions-for-lab-work",
    "href": "module_5.html#sample-questions-for-lab-work",
    "title": "5  Practical Uses Cases",
    "section": "5.20 Sample questions for Lab Work",
    "text": "5.20 Sample questions for Lab Work\nThis lab exam covers Spectral Decomposition, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). For each question, perform the necessary computations and provide your answers.\n\n\n5.20.1 Question 1: Spectral Decomposition of a Symmetric Matrix\nFor the matrix \\[\nA = \\begin{bmatrix} 1 & 2 & 6 \\\\ 3 & 5 & 6 \\\\ 4 & 6 & 9 \\end{bmatrix}\n\\] find its spectral decomposition. Calculate the eigenvalues, eigenvectors, and express \\(A\\) as \\(PDP^{-1}\\), where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) contains the eigenvectors.\n\n\n\n5.20.2 Question 2: Diagonalization of a Matrix\nGiven \\[\nB = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\] find the eigenvalues and eigenvectors, and use them to express \\(B\\) in its spectral decomposition form.\n\n\n\n5.20.3 Question 3: Eigenvalues and Eigenvectors of a Square Matrix\nFor the matrix \\[\nC = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}\n\\] find the eigenvalues and eigenvectors. Verify that the matrix can be reconstructed from its eigenvalues and eigenvectors.\n\n\n\n5.20.4 Question 4: Orthogonal Matrix Decomposition\nFor the matrix \\[\nD = \\begin{bmatrix} 5 & 4 \\\\ 4 & 5 \\end{bmatrix}\n\\] find its spectral decomposition and confirm whether the eigenvectors form an orthogonal matrix.\n\n\n\n5.20.5 Question 5: Singular Value Decomposition (SVD)\nFor the matrix \\[\nE = \\begin{bmatrix} 3 & 1 \\\\ -1 & 3 \\end{bmatrix}\n\\] compute its Singular Value Decomposition. Write the matrix as \\(U \\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is a diagonal matrix with singular values.\n\n\n\n5.20.6 Question 6: Rank-1 Approximation Using SVD\nUsing the matrix \\[\nF = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\n\\] perform a rank-1 approximation. Calculate the Frobenius norm of the difference between \\(F\\) and its rank-1 approximation.\n\n\n\n5.20.7 Question 7: Matrix Compression Using SVD\nFor the matrix \\[\nG = \\begin{bmatrix} 10 & 20 & 30 \\\\ 20 & 30 & 40 \\\\ 30 & 40 & 50 \\end{bmatrix}\n\\] find a compressed form of \\(G\\) using only the most significant singular value. Provide the resulting approximation.\n\n\n\n5.20.8 Question 8: SVD Data Reconstruction\nFor the matrix \\[\nH = \\begin{bmatrix} 4 & 11 \\\\ 14 & 8 \\\\ 1 & 5 \\end{bmatrix}\n\\] find its Singular Value Decomposition. Reconstruct the matrix from the SVD components.\n\n\n\n5.20.9 Question 9: Principal Component Analysis (PCA)\nConsider the dataset \\[\nX = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\] Perform Principal Component Analysis (PCA) by first centering the data and then computing the principal components.\n\n\n\n5.20.10 Question 10: Dimensionality Reduction with PCA\nUsing the dataset \\[\nY = \\begin{bmatrix} 2 & 4 \\\\ 3 & 8 \\\\ 5 & 7 \\\\ 6 & 2 \\\\ 7 & 5 \\end{bmatrix}\n\\] apply PCA to reduce the data to one dimension. Project the data onto the principal component found.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#hint-to-solutions.",
    "href": "module_5.html#hint-to-solutions.",
    "title": "5  Practical Uses Cases",
    "section": "5.21 Hint to Solutions.",
    "text": "5.21 Hint to Solutions.\n\nimport numpy as np\nimport scipy.linalg\n\n# 1. Spectral Decomposition for Matrix A\nA = np.array([[1, 2, 6], [3, 5, 6], [4, 6, 9]])\neigenvalues, eigenvectors = np.linalg.eig(A)\nD = np.diag(eigenvalues)\nP = eigenvectors\nP_inv = np.linalg.inv(P)\nreconstructed_A = P @ D @ P_inv\nprint(\"Eigenvalues:\\n\", eigenvalues)\nprint(\"Eigenvectors:\\n\", eigenvectors)\nprint(\"Reconstructed A:\\n\", reconstructed_A)\n\n# 2. Diagonalization of Matrix B\nB = np.array([[2, 1], [1, 2]])\neigenvalues_B, eigenvectors_B = np.linalg.eig(B)\nD_B = np.diag(eigenvalues_B)\nP_B = eigenvectors_B\nP_inv_B = np.linalg.inv(P_B)\nreconstructed_B = P_B @ D_B @ P_inv_B\nprint(\"\\nEigenvalues of B:\\n\", eigenvalues_B)\nprint(\"Eigenvectors of B:\\n\", eigenvectors_B)\nprint(\"Reconstructed B:\\n\", reconstructed_B)\n\n# 3. Eigenvalues and Eigenvectors for Matrix C\nC = np.array([[4, 0, 0], [0, 3, 1], [0, 1, 2]])\neigenvalues_C, eigenvectors_C = np.linalg.eig(C)\nD_C = np.diag(eigenvalues_C)\nP_C = eigenvectors_C\nreconstructed_C = P_C @ D_C @ np.linalg.inv(P_C)\nprint(\"\\nEigenvalues of C:\\n\", eigenvalues_C)\nprint(\"Eigenvectors of C:\\n\", eigenvectors_C)\nprint(\"Reconstructed C:\\n\", reconstructed_C)\n\n# 4. Orthogonal Matrix Decomposition of D\nD = np.array([[5, 4], [4, 5]])\neigenvalues_D, eigenvectors_D = np.linalg.eig(D)\nP_D = eigenvectors_D\nreconstructed_D = P_D @ np.diag(eigenvalues_D) @ P_D.T\nprint(\"\\nEigenvalues of D:\\n\", eigenvalues_D)\nprint(\"Eigenvectors of D (orthogonal):\\n\", P_D)\nprint(\"Reconstructed D:\\n\", reconstructed_D)\n\n# 5. SVD of Matrix E\nE = np.array([[3, 1], [-1, 3]])\nU, S, Vt = np.linalg.svd(E)\nSigma = np.zeros((U.shape[0], Vt.shape[0]))\nSigma[:len(S), :len(S)] = np.diag(S)\nreconstructed_E = U @ Sigma @ Vt\nprint(\"\\nU:\\n\", U)\nprint(\"Sigma:\\n\", Sigma)\nprint(\"V^T:\\n\", Vt)\nprint(\"Reconstructed E:\\n\", reconstructed_E)\n\n# 6. Rank-1 Approximation Using SVD for Matrix F\nF = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nU_F, S_F, Vt_F = np.linalg.svd(F)\nrank_1_F = np.outer(U_F[:, 0] * S_F[0], Vt_F[0, :])\nfrobenius_norm = np.linalg.norm(F - rank_1_F, 'fro')\nprint(\"\\nRank-1 Approximation of F:\\n\", rank_1_F)\nprint(\"Frobenius Norm of Difference:\\n\", frobenius_norm)\n\n# 7. SVD for Image Compression (Matrix G)\nG = np.array([[10, 20, 30], [20, 30, 40], [30, 40, 50]])\nU_G, S_G, Vt_G = np.linalg.svd(G)\ncompressed_G = np.outer(U_G[:, 0] * S_G[0], Vt_G[0, :])\nprint(\"\\nCompressed Matrix G:\\n\", compressed_G)\n\n# 8. SVD Data Reconstruction for Matrix H\nH = np.array([[4, 11], [14, 8], [1, 5]])\nU_H, S_H, Vt_H = np.linalg.svd(H)\nSigma_H = np.zeros((U_H.shape[0], Vt_H.shape[0]))\nSigma_H[:len(S_H), :len(S_H)] = np.diag(S_H)\nreconstructed_H = U_H @ Sigma_H @ Vt_H\nprint(\"\\nReconstructed H:\\n\", reconstructed_H)\n\n# 9. PCA on Small Dataset X\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\nX_centered = X - np.mean(X, axis=0)\nU_X, S_X, Vt_X = np.linalg.svd(X_centered)\nprint(\"\\nCentered Data X:\\n\", X_centered)\nprint(\"Principal Components of X:\\n\", Vt_X.T)\n\n# 10. Dimensionality Reduction with PCA for Dataset Y\nY = np.array([[2, 4], [3, 8], [5, 7], [6, 2], [7, 5]])\nY_centered = Y - np.mean(Y, axis=0)\nU_Y, S_Y, Vt_Y = np.linalg.svd(Y_centered)\nY_projected = Y_centered @ Vt_Y.T[:, :1]\nprint(\"\\n1D Projection of Y:\\n\", Y_projected)\n\nEigenvalues:\n [15.41619849 -1.          0.58380151]\nEigenvectors:\n [[-0.38597937 -0.94280904  0.67025624]\n [-0.54141317  0.23570226 -0.71674952]\n [-0.74692149  0.23570226  0.19242323]]\nReconstructed A:\n [[1. 2. 6.]\n [3. 5. 6.]\n [4. 6. 9.]]\n\nEigenvalues of B:\n [3. 1.]\nEigenvectors of B:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nReconstructed B:\n [[2. 1.]\n [1. 2.]]\n\nEigenvalues of C:\n [1.38196601 3.61803399 4.        ]\nEigenvectors of C:\n [[ 0.          0.          1.        ]\n [ 0.52573111 -0.85065081  0.        ]\n [-0.85065081 -0.52573111  0.        ]]\nReconstructed C:\n [[4. 0. 0.]\n [0. 3. 1.]\n [0. 1. 2.]]\n\nEigenvalues of D:\n [9. 1.]\nEigenvectors of D (orthogonal):\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nReconstructed D:\n [[5. 4.]\n [4. 5.]]\n\nU:\n [[-0.9486833   0.31622777]\n [ 0.31622777  0.9486833 ]]\nSigma:\n [[3.16227766 0.        ]\n [0.         3.16227766]]\nV^T:\n [[-1. -0.]\n [ 0.  1.]]\nReconstructed E:\n [[ 3.  1.]\n [-1.  3.]]\n\nRank-1 Approximation of F:\n [[1.73621779 2.07174246 2.40726714]\n [4.2071528  5.02018649 5.83322018]\n [6.6780878  7.96863051 9.25917322]]\nFrobenius Norm of Difference:\n 1.068369514554709\n\nCompressed Matrix G:\n [[14.27105069 20.7349008  27.19875091]\n [20.7349008  30.12645113 39.51800146]\n [27.19875091 39.51800146 51.83725201]]\n\nReconstructed H:\n [[ 4. 11.]\n [14.  8.]\n [ 1.  5.]]\n\nCentered Data X:\n [[-3. -3.]\n [-1. -1.]\n [ 1.  1.]\n [ 3.  3.]]\nPrincipal Components of X:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\n1D Projection of Y:\n [[-0.34611992]\n [-3.22299204]\n [-1.32087901]\n [ 3.45810614]\n [ 1.43188483]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "module_5.html#module-review",
    "href": "module_5.html#module-review",
    "title": "5  Practical Uses Cases",
    "section": "5.22 Module review",
    "text": "5.22 Module review\n\nWrite the Singular Value Decomposition of \\(A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) and reconstruct the highest scaled component of \\(A\\).\n\nHint:\n- To perform SVD, use numpy.linalg.svd() to decompose the matrix \\(A\\) into \\(A = U \\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is a diagonal matrix containing the singular values. - After performing the SVD, reconstruct the highest scaled component of \\(A\\) using the first singular value and its corresponding singular vectors.\n\n\nExplain the Principal Component Analysis (PCA) as a special case of the Singular Value Decomposition (SVD).\n\nHint:\n- PCA is a dimensionality reduction technique that projects data onto a set of orthogonal axes called principal components. - SVD is used in PCA to decompose the data matrix \\(X\\) into \\(X = U \\Sigma V^T\\). The columns of \\(U\\) are the principal components, and \\(\\Sigma\\) contains the scaling factors. - PCA involves selecting the top components based on the largest singular values in \\(\\Sigma\\).\n\n\nExplain the applications of the Singular Value Decomposition (SVD) in image processing.\n\nHint:\n- SVD is used in image compression by approximating the original image with a reduced set of singular values and vectors, effectively reducing storage requirements. - It is also used in noise reduction, where small singular values are discarded, preserving only the significant components of an image. - SVD is widely applied in face recognition and feature extraction as it reduces the dimensionality while retaining important features of the image.\n\n\nWrite the steps in Principal Component Analysis (PCA) of the matrix \\(A\\) so as to get the first two principal components using NumPy functions.\n\nHint:\n- Center the matrix by subtracting the mean of each column from the respective column elements. - Compute the covariance matrix of the centered data. - Perform Singular Value Decomposition (SVD) on the covariance matrix using numpy.linalg.svd(). - The first two principal components are the first two columns of the matrix \\(U\\) obtained from the SVD of the covariance matrix. - Optionally, plot the projected data to visualize the results.\n\n\nGiven a matrix \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\), perform PCA and extract the top two principal components.\n\nHint:\n- Center the matrix by subtracting the mean of each column. - Compute the covariance matrix. - Perform SVD on the covariance matrix using numpy.linalg.svd(). - The first two principal components will be the first two columns of the matrix \\(U\\). - Project the original data onto these components to reduce the dimensionality.\n\n\nExplain the difference between the Singular Value Decomposition (SVD) and Eigenvalue Decomposition (EVD).\n\nHint:\n- SVD is applicable to any matrix, while EVD is only defined for square matrices. - SVD decomposes a matrix into three components: \\(A = U \\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices, and \\(\\Sigma\\) is a diagonal matrix containing singular values. - EVD decomposes a square matrix into \\(A = V \\Lambda V^{-1}\\), where \\(V\\) contains the eigenvectors and \\(\\Lambda\\) is a diagonal matrix with eigenvalues. - SVD is more general and can be applied to non-square and non-symmetric matrices.\n\n\nGiven the matrix \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\), perform Singular Value Decomposition (SVD) and extract the first singular value.\n\nHint:\n- Use numpy.linalg.svd() to compute the SVD of \\(A\\). - The singular values are the entries of the diagonal matrix \\(\\Sigma\\). The first singular value corresponds to the largest singular value in \\(\\Sigma\\). - Reconstruct \\(A\\) using the computed \\(U\\), \\(\\Sigma\\), and \\(V^T\\).\n\n\nHow is the Singular Value Decomposition (SVD) used in Latent Semantic Analysis (LSA) for text mining?\n\nHint:\n- LSA is used for extracting the underlying structure in text data by reducing the dimensionality of the term-document matrix. - In LSA, the term-document matrix is decomposed using SVD to obtain a low-rank approximation of the matrix. - This allows for capturing the most important latent semantic structures by selecting the largest singular values and corresponding vectors, leading to better clustering and classification of text data.\n\n\nFor a given matrix \\(A = \\begin{bmatrix} 1 & 2 & 4 \\\\ 0 & 1 & 3 \\\\ 1 & 1 & 5 \\end{bmatrix}\\), compute the rank of the matrix using its Singular Value Decomposition (SVD).\n\nHint:\n- The rank of the matrix \\(A\\) is equal to the number of non-zero singular values in \\(\\Sigma\\). - Use numpy.linalg.svd() to perform the SVD of \\(A\\) and count how many of the singular values are greater than a small threshold (e.g., \\(10^{-10}\\)). - The rank is the number of non-zero singular values in the diagonal matrix \\(\\Sigma\\).\n\n\nUsing the matrix \\(A = \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 3 & 5 \\end{bmatrix}\\), calculate the singular values and vectors using SVD and visualize the result.\n\nHint:\n- Apply numpy.linalg.svd() to compute the singular values and vectors of \\(A\\). - Use the matplotlib library to visualize the data points and the transformation through the singular vectors. - The singular values indicate the importance of each principal component, and the vectors describe the directions of the largest variance in the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Practical Uses Cases</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Harris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nStrang, Gilbert. 2020. Linear Algebra for Everyone. SIAM.\n\n\n———. 2022. Introduction to Linear Algebra. SIAM.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\nReddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python.” Nature Methods\n17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.",
    "crumbs": [
      "References"
    ]
  }
]