<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Practical Uses Cases â€“ Computational Linear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./module_4.html" rel="prev">
<link href="./CME_logo.JPG" rel="icon" type="image/jpeg">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./module_5.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Practical Uses Cases</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Linear Algebra</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sijuswamy/Computational-Linear-Algebra" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Computational-Linear-Algebra.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Computational-Linear-Algebra.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Python for Linear Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Transforming Linear Algebra to Computational Language</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python Libraries for Computational Linear Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra for Advanced Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Practical Uses Cases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="CME_logo.JPG" class="img-fluid figure-img"></p>
<figcaption>Computational Mathematics</figcaption>
</figure>
</div>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#singular-value-decomposition-svd-an-intuitive-and-mathematical-approach" id="toc-singular-value-decomposition-svd-an-intuitive-and-mathematical-approach" class="nav-link active" data-scroll-target="#singular-value-decomposition-svd-an-intuitive-and-mathematical-approach"><span class="header-section-number">5.1</span> Singular Value Decomposition (SVD) â€“ An Intuitive and Mathematical Approach</a></li>
  <li><a href="#the-svd-theorem" id="toc-the-svd-theorem" class="nav-link" data-scroll-target="#the-svd-theorem"><span class="header-section-number">5.2</span> The SVD Theorem</a></li>
  <li><a href="#intuition-behind-svd" id="toc-intuition-behind-svd" class="nav-link" data-scroll-target="#intuition-behind-svd"><span class="header-section-number">5.3</span> Intuition Behind SVD</a></li>
  <li><a href="#spectral-decomposition-vs.-svd" id="toc-spectral-decomposition-vs.-svd" class="nav-link" data-scroll-target="#spectral-decomposition-vs.-svd"><span class="header-section-number">5.4</span> Spectral Decomposition vs.&nbsp;SVD</a>
  <ul class="collapse">
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison"><span class="header-section-number">5.4.1</span> Comparison:</a></li>
  </ul></li>
  <li><a href="#steps-to-find-u-sigma-and-vt" id="toc-steps-to-find-u-sigma-and-vt" class="nav-link" data-scroll-target="#steps-to-find-u-sigma-and-vt"><span class="header-section-number">5.5</span> Steps to Find <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V^T\)</span></a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">5.6</span> Example</a>
  <ul class="collapse">
  <li><a href="#step-1-compute-at-a" id="toc-step-1-compute-at-a" class="nav-link" data-scroll-target="#step-1-compute-at-a"><span class="header-section-number">5.6.1</span> Step 1: Compute <span class="math inline">\(A^T A\)</span></a></li>
  <li><a href="#step-2-find-v-from-the-eigenvectors-of-at-a" id="toc-step-2-find-v-from-the-eigenvectors-of-at-a" class="nav-link" data-scroll-target="#step-2-find-v-from-the-eigenvectors-of-at-a"><span class="header-section-number">5.6.2</span> Step 2: Find <span class="math inline">\(V\)</span> from the eigenvectors of <span class="math inline">\(A^T A\)</span></a></li>
  <li><a href="#step-3-construct-sigma" id="toc-step-3-construct-sigma" class="nav-link" data-scroll-target="#step-3-construct-sigma"><span class="header-section-number">5.6.3</span> Step 3: Construct <span class="math inline">\(\Sigma\)</span></a></li>
  <li><a href="#step-4-find-u-from-the-eigenvectors-of-a-at" id="toc-step-4-find-u-from-the-eigenvectors-of-a-at" class="nav-link" data-scroll-target="#step-4-find-u-from-the-eigenvectors-of-a-at"><span class="header-section-number">5.6.4</span> Step 4: Find <span class="math inline">\(U\)</span> from the eigenvectors of <span class="math inline">\(A A^T\)</span></a></li>
  <li><a href="#step-5-final-svd" id="toc-step-5-final-svd" class="nav-link" data-scroll-target="#step-5-final-svd"><span class="header-section-number">5.6.5</span> Step 5: Final SVD</a></li>
  </ul></li>
  <li><a href="#reconstructing-matrix-a-using-svd" id="toc-reconstructing-matrix-a-using-svd" class="nav-link" data-scroll-target="#reconstructing-matrix-a-using-svd"><span class="header-section-number">5.7</span> Reconstructing Matrix <span class="math inline">\(A\)</span> Using SVD</a>
  <ul class="collapse">
  <li><a href="#breakdown-of-terms" id="toc-breakdown-of-terms" class="nav-link" data-scroll-target="#breakdown-of-terms"><span class="header-section-number">5.7.1</span> Breakdown of Terms:</a></li>
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1"><span class="header-section-number">5.7.2</span> Example:</a></li>
  </ul></li>
  <li><a href="#singular-value-decomposition-in-image-processing" id="toc-singular-value-decomposition-in-image-processing" class="nav-link" data-scroll-target="#singular-value-decomposition-in-image-processing"><span class="header-section-number">5.8</span> Singular Value Decomposition in Image Processing</a>
  <ul class="collapse">
  <li><a href="#image-compression" id="toc-image-compression" class="nav-link" data-scroll-target="#image-compression"><span class="header-section-number">5.8.1</span> Image Compression</a></li>
  <li><a href="#noise-reduction" id="toc-noise-reduction" class="nav-link" data-scroll-target="#noise-reduction"><span class="header-section-number">5.8.2</span> Noise Reduction</a></li>
  <li><a href="#image-reconstruction" id="toc-image-reconstruction" class="nav-link" data-scroll-target="#image-reconstruction"><span class="header-section-number">5.8.3</span> Image Reconstruction</a></li>
  <li><a href="#facial-recognition" id="toc-facial-recognition" class="nav-link" data-scroll-target="#facial-recognition"><span class="header-section-number">5.8.4</span> Facial Recognition</a></li>
  <li><a href="#image-segmentation" id="toc-image-segmentation" class="nav-link" data-scroll-target="#image-segmentation"><span class="header-section-number">5.8.5</span> Image Segmentation</a></li>
  <li><a href="#color-image-processing" id="toc-color-image-processing" class="nav-link" data-scroll-target="#color-image-processing"><span class="header-section-number">5.8.6</span> Color Image Processing</a></li>
  <li><a href="#pattern-recognition" id="toc-pattern-recognition" class="nav-link" data-scroll-target="#pattern-recognition"><span class="header-section-number">5.8.7</span> Pattern Recognition</a></li>
  <li><a href="#example-2" id="toc-example-2" class="nav-link" data-scroll-target="#example-2"><span class="header-section-number">5.8.8</span> Example</a></li>
  </ul></li>
  <li><a href="#takeaway" id="toc-takeaway" class="nav-link" data-scroll-target="#takeaway"><span class="header-section-number">5.9</span> Takeaway</a></li>
  <li><a href="#principal-component-analysis" id="toc-principal-component-analysis" class="nav-link" data-scroll-target="#principal-component-analysis"><span class="header-section-number">5.10</span> Principal Component Analysis</a></li>
  <li><a href="#principal-component-analysis-as-a-special-case-of-svd" id="toc-principal-component-analysis-as-a-special-case-of-svd" class="nav-link" data-scroll-target="#principal-component-analysis-as-a-special-case-of-svd"><span class="header-section-number">5.11</span> Principal Component Analysis as a Special Case of SVD</a></li>
  <li><a href="#problem-setting-for-pca" id="toc-problem-setting-for-pca" class="nav-link" data-scroll-target="#problem-setting-for-pca"><span class="header-section-number">5.12</span> Problem Setting for PCA</a></li>
  <li><a href="#covariance-matrix" id="toc-covariance-matrix" class="nav-link" data-scroll-target="#covariance-matrix"><span class="header-section-number">5.13</span> Covariance Matrix</a></li>
  <li><a href="#eigenvalue-decomposition" id="toc-eigenvalue-decomposition" class="nav-link" data-scroll-target="#eigenvalue-decomposition"><span class="header-section-number">5.14</span> Eigenvalue Decomposition</a></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd"><span class="header-section-number">5.15</span> Singular Value Decomposition (SVD)</a></li>
  <li><a href="#applications-of-pca" id="toc-applications-of-pca" class="nav-link" data-scroll-target="#applications-of-pca"><span class="header-section-number">5.16</span> Applications of PCA</a></li>
  <li><a href="#image-compression-using-pca" id="toc-image-compression-using-pca" class="nav-link" data-scroll-target="#image-compression-using-pca"><span class="header-section-number">5.17</span> Image Compression using PCA</a>
  <ul class="collapse">
  <li><a href="#sample-problems" id="toc-sample-problems" class="nav-link" data-scroll-target="#sample-problems"><span class="header-section-number">5.17.1</span> Sample problems</a></li>
  <li><a href="#step-1-center-the-data" id="toc-step-1-center-the-data" class="nav-link" data-scroll-target="#step-1-center-the-data"><span class="header-section-number">5.17.2</span> Step 1: Center the Data</a></li>
  <li><a href="#step-2-calculate-the-covariance-matrix" id="toc-step-2-calculate-the-covariance-matrix" class="nav-link" data-scroll-target="#step-2-calculate-the-covariance-matrix"><span class="header-section-number">5.17.3</span> Step 2: Calculate the Covariance Matrix</a></li>
  <li><a href="#step-3-calculate-eigenvalues-and-eigenvectors" id="toc-step-3-calculate-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#step-3-calculate-eigenvalues-and-eigenvectors"><span class="header-section-number">5.17.4</span> Step 3: Calculate Eigenvalues and Eigenvectors</a></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation"><span class="header-section-number">5.17.5</span> Python Implementation</a></li>
  </ul></li>
  <li><a href="#principal-component-analysis-pca-for-image-reconstruction" id="toc-principal-component-analysis-pca-for-image-reconstruction" class="nav-link" data-scroll-target="#principal-component-analysis-pca-for-image-reconstruction"><span class="header-section-number">5.18</span> Principal Component Analysis (PCA) for Image Reconstruction</a></li>
  <li><a href="#micro-projects" id="toc-micro-projects" class="nav-link" data-scroll-target="#micro-projects"><span class="header-section-number">5.19</span> Micro Projects</a></li>
  <li><a href="#sample-questions-for-lab-work" id="toc-sample-questions-for-lab-work" class="nav-link" data-scroll-target="#sample-questions-for-lab-work"><span class="header-section-number">5.20</span> Sample questions for Lab Work</a>
  <ul class="collapse">
  <li><a href="#question-1-spectral-decomposition-of-a-symmetric-matrix" id="toc-question-1-spectral-decomposition-of-a-symmetric-matrix" class="nav-link" data-scroll-target="#question-1-spectral-decomposition-of-a-symmetric-matrix"><span class="header-section-number">5.20.1</span> Question 1: Spectral Decomposition of a Symmetric Matrix</a></li>
  <li><a href="#question-2-diagonalization-of-a-matrix" id="toc-question-2-diagonalization-of-a-matrix" class="nav-link" data-scroll-target="#question-2-diagonalization-of-a-matrix"><span class="header-section-number">5.20.2</span> Question 2: Diagonalization of a Matrix</a></li>
  <li><a href="#question-3-eigenvalues-and-eigenvectors-of-a-square-matrix" id="toc-question-3-eigenvalues-and-eigenvectors-of-a-square-matrix" class="nav-link" data-scroll-target="#question-3-eigenvalues-and-eigenvectors-of-a-square-matrix"><span class="header-section-number">5.20.3</span> Question 3: Eigenvalues and Eigenvectors of a Square Matrix</a></li>
  <li><a href="#question-4-orthogonal-matrix-decomposition" id="toc-question-4-orthogonal-matrix-decomposition" class="nav-link" data-scroll-target="#question-4-orthogonal-matrix-decomposition"><span class="header-section-number">5.20.4</span> Question 4: Orthogonal Matrix Decomposition</a></li>
  <li><a href="#question-5-singular-value-decomposition-svd" id="toc-question-5-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#question-5-singular-value-decomposition-svd"><span class="header-section-number">5.20.5</span> Question 5: Singular Value Decomposition (SVD)</a></li>
  <li><a href="#question-6-rank-1-approximation-using-svd" id="toc-question-6-rank-1-approximation-using-svd" class="nav-link" data-scroll-target="#question-6-rank-1-approximation-using-svd"><span class="header-section-number">5.20.6</span> Question 6: Rank-1 Approximation Using SVD</a></li>
  <li><a href="#question-7-matrix-compression-using-svd" id="toc-question-7-matrix-compression-using-svd" class="nav-link" data-scroll-target="#question-7-matrix-compression-using-svd"><span class="header-section-number">5.20.7</span> Question 7: Matrix Compression Using SVD</a></li>
  <li><a href="#question-8-svd-data-reconstruction" id="toc-question-8-svd-data-reconstruction" class="nav-link" data-scroll-target="#question-8-svd-data-reconstruction"><span class="header-section-number">5.20.8</span> Question 8: SVD Data Reconstruction</a></li>
  <li><a href="#question-9-principal-component-analysis-pca" id="toc-question-9-principal-component-analysis-pca" class="nav-link" data-scroll-target="#question-9-principal-component-analysis-pca"><span class="header-section-number">5.20.9</span> Question 9: Principal Component Analysis (PCA)</a></li>
  <li><a href="#question-10-dimensionality-reduction-with-pca" id="toc-question-10-dimensionality-reduction-with-pca" class="nav-link" data-scroll-target="#question-10-dimensionality-reduction-with-pca"><span class="header-section-number">5.20.10</span> Question 10: Dimensionality Reduction with PCA</a></li>
  </ul></li>
  <li><a href="#hint-to-solutions." id="toc-hint-to-solutions." class="nav-link" data-scroll-target="#hint-to-solutions."><span class="header-section-number">5.21</span> Hint to Solutions.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Practical Uses Cases</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="singular-value-decomposition-svd-an-intuitive-and-mathematical-approach" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="singular-value-decomposition-svd-an-intuitive-and-mathematical-approach"><span class="header-section-number">5.1</span> Singular Value Decomposition (SVD) â€“ An Intuitive and Mathematical Approach</h2>
<p>Singular Value Decomposition (SVD) is one of the most powerful matrix factorization tools in linear algebra, extensively used in areas like data compression, signal processing, machine learning, and more. SVD generalizes the concept of diagonalization to non-square matrices, decomposing any <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> into three matrices with well-defined geometric interpretations.</p>
</section>
<section id="the-svd-theorem" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="the-svd-theorem"><span class="header-section-number">5.2</span> The SVD Theorem</h2>
<p>For any real or complex <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span>, SVD states that:</p>
<p><span class="math display">\[
A = U \Sigma V^T
\]</span></p>
<p>Where: - <span class="math inline">\(U\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix (or unitary in the complex case), - <span class="math inline">\(\Sigma\)</span> is an <span class="math inline">\(m \times n\)</span> diagonal matrix, with non-negative real numbers (the singular values of <span class="math inline">\(A\)</span>) on the diagonal, - <span class="math inline">\(V^T\)</span> is the transpose (or conjugate transpose in the complex case) of an <span class="math inline">\(n \times n\)</span> orthogonal matrix <span class="math inline">\(V\)</span>.</p>
<p>These are range and null spaces for both the column and the row spaces.</p>
<p><span class="math display">\[\begin{align}
%
  \mathbf{C}^{n} &amp;=
    \color{blue}{\mathcal{R} \left( \mathbf{A}^{*} \right)} \oplus
    \color{red}{\mathcal{N} \left( \mathbf{A} \right)} \\
%
  \mathbf{C}^{m} &amp;=
    \color{blue}{\mathcal{R} \left( \mathbf{A} \right)} \oplus
    \color{red} {\mathcal{N} \left( \mathbf{A}^{*} \right)}
%
\end{align}\]</span></p>
<p>The singular value decomposition provides an orthonormal basis for the four fundamental subspaces.</p>
</section>
<section id="intuition-behind-svd" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="intuition-behind-svd"><span class="header-section-number">5.3</span> Intuition Behind SVD</h2>
<p>The SVD can be understood geometrically: - The columns of <span class="math inline">\(V\)</span> form an orthonormal basis of the input space. - The matrix <span class="math inline">\(\Sigma\)</span> scales and transforms this space along the principal axes. - The columns of <span class="math inline">\(U\)</span> form an orthonormal basis of the output space, representing how the transformed vectors in the input space map to the output space.</p>
<p>SVD essentially performs three steps on any vector <span class="math inline">\(x\)</span>: 1. <strong>Rotation</strong>: <span class="math inline">\(V^T\)</span> aligns <span class="math inline">\(x\)</span> with the principal axes.</p>
<ol start="2" type="1">
<li><p><strong>Scaling</strong>: <span class="math inline">\(\Sigma\)</span> scales along these axes.</p></li>
<li><p><strong>Rotation</strong>: <span class="math inline">\(U\)</span> maps the result back to the output space.</p></li>
</ol>
</section>
<section id="spectral-decomposition-vs.-svd" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="spectral-decomposition-vs.-svd"><span class="header-section-number">5.4</span> Spectral Decomposition vs.&nbsp;SVD</h2>
<ul>
<li><strong>Spectral Decomposition</strong> (also known as <strong>Eigendecomposition</strong>) applies to <strong>square</strong> matrices and decomposes a matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(A = Q \Lambda Q^{-1}\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix of eigenvectors, and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of eigenvalues.</li>
<li><strong>SVD</strong>, on the other hand, applies to <strong>any</strong> matrix (square or rectangular) and generalizes this idea by using singular values (which are always non-negative) instead of eigenvalues.</li>
</ul>
<section id="comparison" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="comparison"><span class="header-section-number">5.4.1</span> Comparison:</h3>
<ul>
<li><strong>Eigenvectors and Eigenvalues</strong>: Spectral decomposition only works if <span class="math inline">\(A\)</span> is square and diagonalizable. It gives insight into the properties of a matrix (e.g., whether it is invertible).</li>
<li><strong>Singular Vectors and Singular Values</strong>: SVD works for any matrix and provides a more general and stable decomposition, useful even for non-square matrices.</li>
</ul>
</section>
</section>
<section id="steps-to-find-u-sigma-and-vt" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="steps-to-find-u-sigma-and-vt"><span class="header-section-number">5.5</span> Steps to Find <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V^T\)</span></h2>
<p>Given a matrix <span class="math inline">\(A\)</span>, the SVD factors <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V^T\)</span> can be computed as follows:</p>
<ol type="1">
<li><p><strong>Compute <span class="math inline">\(A^T A\)</span> and find the eigenvalues and eigenvectors:</strong></p>
<ul>
<li><p>The matrix <span class="math inline">\(V\)</span> is formed from the eigenvectors of <span class="math inline">\(A^T A\)</span>.</p></li>
<li><p>The singular values <span class="math inline">\(\sigma_i\)</span> are the square roots of the eigenvalues of <span class="math inline">\(A^T A\)</span>.</p></li>
</ul></li>
<li><p><strong>Construct <span class="math inline">\(\Sigma\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(\Sigma\)</span> is a diagonal matrix where the non-zero entries are the singular values <span class="math inline">\(\sigma_1, \sigma_2, \dots\)</span>, arranged in decreasing order.</li>
</ul></li>
<li><p><strong>Compute <span class="math inline">\(A A^T\)</span> and find the eigenvectors:</strong></p>
<ul>
<li>The matrix <span class="math inline">\(U\)</span> is formed from the eigenvectors of <span class="math inline">\(A A^T\)</span>.</li>
</ul></li>
<li><p><strong>Transpose <span class="math inline">\(V\)</span>:</strong></p>
<ul>
<li>The matrix <span class="math inline">\(V^T\)</span> is simply the transpose of <span class="math inline">\(V\)</span>.</li>
</ul></li>
</ol>
</section>
<section id="example" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="example"><span class="header-section-number">5.6</span> Example</h2>
<p>Letâ€™s consider a simple example where <span class="math inline">\(A\)</span> is a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p><span class="math display">\[
A = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{pmatrix}
\]</span></p>
<section id="step-1-compute-at-a" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="step-1-compute-at-a"><span class="header-section-number">5.6.1</span> Step 1: Compute <span class="math inline">\(A^T A\)</span></h3>
<p><span class="math display">\[
A^T A = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} = \begin{pmatrix} 10 &amp; 6 \\ 6 &amp; 10 \end{pmatrix}
\]</span></p>
<p>Find the eigenvalues of <span class="math inline">\(A^T A\)</span>:</p>
<p><span class="math display">\[
\det(A^T A - \lambda I) = \det\begin{pmatrix} 10 - \lambda &amp; 6 \\ 6 &amp; 10 - \lambda \end{pmatrix} = 0
\]</span></p>
<p><span class="math display">\[
(10 - \lambda)^2 - 36 = 0 \quad \Rightarrow \quad \lambda = 16, \lambda = 4
\]</span></p>
<p>The eigenvalues of <span class="math inline">\(A^T A\)</span> are <span class="math inline">\(16\)</span> and <span class="math inline">\(4\)</span>, so the singular values of <span class="math inline">\(A\)</span> are <span class="math inline">\(\sigma_1 = 4\)</span> and <span class="math inline">\(\sigma_2 = 2\)</span>.</p>
</section>
<section id="step-2-find-v-from-the-eigenvectors-of-at-a" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="step-2-find-v-from-the-eigenvectors-of-at-a"><span class="header-section-number">5.6.2</span> Step 2: Find <span class="math inline">\(V\)</span> from the eigenvectors of <span class="math inline">\(A^T A\)</span></h3>
<p>Solve <span class="math inline">\((A^T A - \lambda I)v = 0\)</span> for each eigenvalue:</p>
<ul>
<li>For <span class="math inline">\(\lambda = 16\)</span>, the eigenvector is <span class="math inline">\(v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>,</li>
<li>For <span class="math inline">\(\lambda = 4\)</span>, the eigenvector is <span class="math inline">\(v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}\)</span>.</li>
</ul>
<p>Thus,</p>
<p><span class="math display">\[
V = \begin{pmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}
\]</span></p>
</section>
<section id="step-3-construct-sigma" class="level3" data-number="5.6.3">
<h3 data-number="5.6.3" class="anchored" data-anchor-id="step-3-construct-sigma"><span class="header-section-number">5.6.3</span> Step 3: Construct <span class="math inline">\(\Sigma\)</span></h3>
<p>The singular values <span class="math inline">\(\sigma_1 = 4\)</span> and <span class="math inline">\(\sigma_2 = 2\)</span>, so:</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix} 4 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}
\]</span></p>
</section>
<section id="step-4-find-u-from-the-eigenvectors-of-a-at" class="level3" data-number="5.6.4">
<h3 data-number="5.6.4" class="anchored" data-anchor-id="step-4-find-u-from-the-eigenvectors-of-a-at"><span class="header-section-number">5.6.4</span> Step 4: Find <span class="math inline">\(U\)</span> from the eigenvectors of <span class="math inline">\(A A^T\)</span></h3>
<p>Similarly, compute <span class="math inline">\(A A^T\)</span>:</p>
<p><span class="math display">\[
A A^T = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} = \begin{pmatrix} 10 &amp; 6 \\ 6 &amp; 10 \end{pmatrix}
\]</span></p>
<p>Solve for the eigenvectors of <span class="math inline">\(A A^T\)</span> (same as <span class="math inline">\(A^T A\)</span>):</p>
<p>The eigenvectors are <span class="math inline">\(u_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}\)</span> and <span class="math inline">\(u_2 = \begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[
U = \begin{pmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}
\]</span></p>
</section>
<section id="step-5-final-svd" class="level3" data-number="5.6.5">
<h3 data-number="5.6.5" class="anchored" data-anchor-id="step-5-final-svd"><span class="header-section-number">5.6.5</span> Step 5: Final SVD</h3>
<p>We can now write the SVD of <span class="math inline">\(A\)</span> as:</p>
<p><span class="math display">\[
A = U \Sigma V^T
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
U = \begin{pmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 4 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}, \quad V^T = \begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}
\]</span></p>
<p><code>Python</code> code to find SVD of this example is given below.</p>
<div id="8ff5dccc" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the matrix A</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">1</span>],</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform SVD decomposition</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>U, Sigma, VT <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Sigma matrix from singular values</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>Sigma_matrix <span class="op">=</span> np.zeros((A.shape[<span class="dv">0</span>], A.shape[<span class="dv">1</span>]))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>np.fill_diagonal(Sigma_matrix, Sigma)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix A:"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">U matrix:"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(U)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Sigma matrix:"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Sigma_matrix)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">V^T matrix:"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(VT)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the decomposition A = U * Sigma * V^T</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>A_reconstructed <span class="op">=</span> U <span class="op">@</span> Sigma_matrix <span class="op">@</span> VT</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reconstructed A (U * Sigma * V^T):"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A_reconstructed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Matrix A:
[[3 1]
 [1 3]]

U matrix:
[[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]

Sigma matrix:
[[4. 0.]
 [0. 2.]]

V^T matrix:
[[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]

Reconstructed A (U * Sigma * V^T):
[[3. 1.]
 [1. 3.]]</code></pre>
</div>
</div>
</section>
</section>
<section id="reconstructing-matrix-a-using-svd" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="reconstructing-matrix-a-using-svd"><span class="header-section-number">5.7</span> Reconstructing Matrix <span class="math inline">\(A\)</span> Using SVD</h2>
<p>Given the Singular Value Decomposition (SVD) of a matrix <span class="math inline">\(A\)</span>, the matrix can be reconstructed as a linear combination of low-rank matrices using the left singular vectors <span class="math inline">\(u_i\)</span>, singular values <span class="math inline">\(\sigma_i\)</span>, and the right singular vectors <span class="math inline">\(v_i^T\)</span>.</p>
<p>The formula to reconstruct the matrix <span class="math inline">\(A\)</span> is:</p>
<p><span class="math display">\[
A = \sum_{i=1}^{r} \sigma_i \, u_i \, v_i^T
\]</span></p>
<p>where: - <span class="math inline">\(r\)</span> is the rank of the matrix <span class="math inline">\(A\)</span> (i.e., the number of non-zero singular values), - <span class="math inline">\(\sigma_i\)</span> is the <span class="math inline">\(i\)</span>-th singular value from the diagonal matrix <span class="math inline">\(\Sigma\)</span>, - <span class="math inline">\(u_i\)</span> is the <span class="math inline">\(i\)</span>-th column of the matrix <span class="math inline">\(U\)</span> (left singular vectors), - <span class="math inline">\(v_i^T\)</span> is the transpose of the <span class="math inline">\(i\)</span>-th row of the matrix <span class="math inline">\(V^T\)</span> (right singular vectors).</p>
<section id="breakdown-of-terms" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="breakdown-of-terms"><span class="header-section-number">5.7.1</span> Breakdown of Terms:</h3>
<ul>
<li><span class="math inline">\(u_i \in \mathbb{R}^m\)</span> is a column vector from the matrix <span class="math inline">\(U\)</span> (size <span class="math inline">\(m \times 1\)</span>),</li>
<li><span class="math inline">\(v_i^T \in \mathbb{R}^n\)</span> is a row vector from the matrix <span class="math inline">\(V^T\)</span> (size <span class="math inline">\(1 \times n\)</span>),</li>
<li><span class="math inline">\(\sigma_i \in \mathbb{R}\)</span> is a scalar representing the <span class="math inline">\(i\)</span>-th singular value.</li>
</ul>
<p>Each term <span class="math inline">\(\sigma_i u_i v_i^T\)</span> represents a <strong>rank-1 matrix</strong> (the outer product of two vectors). The sum of these rank-1 matrices reconstructs the original matrix <span class="math inline">\(A\)</span>.</p>
</section>
<section id="example-1" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="example-1"><span class="header-section-number">5.7.2</span> Example:</h3>
<p>For a matrix <span class="math inline">\(A\)</span>, its SVD is represented as:</p>
<p><span class="math display">\[
A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_i \, u_i \, v_i^T
\]</span></p>
<p>If the rank of <span class="math inline">\(A\)</span> is 2, then the reconstructed form of <span class="math inline">\(A\)</span> would be:</p>
<p><span class="math display">\[
A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T
\]</span></p>
<p>Each term <span class="math inline">\(\sigma_i u_i v_i^T\)</span> corresponds to a <strong>low-rank approximation</strong> that contributes to the final matrix. By summing these terms, the full matrix <span class="math inline">\(A\)</span> is obtained.</p>
<p>Python code demonstrating reconstruction is given below:</p>
<div id="01131bb3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the matrix A and convert it to float64</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">1</span>], </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>]], dtype<span class="op">=</span>np.float64)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform SVD</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>U, Sigma, VT <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconstruct A using the singular values and singular vectors</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>A_reconstructed <span class="op">=</span> np.zeros_like(A)  <span class="co"># This will be float64 now</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(Sigma)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    A_reconstructed <span class="op">+=</span> Sigma[i] <span class="op">*</span> np.outer(U[:, i], VT[i, :])</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original matrix A:"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reconstructed A from rank-1 matrices:"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A_reconstructed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original matrix A:
[[3. 1.]
 [1. 3.]]

Reconstructed A from rank-1 matrices:
[[2. 2.]
 [2. 2.]]</code></pre>
</div>
</div>
</section>
</section>
<section id="singular-value-decomposition-in-image-processing" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="singular-value-decomposition-in-image-processing"><span class="header-section-number">5.8</span> Singular Value Decomposition in Image Processing</h2>
<section id="image-compression" class="level3" data-number="5.8.1">
<h3 data-number="5.8.1" class="anchored" data-anchor-id="image-compression"><span class="header-section-number">5.8.1</span> Image Compression</h3>
<p>SVD is widely used for compressing images. By approximating an image with a lower rank matrix, significant amounts of data can be reduced without a substantial loss in quality. The largest singular values and their corresponding singular vectors are retained, allowing for effective storage and transmission.</p>
</section>
<section id="noise-reduction" class="level3" data-number="5.8.2">
<h3 data-number="5.8.2" class="anchored" data-anchor-id="noise-reduction"><span class="header-section-number">5.8.2</span> Noise Reduction</h3>
<p>SVD helps in denoising images by separating noise from the original image data. By reconstructing the image using only the most significant singular values and vectors, the impact of noise (often associated with smaller singular values) can be minimized, resulting in a clearer image.</p>
</section>
<section id="image-reconstruction" class="level3" data-number="5.8.3">
<h3 data-number="5.8.3" class="anchored" data-anchor-id="image-reconstruction"><span class="header-section-number">5.8.3</span> Image Reconstruction</h3>
<p>In applications where parts of an image are missing or corrupted, SVD can facilitate reconstruction. By analyzing the singular values and vectors, missing data can be inferred and filled in, preserving the structural integrity of the image.</p>
</section>
<section id="facial-recognition" class="level3" data-number="5.8.4">
<h3 data-number="5.8.4" class="anchored" data-anchor-id="facial-recognition"><span class="header-section-number">5.8.4</span> Facial Recognition</h3>
<p>SVD is employed in facial recognition systems as a means to extract features. By decomposing facial images into their constituent parts, SVD helps identify key features that distinguish different faces, enhancing recognition accuracy.</p>
</section>
<section id="image-segmentation" class="level3" data-number="5.8.5">
<h3 data-number="5.8.5" class="anchored" data-anchor-id="image-segmentation"><span class="header-section-number">5.8.5</span> Image Segmentation</h3>
<p>In image segmentation, SVD can aid in clustering pixels based on their attributes. By reducing dimensionality, it helps identify distinct regions in an image, facilitating the separation of objects and backgrounds.</p>
</section>
<section id="color-image-processing" class="level3" data-number="5.8.6">
<h3 data-number="5.8.6" class="anchored" data-anchor-id="color-image-processing"><span class="header-section-number">5.8.6</span> Color Image Processing</h3>
<p>SVD can be applied to color images by treating each color channel separately. This allows for efficient manipulation, compression, and analysis of color images, improving overall processing performance.</p>
</section>
<section id="pattern-recognition" class="level3" data-number="5.8.7">
<h3 data-number="5.8.7" class="anchored" data-anchor-id="pattern-recognition"><span class="header-section-number">5.8.7</span> Pattern Recognition</h3>
<p>SVD is utilized in pattern recognition tasks where it helps to identify and classify patterns within images. By simplifying the data representation, SVD enhances the efficiency and accuracy of recognition algorithms.</p>
</section>
<section id="example-2" class="level3" data-number="5.8.8">
<h3 data-number="5.8.8" class="anchored" data-anchor-id="example-2"><span class="header-section-number">5.8.8</span> Example</h3>
<div id="211a335d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'http://lenna.org/len_top.jpg'</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>   <span class="st">"input.jpg"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"input.jpg"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7b5d16b5" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to grayscale</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>imggray <span class="op">=</span> img.convert(<span class="st">'LA'</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(imggray)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-5-output-1.png" width="649" height="381" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="054da4e4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creating image histogram</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>imgmat <span class="op">=</span> np.array(<span class="bu">list</span>(imggray.getdata(band<span class="op">=</span><span class="dv">0</span>)), <span class="bu">float</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>A<span class="op">=</span>pd.Series(imgmat)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>A.hist(bins<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-6-output-1.png" width="592" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f8e9836b" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># printing the pixel values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(imgmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 80.  80.  79. ... 100.  94.  99.]</code></pre>
</div>
</div>
<div id="40dc977f" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dimension of the gray scale image matrix</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>imgmat.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(90000,)</code></pre>
</div>
</div>
<div id="f68f5de8" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">##loading an image and show it using matrices of pixel values</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> io</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="st">"http://lenna.org/len_top.jpg"</span> <span class="co">#url of the image</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> io.imread(f) <span class="co"># read the image to a tensor</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>c1<span class="op">=</span>a[:,:,<span class="dv">0</span>] <span class="co"># channel 1</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>c2<span class="op">=</span>a[:,:,<span class="dv">1</span>] <span class="co"># channel 2</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>c3<span class="op">=</span>a[:,:,<span class="dv">2</span>] <span class="co"># channel 3</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(c1)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># dimension of channel-1</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>c1.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[109 109 108 ...  54  60  67]
 [112 111 107 ...  52  55  61]
 [111 110 107 ...  51  54  60]
 ...
 [130 129 133 ... 122 119 125]
 [128 127 132 ... 125 119 123]
 [133 130 127 ... 139 133 140]]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>(225, 400)</code></pre>
</div>
</div>
<div id="6eb06ad8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">131</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">132</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>ax3 <span class="op">=</span> fig.add_subplot(<span class="dv">133</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>ax1.imshow(c1, cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">255</span>,interpolation<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>ax2.imshow(c2, cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">255</span>,interpolation<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>ax3.imshow(c3, cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">255</span>,interpolation<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-10-output-1.png" width="947" height="194" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="1d09a32e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>c1_array<span class="op">=</span>np.array(<span class="bu">list</span>(c1)).reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>pd.Series(c1_array).hist()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-11-output-1.png" width="592" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ad6ee290" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">## an application of matrix addition</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(<span class="fl">0.34</span><span class="op">*</span>c1<span class="op">-</span><span class="fl">0.2</span><span class="op">*</span>c2<span class="op">-</span><span class="fl">0.01</span><span class="op">*</span>c3, cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">255</span>,interpolation<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-12-output-1.png" width="575" height="339" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ff390d10" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#converting a grayscale image to numpy array</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>imgmat <span class="op">=</span> np.array(<span class="bu">list</span>(imggray.getdata(band<span class="op">=</span><span class="dv">0</span>)), <span class="bu">float</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>imgmat.shape <span class="op">=</span> (imggray.size[<span class="dv">1</span>], imggray.size[<span class="dv">0</span>])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>imgmat <span class="op">=</span> np.matrix(imgmat)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(imgmat, cmap<span class="op">=</span><span class="st">'gray'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-13-output-1.png" width="649" height="381" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As promised, one line of command is enough to get the singular value decomposition. <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are the left-hand side and the right-hand side matrices, respectively. â€˜sigmaâ€™ is a vector containing the diagonal entries of the matrix <span class="math inline">\(\Sigma\)</span> The other two lines reconstruct the matrix using the first singular value only. You can already guess the rough shape of the original image.</p>
<div id="27b456b6" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>U, sigma, V <span class="op">=</span> np.linalg.svd(imgmat)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>reconstimg <span class="op">=</span> np.matrix(U[:, :<span class="dv">1</span>]) <span class="op">*</span> np.diag(sigma[:<span class="dv">1</span>]) <span class="op">*</span> np.matrix(V[:<span class="dv">1</span>, :])</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(reconstimg, cmap<span class="op">=</span><span class="st">'gray'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-14-output-1.png" width="575" height="339" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Letâ€™s see what we get when we use the second, third and fourth singular value as well.</p>
<div id="8b5b7e1e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">4</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    reconstimg <span class="op">=</span> np.matrix(U[:, :i]) <span class="op">*</span> np.diag(sigma[:i]) <span class="op">*</span> np.matrix(V[:i, :])</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    plt.imshow(reconstimg, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="st">"n = </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> i</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-15-output-1.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-15-output-2.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now we let <span class="math inline">\(i\)</span> run from 5 to 51, using a step width of 5. For <span class="math inline">\(i=50\)</span>, we already get a pretty good image!</p>
<div id="ad0ae37f" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">51</span>, <span class="dv">5</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    reconstimg <span class="op">=</span> np.matrix(U[:, :i]) <span class="op">*</span> np.diag(sigma[:i]) <span class="op">*</span> np.matrix(V[:i, :])</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    plt.imshow(reconstimg, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="st">"n = </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> i</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-1.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-2.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-3.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-4.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-5.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-6.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-7.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-8.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-9.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-16-output-10.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>But how many singular values do we have after all? The following command gives us the number of entries in sigma. As it is the diagonal matrix, it is stored as a vector and we do not save the zero entries. We now output the number of singular values (the length of the vector sigma, containing the diagonal entries), as well as the size of the matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>.</p>
<div id="5b755ea1" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"We have </span><span class="sc">%d</span><span class="st"> singular values."</span> <span class="op">%</span> sigma.shape)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"U is of size"</span>, U.shape, <span class="st">"."</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V is of size"</span>, V.shape, <span class="st">"."</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The last, or smallest entry in sigma is"</span>, sigma[<span class="dv">224</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>We have 225 singular values.
U is of size (225, 225) .
V is of size (400, 400) .
The last, or smallest entry in sigma is 9.637679189276597</code></pre>
</div>
</div>
<p>As Python stores the whole singular value decomposition, we do not really save space. But as you saw in the first theoretical exercise of the 10th series, we do not have to compute the whole matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> if we know that we only want to reconstruct the rank <span class="math inline">\(k\)</span> approximation. How many numbers do you have to store for the initial matrix of the picture? How many numbers do you have to store if you want to reconstruct the rank <span class="math inline">\(k\)</span> approximation only?</p>
<p>Use the following Cell to find an <span class="math inline">\(i\)</span> large enough that you are satisfied with the quality of the image. Check, how much percent of the initial size you have to store. If your picture has a different resolution, you will have to correct the terms.</p>
<div id="70e1eaae" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>reconstimg <span class="op">=</span> np.matrix(U[:, :i]) <span class="op">*</span> np.diag(sigma[:i]) <span class="op">*</span> np.matrix(V[:i, :])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(reconstimg, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"n = </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> i</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.title(title)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>numbers <span class="op">=</span> <span class="dv">400</span><span class="op">*</span>i <span class="op">+</span> i <span class="op">+</span> <span class="dv">225</span><span class="op">*</span> i</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"For this quality, we have to store </span><span class="sc">%d</span><span class="st"> numbers."</span> <span class="op">%</span> numbers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-18-output-1.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For this quality, we have to store 6260 numbers.</code></pre>
</div>
</div>
<p>If you really want to have a good quality, say you want to reconstruct using <span class="math inline">\(r - 1\)</span> singular values, where <span class="math inline">\(r\)</span> is the total number of singular values, is it still a good idea to use the singular value decomposition?</p>
<div id="c8bfb14e" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>reconstimg <span class="op">=</span> np.matrix(U[:, :i]) <span class="op">*</span> np.diag(sigma[:i]) <span class="op">*</span> np.matrix(V[:i, :])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(reconstimg, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"n = </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> i</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>plt.title(title)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-19-output-1.png" width="575" height="354" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="takeaway" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="takeaway"><span class="header-section-number">5.9</span> Takeaway</h2>
<p>Singular Value Decomposition provides a general framework for decomposing any matrix into orthogonal components, revealing the underlying structure of the matrix. SVD has numerous applications in machine learning, signal processing, and more. The method to find the matrices <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V^T\)</span> involves using the eigenvalues and eigenvectors of <span class="math inline">\(A^T A\)</span> and <span class="math inline">\(A A^T\)</span>.</p>
</section>
<section id="principal-component-analysis" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="principal-component-analysis"><span class="header-section-number">5.10</span> Principal Component Analysis</h2>
<p>Principal Component Analysis (PCA) is fundamentally a technique used for dimensionality reduction, feature extraction, and data visualization. While itâ€™s widely used in various fields, its mathematical foundation lies in Singular Value Decomposition (SVD). This connection between PCA and SVD helps in understanding the underlying mechanics and simplifies PCA computations in high-dimensional spaces. In this section, we will explore PCA as a special case of SVD, present the mathematical derivations, and conclude with an advanced illustration involving image compression.</p>
</section>
<section id="principal-component-analysis-as-a-special-case-of-svd" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="principal-component-analysis-as-a-special-case-of-svd"><span class="header-section-number">5.11</span> Principal Component Analysis as a Special Case of SVD</h2>
<p>Principal Component Analysis (PCA) is a widely used technique for dimensionality reduction, feature extraction, and data compression. It is based on the mathematical foundations of linear algebra, particularly eigenvalue decomposition and singular value decomposition (SVD).</p>
</section>
<section id="problem-setting-for-pca" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="problem-setting-for-pca"><span class="header-section-number">5.12</span> Problem Setting for PCA</h2>
<p>Let <span class="math inline">\(X \in \mathbb{R}^{m \times n}\)</span> represent the data matrix, where: - <span class="math inline">\(m\)</span> is the number of data samples, - <span class="math inline">\(n\)</span> is the number of features for each sample.</p>
<p>To begin, we center the data by subtracting the mean of each feature from the dataset:</p>
<p><span class="math display">\[
X_{\text{centered}} = X - \bar{X}
\]</span></p>
<p>where <span class="math inline">\(\bar{X}\)</span> is the mean vector for the columns of <span class="math inline">\(X\)</span>.</p>
</section>
<section id="covariance-matrix" class="level2" data-number="5.13">
<h2 data-number="5.13" class="anchored" data-anchor-id="covariance-matrix"><span class="header-section-number">5.13</span> Covariance Matrix</h2>
<p>The covariance matrix <span class="math inline">\(C\)</span> of the centered data <span class="math inline">\(X_{\text{centered}}\)</span> is given by:</p>
<p><span class="math display">\[
C = \frac{1}{m-1} X_{\text{centered}}^T X_{\text{centered}} \in \mathbb{R}^{n \times n}
\]</span></p>
<p>The covariance matrix captures the relationships between the features of the data. PCA identifies the directions (principal components) that correspond to the maximum variance in the data.</p>
</section>
<section id="eigenvalue-decomposition" class="level2" data-number="5.14">
<h2 data-number="5.14" class="anchored" data-anchor-id="eigenvalue-decomposition"><span class="header-section-number">5.14</span> Eigenvalue Decomposition</h2>
<p>PCA reduces to finding the eigenvalue decomposition of the covariance matrix <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
C v_i = \lambda_i v_i
\]</span></p>
<p>where: - <span class="math inline">\(v_i\)</span> is the <span class="math inline">\(i\)</span>-th eigenvector, - <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(i\)</span>-th eigenvalue associated with <span class="math inline">\(v_i\)</span>.</p>
<p>The eigenvectors <span class="math inline">\(v_i\)</span> represent the principal components, and the eigenvalues <span class="math inline">\(\lambda_i\)</span> represent the variance captured by each principal component.</p>
</section>
<section id="singular-value-decomposition-svd" class="level2" data-number="5.15">
<h2 data-number="5.15" class="anchored" data-anchor-id="singular-value-decomposition-svd"><span class="header-section-number">5.15</span> Singular Value Decomposition (SVD)</h2>
<p>PCA can be interpreted as a special case of Singular Value Decomposition (SVD). The SVD of the centered data matrix <span class="math inline">\(X_{\text{centered}}\)</span> is:</p>
<p><span class="math display">\[
X_{\text{centered}} = U \Sigma V^T
\]</span></p>
<p>where: - <span class="math inline">\(U \in \mathbb{R}^{m \times m}\)</span> contains the left singular vectors, - <span class="math inline">\(\Sigma \in \mathbb{R}^{m \times n}\)</span> is the diagonal matrix of singular values, - <span class="math inline">\(V \in \mathbb{R}^{n \times n}\)</span> contains the right singular vectors.</p>
<p>The right singular vectors <span class="math inline">\(V\)</span> are equivalent to the eigenvectors of the covariance matrix <span class="math inline">\(C\)</span>, and the singular values <span class="math inline">\(\sigma_i\)</span> are related to the eigenvalues <span class="math inline">\(\lambda_i\)</span> by:</p>
<p><span class="math display">\[
\lambda_i = \sigma_i^2
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
PCA Derivation Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>To summarize:</p>
<ol type="1">
<li>Center the data by subtracting the mean of each feature.</li>
<li>Compute the covariance matrix <span class="math inline">\(C = \frac{1}{m-1} X_{\text{centered}}^T X_{\text{centered}}\)</span>.</li>
<li>Perform eigenvalue decomposition of <span class="math inline">\(C\)</span> to find the eigenvectors and eigenvalues.</li>
<li>Alternatively, perform SVD of <span class="math inline">\(X_{\text{centered}}\)</span>. The right singular vectors are the principal components, and the singular values are related to the variance.</li>
</ol>
</div>
</div>
</section>
<section id="applications-of-pca" class="level2" data-number="5.16">
<h2 data-number="5.16" class="anchored" data-anchor-id="applications-of-pca"><span class="header-section-number">5.16</span> Applications of PCA</h2>
<p>PCA has several practical applications:</p>
<ul>
<li><strong>Dimensionality Reduction</strong>: PCA reduces the number of dimensions in a dataset while retaining the most important information.</li>
<li><strong>Image Compression</strong>: PCA can compress images by keeping only the principal components that capture the most variance.</li>
<li><strong>Feature Extraction</strong>: PCA is used in machine learning to extract the most significant features from high-dimensional datasets, speeding up computations and preventing overfitting.</li>
</ul>
</section>
<section id="image-compression-using-pca" class="level2" data-number="5.17">
<h2 data-number="5.17" class="anchored" data-anchor-id="image-compression-using-pca"><span class="header-section-number">5.17</span> Image Compression using PCA</h2>
<p>Consider an image represented as a matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, where each entry is a pixel intensity. To apply PCA for image compression:</p>
<ol type="1">
<li>Flatten the image into a matrix where each row represents a pixel.</li>
<li>Compute the covariance matrix of the image.</li>
<li>Perform eigenvalue decomposition or SVD on the covariance matrix.</li>
<li>Select the top <span class="math inline">\(k\)</span> eigenvectors and project the image onto these eigenvectors.</li>
<li>Reconstruct the image using the reduced data.</li>
</ol>
<section id="sample-problems" class="level3" data-number="5.17.1">
<h3 data-number="5.17.1" class="anchored" data-anchor-id="sample-problems"><span class="header-section-number">5.17.1</span> Sample problems</h3>
<p>Consider the following dataset:</p>
<p><span class="math display">\[
X = \begin{pmatrix}
2 &amp; 3 \\
3 &amp; 5 \\
5 &amp; 7
\end{pmatrix}
\]</span></p>
</section>
<section id="step-1-center-the-data" class="level3" data-number="5.17.2">
<h3 data-number="5.17.2" class="anchored" data-anchor-id="step-1-center-the-data"><span class="header-section-number">5.17.2</span> Step 1: Center the Data</h3>
<p>Calculate the mean of each feature (column):</p>
<p><span class="math display">\[
\text{Mean} = \begin{pmatrix}
\frac{2+3+5}{3} \\
\frac{3+5+7}{3}
\end{pmatrix} = \begin{pmatrix}
3.33 \\
5
\end{pmatrix}
\]</span></p>
<p>Subtract the mean from each data point to center the data:</p>
<p><span class="math display">\[
X_{\text{centered}} = X - \text{Mean} = \begin{pmatrix}
2 - 3.33 &amp; 3 - 5 \\
3 - 3.33 &amp; 5 - 5 \\
5 - 3.33 &amp; 7 - 5
\end{pmatrix} = \begin{pmatrix}
-1.33 &amp; -2 \\
-0.33 &amp; 0 \\
1.67 &amp; 2
\end{pmatrix}
\]</span></p>
</section>
<section id="step-2-calculate-the-covariance-matrix" class="level3" data-number="5.17.3">
<h3 data-number="5.17.3" class="anchored" data-anchor-id="step-2-calculate-the-covariance-matrix"><span class="header-section-number">5.17.3</span> Step 2: Calculate the Covariance Matrix</h3>
<p>Using the formula:</p>
<p><span class="math display">\[
C = \frac{1}{n-1} X_{\text{centered}}^T X_{\text{centered}}
\]</span></p>
<p>where <span class="math inline">\(n = 3\)</span> (the number of data points).</p>
<ol type="1">
<li><strong>Transpose of Centered Data:</strong></li>
</ol>
<p><span class="math display">\[
X_{\text{centered}}^T = \begin{pmatrix}
-1.33 &amp; -0.33 &amp; 1.67 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
\]</span></p>
<ol start="2" type="1">
<li><strong>Multiply <span class="math inline">\(X_{\text{centered}}^T\)</span> by <span class="math inline">\(X_{\text{centered}}\)</span>:</strong></li>
</ol>
<p><span class="math display">\[
X_{\text{centered}}^T X_{\text{centered}} = \begin{pmatrix}
(-1.33)(-1.33) + (-0.33)(-0.33) + (1.67)(1.67) &amp; (-1.33)(-2) + (-0.33)(0) + (1.67)(2) \\
(-2)(-1.33) + (0)(-0.33) + (2)(1.67) &amp; (-2)(-2) + (0)(0) + (2)(2)
\end{pmatrix}
\]</span></p>
<p>Calculating each entry:</p>
<ul>
<li><p>First entry: <span class="math display">\[
(-1.33)^2 + (-0.33)^2 + (1.67)^2 = 1.7689 + 0.1089 + 2.7889 = 4.6667
\]</span></p></li>
<li><p>Second entry: <span class="math display">\[
(-1.33)(-2) + (1.67)(2) = 2.66 + 3.34 = 6
\]</span></p></li>
</ul>
<p>Thus, <span class="math display">\[
X_{\text{centered}}^T X_{\text{centered}} = \begin{pmatrix} 4.67 &amp; 6 \\ 6 &amp; 8 \end{pmatrix}
\]</span></p>
<ol start="3" type="1">
<li><strong>Calculate the Covariance Matrix:</strong></li>
</ol>
<p><span class="math display">\[
C = \frac{1}{3-1} \begin{pmatrix} 4.67 &amp; 6 \\ 6 &amp; 8 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 4.67 &amp; 6 \\ 6 &amp; 8 \end{pmatrix} = \begin{pmatrix} 2.335 &amp; 3 \\ 3 &amp; 4 \end{pmatrix}
\]</span></p>
</section>
<section id="step-3-calculate-eigenvalues-and-eigenvectors" class="level3" data-number="5.17.4">
<h3 data-number="5.17.4" class="anchored" data-anchor-id="step-3-calculate-eigenvalues-and-eigenvectors"><span class="header-section-number">5.17.4</span> Step 3: Calculate Eigenvalues and Eigenvectors</h3>
<ol type="1">
<li><p><strong>Eigenvalue Equation:</strong></p>
<p>The characteristic polynomial is given by:</p>
<p><span class="math display">\[
\text{det}(C - \lambda I) = 0
\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the identity matrix. So:</p>
<p><span class="math display">\[
C - \lambda I = \begin{pmatrix} 2.335 - \lambda &amp; 3 \\ 3 &amp; 4 - \lambda \end{pmatrix}
\]</span></p>
<p>The determinant is:</p>
<p><span class="math display">\[
\text{det}(C - \lambda I) = (2.335 - \lambda)(4 - \lambda) - (3)(3)
\]</span></p>
<p>Expanding this, we get:</p>
<p><span class="math display">\[
\text{det}(C - \lambda I) = (2.335 \cdot 4 - 2.335 \lambda - 4 \lambda + \lambda^2 - 9) = \lambda^2 - (6.335)\lambda + (9.34 - 9) = \lambda^2 - 6.335\lambda + 0.34
\]</span></p></li>
<li><p><strong>Finding Eigenvalues:</strong></p>
<p>Solving the characteristic equation:</p>
<p><span class="math display">\[
\lambda^2 - 6.335\lambda + 0.34 = 0
\]</span></p>
<p>Using the quadratic formula:</p>
<p><span class="math display">\[
\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{6.335 \pm \sqrt{(-6.335)^2 - 4 \cdot 1 \cdot 0.34}}{2}
\]</span></p>
<p>Calculate <span class="math inline">\(b^2 - 4ac\)</span>:</p>
<p><span class="math display">\[
(-6.335)^2 - 4 \cdot 1 \cdot 0.34 \approx 40.096225 - 1.36 = 38.736225
\]</span></p>
<p>Thus, the eigenvalues are:</p>
<p><span class="math display">\[
\lambda_1 = \frac{6.335 + \sqrt{38.736225}}{2}, \quad \lambda_2 = \frac{6.335 - \sqrt{38.736225}}{2}
\]</span></p></li>
<li><p><strong>Finding Eigenvectors:</strong></p></li>
</ol>
<p>For each eigenvalue <span class="math inline">\(\lambda\)</span>, solve:</p>
<p><span class="math display">\[
(C - \lambda I) \mathbf{v} = 0
\]</span></p>
<p>Letâ€™s denote the eigenvalues we find as <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. For each <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>Substitute <span class="math inline">\(\lambda\)</span> into <span class="math inline">\((C - \lambda I)\)</span> and set up the equation to find the eigenvectors.</li>
</ul>
</section>
<section id="python-implementation" class="level3" data-number="5.17.5">
<h3 data-number="5.17.5" class="anchored" data-anchor-id="python-implementation"><span class="header-section-number">5.17.5</span> Python Implementation</h3>
<p>Hereâ€™s the Python code to perform PCA step by step, corresponding to the tasks outlined above:</p>
<div id="889094f0" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Given dataset</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">5</span>, <span class="dv">7</span>]])</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Center the data</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>X_centered <span class="op">=</span> X <span class="op">-</span> mean</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Calculate the covariance matrix</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="op">=</span> np.cov(X_centered, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Calculate eigenvalues and eigenvectors</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(cov_matrix)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the results</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean:</span><span class="ch">\n</span><span class="st">"</span>, mean)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Centered Data:</span><span class="ch">\n</span><span class="st">"</span>, X_centered)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Covariance Matrix:</span><span class="ch">\n</span><span class="st">"</span>, cov_matrix)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:</span><span class="ch">\n</span><span class="st">"</span>, eigenvalues)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors:</span><span class="ch">\n</span><span class="st">"</span>, eigenvectors)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Project data onto principal components</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> X_centered <span class="op">@</span> eigenvectors</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PCA Result (Projected Data):</span><span class="ch">\n</span><span class="st">"</span>, X_pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean:
 [3.33333333 5.        ]
Centered Data:
 [[-1.33333333 -2.        ]
 [-0.33333333  0.        ]
 [ 1.66666667  2.        ]]
Covariance Matrix:
 [[2.33333333 3.        ]
 [3.         4.        ]]
Eigenvalues:
 [0.05307638 6.28025695]
Eigenvectors:
 [[-0.79612934 -0.60512649]
 [ 0.60512649 -0.79612934]]
PCA Result (Projected Data):
 [[-0.14874719  2.39909401]
 [ 0.26537645  0.20170883]
 [-0.11662926 -2.60080284]]</code></pre>
</div>
</div>
</section>
</section>
<section id="principal-component-analysis-pca-for-image-reconstruction" class="level2" data-number="5.18">
<h2 data-number="5.18" class="anchored" data-anchor-id="principal-component-analysis-pca-for-image-reconstruction"><span class="header-section-number">5.18</span> Principal Component Analysis (PCA) for Image Reconstruction</h2>
<p>In this example, we will use PCA to reduce the dimensionality of an image and then reconstruct it using only a fraction of the principal components. We will demonstrate this with the famous Lena image.</p>
<p><strong>Step 1: Load and Display the Image</strong></p>
<p>First, letâ€™s load the image using the <code>PIL</code> library and display it.</p>
<div id="b9b9af9b" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image from URL</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(<span class="st">'http://lenna.org/len_top.jpg'</span>, <span class="st">"input.jpg"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Open and display the image</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"input.jpg"</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-21-output-1.png" width="540" height="332" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Step 2: Convert the Image to Grayscale</strong></p>
<p>To simplify the PCA process, we will work with the grayscale version of the image.</p>
<div id="35286f8b" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert image to grayscale</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>img_gray <span class="op">=</span> img.convert(<span class="st">'L'</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_gray, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Grayscale Image"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to NumPy array</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> np.array(img_gray)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Shape:"</span>, img_array.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-22-output-1.png" width="540" height="332" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Shape: (225, 400)</code></pre>
</div>
</div>
<p><strong>Step 3: Apply PCA to the Grayscale Image</strong></p>
<p>We will apply PCA to compress the image by reducing the number of principal components used for reconstruction.</p>
<div id="a452b79c" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the image into 2D (pixels x features)</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>img_flattened <span class="op">=</span> img_array <span class="op">/</span> <span class="fl">255.0</span>  <span class="co"># Normalize pixel values</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">50</span>)  <span class="co"># Choose 50 components</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit PCA on the image and transform</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>img_transformed <span class="op">=</span> pca.fit_transform(img_flattened)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the explained variance ratio</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Explained Variance Ratio:"</span>, np.<span class="bu">sum</span>(pca.explained_variance_ratio_))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Explained Variance Ratio: 0.9480357612223579</code></pre>
</div>
</div>
<p><strong>Step 4: Reconstruct the Image using the Reduced Components</strong></p>
<p>Now we use the transformed PCA components to reconstruct the image and compare it with the original.</p>
<div id="b386a078" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconstruct the image from PCA components</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>img_reconstructed <span class="op">=</span> pca.inverse_transform(img_transformed)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Rescale the image back to original pixel values (0-255)</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>img_reconstructed <span class="op">=</span> (img_reconstructed <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the reconstructed image</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_reconstructed, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Reconstructed Image with 50 Components"</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-24-output-1.png" width="540" height="332" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Step 5: Compare the Original and Reconstructed Images</strong></p>
<p>Finally, letâ€™s compare the original grayscale image with the PCA-reconstructed image to see how well it retains essential details.</p>
<div id="179a7236" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot original and reconstructed side by side</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Original Grayscale Image</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(img_array, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Original Grayscale Image"</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconstructed Image</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(img_reconstructed, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Reconstructed Image (50 Components)"</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="module_5_files/figure-html/cell-25-output-1.png" width="763" height="229" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="micro-projects" class="level2" data-number="5.19">
<h2 data-number="5.19" class="anchored" data-anchor-id="micro-projects"><span class="header-section-number">5.19</span> Micro Projects</h2>
<ol type="1">
<li>Linear Regression</li>
</ol>
<p>Link to GitHub</p>
<p><a href="https://github.com/sijuswamy/AIML_Files" class="uri">https://github.com/sijuswamy/AIML_Files</a></p>
</section>
<section id="sample-questions-for-lab-work" class="level2" data-number="5.20">
<h2 data-number="5.20" class="anchored" data-anchor-id="sample-questions-for-lab-work"><span class="header-section-number">5.20</span> Sample questions for Lab Work</h2>
<p>This lab exam covers Spectral Decomposition, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). For each question, perform the necessary computations and provide your answers.</p>
<hr>
<section id="question-1-spectral-decomposition-of-a-symmetric-matrix" class="level3" data-number="5.20.1">
<h3 data-number="5.20.1" class="anchored" data-anchor-id="question-1-spectral-decomposition-of-a-symmetric-matrix"><span class="header-section-number">5.20.1</span> Question 1: Spectral Decomposition of a Symmetric Matrix</h3>
<p>For the matrix <span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 6 \\ 3 &amp; 5 &amp; 6 \\ 4 &amp; 6 &amp; 9 \end{bmatrix}
\]</span> find its spectral decomposition. Calculate the eigenvalues, eigenvectors, and express <span class="math inline">\(A\)</span> as <span class="math inline">\(PDP^{-1}\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix of eigenvalues and <span class="math inline">\(P\)</span> contains the eigenvectors.</p>
<hr>
</section>
<section id="question-2-diagonalization-of-a-matrix" class="level3" data-number="5.20.2">
<h3 data-number="5.20.2" class="anchored" data-anchor-id="question-2-diagonalization-of-a-matrix"><span class="header-section-number">5.20.2</span> Question 2: Diagonalization of a Matrix</h3>
<p>Given <span class="math display">\[
B = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}
\]</span> find the eigenvalues and eigenvectors, and use them to express <span class="math inline">\(B\)</span> in its spectral decomposition form.</p>
<hr>
</section>
<section id="question-3-eigenvalues-and-eigenvectors-of-a-square-matrix" class="level3" data-number="5.20.3">
<h3 data-number="5.20.3" class="anchored" data-anchor-id="question-3-eigenvalues-and-eigenvectors-of-a-square-matrix"><span class="header-section-number">5.20.3</span> Question 3: Eigenvalues and Eigenvectors of a Square Matrix</h3>
<p>For the matrix <span class="math display">\[
C = \begin{bmatrix} 4 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 1 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}
\]</span> find the eigenvalues and eigenvectors. Verify that the matrix can be reconstructed from its eigenvalues and eigenvectors.</p>
<hr>
</section>
<section id="question-4-orthogonal-matrix-decomposition" class="level3" data-number="5.20.4">
<h3 data-number="5.20.4" class="anchored" data-anchor-id="question-4-orthogonal-matrix-decomposition"><span class="header-section-number">5.20.4</span> Question 4: Orthogonal Matrix Decomposition</h3>
<p>For the matrix <span class="math display">\[
D = \begin{bmatrix} 5 &amp; 4 \\ 4 &amp; 5 \end{bmatrix}
\]</span> find its spectral decomposition and confirm whether the eigenvectors form an orthogonal matrix.</p>
<hr>
</section>
<section id="question-5-singular-value-decomposition-svd" class="level3" data-number="5.20.5">
<h3 data-number="5.20.5" class="anchored" data-anchor-id="question-5-singular-value-decomposition-svd"><span class="header-section-number">5.20.5</span> Question 5: Singular Value Decomposition (SVD)</h3>
<p>For the matrix <span class="math display">\[
E = \begin{bmatrix} 3 &amp; 1 \\ -1 &amp; 3 \end{bmatrix}
\]</span> compute its Singular Value Decomposition. Write the matrix as <span class="math inline">\(U \Sigma V^T\)</span>, where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matrices and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with singular values.</p>
<hr>
</section>
<section id="question-6-rank-1-approximation-using-svd" class="level3" data-number="5.20.6">
<h3 data-number="5.20.6" class="anchored" data-anchor-id="question-6-rank-1-approximation-using-svd"><span class="header-section-number">5.20.6</span> Question 6: Rank-1 Approximation Using SVD</h3>
<p>Using the matrix <span class="math display">\[
F = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}
\]</span> perform a rank-1 approximation. Calculate the Frobenius norm of the difference between <span class="math inline">\(F\)</span> and its rank-1 approximation.</p>
<hr>
</section>
<section id="question-7-matrix-compression-using-svd" class="level3" data-number="5.20.7">
<h3 data-number="5.20.7" class="anchored" data-anchor-id="question-7-matrix-compression-using-svd"><span class="header-section-number">5.20.7</span> Question 7: Matrix Compression Using SVD</h3>
<p>For the matrix <span class="math display">\[
G = \begin{bmatrix} 10 &amp; 20 &amp; 30 \\ 20 &amp; 30 &amp; 40 \\ 30 &amp; 40 &amp; 50 \end{bmatrix}
\]</span> find a compressed form of <span class="math inline">\(G\)</span> using only the most significant singular value. Provide the resulting approximation.</p>
<hr>
</section>
<section id="question-8-svd-data-reconstruction" class="level3" data-number="5.20.8">
<h3 data-number="5.20.8" class="anchored" data-anchor-id="question-8-svd-data-reconstruction"><span class="header-section-number">5.20.8</span> Question 8: SVD Data Reconstruction</h3>
<p>For the matrix <span class="math display">\[
H = \begin{bmatrix} 4 &amp; 11 \\ 14 &amp; 8 \\ 1 &amp; 5 \end{bmatrix}
\]</span> find its Singular Value Decomposition. Reconstruct the matrix from the SVD components.</p>
<hr>
</section>
<section id="question-9-principal-component-analysis-pca" class="level3" data-number="5.20.9">
<h3 data-number="5.20.9" class="anchored" data-anchor-id="question-9-principal-component-analysis-pca"><span class="header-section-number">5.20.9</span> Question 9: Principal Component Analysis (PCA)</h3>
<p>Consider the dataset <span class="math display">\[
X = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix}
\]</span> Perform Principal Component Analysis (PCA) by first centering the data and then computing the principal components.</p>
<hr>
</section>
<section id="question-10-dimensionality-reduction-with-pca" class="level3" data-number="5.20.10">
<h3 data-number="5.20.10" class="anchored" data-anchor-id="question-10-dimensionality-reduction-with-pca"><span class="header-section-number">5.20.10</span> Question 10: Dimensionality Reduction with PCA</h3>
<p>Using the dataset <span class="math display">\[
Y = \begin{bmatrix} 2 &amp; 4 \\ 3 &amp; 8 \\ 5 &amp; 7 \\ 6 &amp; 2 \\ 7 &amp; 5 \end{bmatrix}
\]</span> apply PCA to reduce the data to one dimension. Project the data onto the principal component found.</p>
<hr>
</section>
</section>
<section id="hint-to-solutions." class="level2" data-number="5.21">
<h2 data-number="5.21" class="anchored" data-anchor-id="hint-to-solutions."><span class="header-section-number">5.21</span> Hint to Solutions.</h2>
<div id="595db06c" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.linalg</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Spectral Decomposition for Matrix A</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">6</span>], [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">9</span>]])</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(eigenvalues)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> eigenvectors</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>P_inv <span class="op">=</span> np.linalg.inv(P)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>reconstructed_A <span class="op">=</span> P <span class="op">@</span> D <span class="op">@</span> P_inv</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:</span><span class="ch">\n</span><span class="st">"</span>, eigenvalues)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors:</span><span class="ch">\n</span><span class="st">"</span>, eigenvectors)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reconstructed A:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_A)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Diagonalization of Matrix B</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>eigenvalues_B, eigenvectors_B <span class="op">=</span> np.linalg.eig(B)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>D_B <span class="op">=</span> np.diag(eigenvalues_B)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>P_B <span class="op">=</span> eigenvectors_B</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>P_inv_B <span class="op">=</span> np.linalg.inv(P_B)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>reconstructed_B <span class="op">=</span> P_B <span class="op">@</span> D_B <span class="op">@</span> P_inv_B</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Eigenvalues of B:</span><span class="ch">\n</span><span class="st">"</span>, eigenvalues_B)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors of B:</span><span class="ch">\n</span><span class="st">"</span>, eigenvectors_B)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reconstructed B:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_B)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Eigenvalues and Eigenvectors for Matrix C</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.array([[<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>eigenvalues_C, eigenvectors_C <span class="op">=</span> np.linalg.eig(C)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>D_C <span class="op">=</span> np.diag(eigenvalues_C)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>P_C <span class="op">=</span> eigenvectors_C</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>reconstructed_C <span class="op">=</span> P_C <span class="op">@</span> D_C <span class="op">@</span> np.linalg.inv(P_C)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Eigenvalues of C:</span><span class="ch">\n</span><span class="st">"</span>, eigenvalues_C)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors of C:</span><span class="ch">\n</span><span class="st">"</span>, eigenvectors_C)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reconstructed C:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_C)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Orthogonal Matrix Decomposition of D</span></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.array([[<span class="dv">5</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>eigenvalues_D, eigenvectors_D <span class="op">=</span> np.linalg.eig(D)</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>P_D <span class="op">=</span> eigenvectors_D</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>reconstructed_D <span class="op">=</span> P_D <span class="op">@</span> np.diag(eigenvalues_D) <span class="op">@</span> P_D.T</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Eigenvalues of D:</span><span class="ch">\n</span><span class="st">"</span>, eigenvalues_D)</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors of D (orthogonal):</span><span class="ch">\n</span><span class="st">"</span>, P_D)</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reconstructed D:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_D)</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. SVD of Matrix E</span></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(E)</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.zeros((U.shape[<span class="dv">0</span>], Vt.shape[<span class="dv">0</span>]))</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>Sigma[:<span class="bu">len</span>(S), :<span class="bu">len</span>(S)] <span class="op">=</span> np.diag(S)</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>reconstructed_E <span class="op">=</span> U <span class="op">@</span> Sigma <span class="op">@</span> Vt</span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">U:</span><span class="ch">\n</span><span class="st">"</span>, U)</span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sigma:</span><span class="ch">\n</span><span class="st">"</span>, Sigma)</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V^T:</span><span class="ch">\n</span><span class="st">"</span>, Vt)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reconstructed E:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_E)</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Rank-1 Approximation Using SVD for Matrix F</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a>F <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>]])</span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>U_F, S_F, Vt_F <span class="op">=</span> np.linalg.svd(F)</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>rank_1_F <span class="op">=</span> np.outer(U_F[:, <span class="dv">0</span>] <span class="op">*</span> S_F[<span class="dv">0</span>], Vt_F[<span class="dv">0</span>, :])</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a>frobenius_norm <span class="op">=</span> np.linalg.norm(F <span class="op">-</span> rank_1_F, <span class="st">'fro'</span>)</span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Rank-1 Approximation of F:</span><span class="ch">\n</span><span class="st">"</span>, rank_1_F)</span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frobenius Norm of Difference:</span><span class="ch">\n</span><span class="st">"</span>, frobenius_norm)</span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. SVD for Image Compression (Matrix G)</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> np.array([[<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>], [<span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>], [<span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>]])</span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a>U_G, S_G, Vt_G <span class="op">=</span> np.linalg.svd(G)</span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a>compressed_G <span class="op">=</span> np.outer(U_G[:, <span class="dv">0</span>] <span class="op">*</span> S_G[<span class="dv">0</span>], Vt_G[<span class="dv">0</span>, :])</span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Compressed Matrix G:</span><span class="ch">\n</span><span class="st">"</span>, compressed_G)</span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. SVD Data Reconstruction for Matrix H</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.array([[<span class="dv">4</span>, <span class="dv">11</span>], [<span class="dv">14</span>, <span class="dv">8</span>], [<span class="dv">1</span>, <span class="dv">5</span>]])</span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>U_H, S_H, Vt_H <span class="op">=</span> np.linalg.svd(H)</span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a>Sigma_H <span class="op">=</span> np.zeros((U_H.shape[<span class="dv">0</span>], Vt_H.shape[<span class="dv">0</span>]))</span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>Sigma_H[:<span class="bu">len</span>(S_H), :<span class="bu">len</span>(S_H)] <span class="op">=</span> np.diag(S_H)</span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a>reconstructed_H <span class="op">=</span> U_H <span class="op">@</span> Sigma_H <span class="op">@</span> Vt_H</span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reconstructed H:</span><span class="ch">\n</span><span class="st">"</span>, reconstructed_H)</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a><span class="co"># 9. PCA on Small Dataset X</span></span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]])</span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>X_centered <span class="op">=</span> X <span class="op">-</span> np.mean(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a>U_X, S_X, Vt_X <span class="op">=</span> np.linalg.svd(X_centered)</span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Centered Data X:</span><span class="ch">\n</span><span class="st">"</span>, X_centered)</span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Principal Components of X:</span><span class="ch">\n</span><span class="st">"</span>, Vt_X.T)</span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a><span class="co"># 10. Dimensionality Reduction with PCA for Dataset Y</span></span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>], [<span class="dv">3</span>, <span class="dv">8</span>], [<span class="dv">5</span>, <span class="dv">7</span>], [<span class="dv">6</span>, <span class="dv">2</span>], [<span class="dv">7</span>, <span class="dv">5</span>]])</span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a>Y_centered <span class="op">=</span> Y <span class="op">-</span> np.mean(Y, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>U_Y, S_Y, Vt_Y <span class="op">=</span> np.linalg.svd(Y_centered)</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a>Y_projected <span class="op">=</span> Y_centered <span class="op">@</span> Vt_Y.T[:, :<span class="dv">1</span>]</span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">1D Projection of Y:</span><span class="ch">\n</span><span class="st">"</span>, Y_projected)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalues:
 [15.41619849 -1.          0.58380151]
Eigenvectors:
 [[-0.38597937 -0.94280904  0.67025624]
 [-0.54141317  0.23570226 -0.71674952]
 [-0.74692149  0.23570226  0.19242323]]
Reconstructed A:
 [[1. 2. 6.]
 [3. 5. 6.]
 [4. 6. 9.]]

Eigenvalues of B:
 [3. 1.]
Eigenvectors of B:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]
Reconstructed B:
 [[2. 1.]
 [1. 2.]]

Eigenvalues of C:
 [1.38196601 3.61803399 4.        ]
Eigenvectors of C:
 [[ 0.          0.          1.        ]
 [ 0.52573111 -0.85065081  0.        ]
 [-0.85065081 -0.52573111  0.        ]]
Reconstructed C:
 [[4. 0. 0.]
 [0. 3. 1.]
 [0. 1. 2.]]

Eigenvalues of D:
 [9. 1.]
Eigenvectors of D (orthogonal):
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]
Reconstructed D:
 [[5. 4.]
 [4. 5.]]

U:
 [[-0.9486833   0.31622777]
 [ 0.31622777  0.9486833 ]]
Sigma:
 [[3.16227766 0.        ]
 [0.         3.16227766]]
V^T:
 [[-1. -0.]
 [ 0.  1.]]
Reconstructed E:
 [[ 3.  1.]
 [-1.  3.]]

Rank-1 Approximation of F:
 [[1.73621779 2.07174246 2.40726714]
 [4.2071528  5.02018649 5.83322018]
 [6.6780878  7.96863051 9.25917322]]
Frobenius Norm of Difference:
 1.068369514554709

Compressed Matrix G:
 [[14.27105069 20.7349008  27.19875091]
 [20.7349008  30.12645113 39.51800146]
 [27.19875091 39.51800146 51.83725201]]

Reconstructed H:
 [[ 4. 11.]
 [14.  8.]
 [ 1.  5.]]

Centered Data X:
 [[-3. -3.]
 [-1. -1.]
 [ 1.  1.]
 [ 3.  3.]]
Principal Components of X:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]

1D Projection of Y:
 [[-0.34611992]
 [-3.22299204]
 [-1.32087901]
 [ 3.45810614]
 [ 1.43188483]]</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./module_4.html" class="pagination-link" aria-label="Linear Algebra for Advanced Applications">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra for Advanced Applications</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>