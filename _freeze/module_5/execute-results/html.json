{
  "hash": "99aa41f77b0f90243e759a32d0e2984a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Algebra for Advanced Applications\"\nexecute: \n  enabled: true\njupyter: python3\n---\n\n\n\n\n\n\n\n\n## Singular Value Decomposition (SVD) â€“ An Intuitive and Mathematical Approach\n\nSingular Value Decomposition (SVD) is one of the most powerful matrix factorization tools in linear algebra, extensively used in areas like data compression, signal processing, machine learning, and more. SVD generalizes the concept of diagonalization to non-square matrices, decomposing any $m \\times n$ matrix $A$ into three matrices with well-defined geometric interpretations.\n\n## The SVD Theorem\n\nFor any real or complex $m \\times n$ matrix $A$, SVD states that:\n\n$$\nA = U \\Sigma V^T\n$$\n\nWhere:\n- $U$ is an $m \\times m$ orthogonal matrix (or unitary in the complex case),\n- $\\Sigma$ is an $m \\times n$ diagonal matrix, with non-negative real numbers (the singular values of $A$) on the diagonal,\n- $V^T$ is the transpose (or conjugate transpose in the complex case) of an $n \\times n$ orthogonal matrix $V$.\n\nThese are range and null spaces for both the column and the row spaces.\n\n\\begin{align}\n%\n  \\mathbf{C}^{n} = \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} \\oplus\n    \\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} \\\\\n%\n  \\mathbf{C}^{m} = \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} \\oplus\n    \\color{red} {\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)}\n%\n\\end{align}\n\nThe singular value decomposition provides an orthonormal basis for the four fundamental subspaces.\n\n:::{.callout-note}\n\n### Singular Value Decomposition- A matrix visualization\n\n\\begin{align}\n  \\mathbf{A} &=\n  \\mathbf{U} \\, \\Sigma \\, \\mathbf{V}^{*} \\\\\n%\n &=\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right]  \n% Sigma\n  \\left[ \\begin{array}{cccc|cc}\n     \\sigma_{1} & 0 & \\dots &  &   & \\dots &  0 \\\\\n     0 & \\sigma_{2}  \\\\\n     \\vdots && \\ddots \\\\\n       & & & \\sigma_{\\rho} \\\\\\hline\n       & & & & 0 & \\\\\n     \\vdots &&&&&\\ddots \\\\\n     0 & & &   &   &  & 0 \\\\\n  \\end{array} \\right]\n% V \n  \\left[ \\begin{array}{c}\n     \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{*} \\\\ \n     \\color{red}{\\mathbf{V}_{\\mathcal{N}}}^{*}\n  \\end{array} \\right]  \\\\\n%\n  & =\n% U\n   \\left[ \\begin{array}{cccccccc}\n    \\color{blue}{u_{1}} & \\dots & \\color{blue}{u_{\\rho}} & \\color{red}{u_{\\rho+1}} & \\dots & \\color{red}{u_{m}}\n  \\end{array} \\right]\n% Sigma\n  \\left[ \\begin{array}{cc}\n     \\mathbf{S}_{\\rho\\times \\rho} & \\mathbf{0} \\\\\n     \\mathbf{0} & \\mathbf{0} \n  \\end{array} \\right]\n% V\n   \\left[ \\begin{array}{c}\n    \\color{blue}{v_{1}^{*}} \\\\ \n    \\vdots \\\\\n    \\color{blue}{v_{\\rho}^{*}} \\\\\n    \\color{red}{v_{\\rho+1}^{*}} \\\\\n    \\vdots \\\\ \n    \\color{red}{v_{n}^{*}}\n  \\end{array} \\right]\n%\n\\end{align}\n\nThe column vectors of $U$  are an orthonormal span of $\\mathbb{C}^{m}$ (column space), while the column vectors of $V$  are an orthonormal span of $\\mathbb{C}^{n}$ (row space).\n\nThe $\\rho$ singular values are real and ordered (descending):\n\n$$\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{\\rho}>0.$$\n\nThese singular values for the diagonal matrix of singular values\n\n$$\\mathbf{S} = \\text{diagonal} (\\sigma_{1},\\sigma_{1},\\dots,\\sigma_{\\rho}) \\in\\mathbb{R}^{\\rho\\times\\rho}.$$\n\nThe $S$  matrix is embedded in the sabot matrix $\\Sigma\\in\\mathbb{R}^{m\\times n}$ whose shape insures conformability.\n\nPlease note that the singular values _only_ correspond to _range space_ vectors.\n\nThe column vectors form spans for the subspaces:\n\\begin{align} \n% R A\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{u_{1}}, \\dots , \\color{blue}{u_{\\rho}}\n\\right\\} \\\\\n% R A*\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{v_{1}}, \\dots , \\color{blue}{v_{\\rho}}\n\\right\\} \\\\\n% N A*\n\\color{red}{\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{u_{\\rho+1}}, \\dots , \\color{red}{u_{m}}\n\\right\\} \\\\\n% N A\n\\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{v_{\\rho+1}}, \\dots , \\color{red}{v_{n}}\n\\right\\} \\\\\n%\n\\end{align}\n\n:::\n\n## Intuition Behind SVD\n\nThe SVD can be understood geometrically:\n- The columns of $V$ form an orthonormal basis of the input space.\n- The matrix $\\Sigma$ scales and transforms this space along the principal axes.\n- The columns of $U$ form an orthonormal basis of the output space, representing how the transformed vectors in the input space map to the output space.\n\nSVD essentially performs three steps on any vector $x$:\n1. **Rotation**: $V^T$ aligns $x$ with the principal axes.\n\n2. **Scaling**: $\\Sigma$ scales along these axes.\n\n3. **Rotation**: $U$ maps the result back to the output space.\n\n## Spectral Decomposition vs. SVD\n\n- **Spectral Decomposition** (also known as **Eigendecomposition**) applies to **square** matrices and decomposes a matrix $A$ into $A = Q \\Lambda Q^{-1}$, where $Q$ is an orthogonal matrix of eigenvectors, and $\\Lambda$ is a diagonal matrix of eigenvalues.\n- **SVD**, on the other hand, applies to **any** matrix (square or rectangular) and generalizes this idea by using singular values (which are always non-negative) instead of eigenvalues.\n\n### Comparison:\n\n- **Eigenvectors and Eigenvalues**: Spectral decomposition only works if $A$ is square and diagonalizable. It gives insight into the properties of a matrix (e.g., whether it is invertible).\n- **Singular Vectors and Singular Values**: SVD works for any matrix and provides a more general and stable decomposition, useful even for non-square matrices.\n\n## Steps to Find $U$, $\\Sigma$, and $V^T$\n\nGiven a matrix $A$, the SVD factors $U$, $\\Sigma$, and $V^T$ can be computed as follows:\n\n1. **Compute $A^T A$ and find the eigenvalues and eigenvectors:**\n\n   - The matrix $V$ is formed from the eigenvectors of $A^T A$.\n\n   - The singular values $\\sigma_i$ are the square roots of the eigenvalues of $A^T A$.\n   \n2. **Construct $\\Sigma$:**\n\n   - $\\Sigma$ is a diagonal matrix where the non-zero entries are the singular values $\\sigma_1, \\sigma_2, \\dots$, arranged in decreasing order.\n   \n3. **Compute $A A^T$ and find the eigenvectors:**\n\n   - The matrix $U$ is formed from the eigenvectors of $A A^T$.\n   \n4. **Transpose $V$:**\n\n   - The matrix $V^T$ is simply the transpose of $V$.\n\n## Example\n\nLet's consider a simple example where $A$ is a $2 \\times 2$ matrix:\n\n$$\nA = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\n\n### Step 1: Compute $A^T A$\n\n$$\nA^T A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n$$\n\nFind the eigenvalues of $A^T A$:\n\n$$\n\\det(A^T A - \\lambda I) = \\det\\begin{pmatrix} 10 - \\lambda & 6 \\\\ 6 & 10 - \\lambda \\end{pmatrix} = 0\n$$\n\n$$\n(10 - \\lambda)^2 - 36 = 0 \\quad \\Rightarrow \\quad \\lambda = 16, \\lambda = 4\n$$\n\nThe eigenvalues of $A^T A$ are $16$ and $4$, so the singular values of $A$ are $\\sigma_1 = 4$ and $\\sigma_2 = 2$.\n\n### Step 2: Find $V$ from the eigenvectors of $A^T A$\n\nSolve $(A^T A - \\lambda I)v = 0$ for each eigenvalue:\n\n- For $\\lambda = 16$, the eigenvector is $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- For $\\lambda = 4$, the eigenvector is $v_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n\nThus, \n\n$$\nV = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n### Step 3: Construct $\\Sigma$\n\nThe singular values $\\sigma_1 = 4$ and $\\sigma_2 = 2$, so:\n\n$$\n\\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\n\n### Step 4: Find $U$ from the eigenvectors of $A A^T$\n\nSimilarly, compute $A A^T$:\n\n$$\nA A^T = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n$$\n\nSolve for the eigenvectors of $A A^T$ (same as $A^T A$):\n\nThe eigenvectors are $u_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$ and $u_2 = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$.\n\nThus,\n\n$$\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n### Step 5: Final SVD\n\nWe can now write the SVD of $A$ as:\n\n$$\nA = U \\Sigma V^T\n$$\n\nWhere:\n\n$$\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad V^T = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n`Python` code to find SVD of this example is given below.\n\n::: {#be271cf9 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the matrix A\nA = np.array([[3, 1],\n              [1, 3]])\n\n# Perform SVD decomposition\nU, Sigma, VT = np.linalg.svd(A)\n\n# Create Sigma matrix from singular values\nSigma_matrix = np.zeros((A.shape[0], A.shape[1]))\nnp.fill_diagonal(Sigma_matrix, Sigma)\n\n# Print results\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nU matrix:\")\nprint(U)\nprint(\"\\nSigma matrix:\")\nprint(Sigma_matrix)\nprint(\"\\nV^T matrix:\")\nprint(VT)\n\n# Verify the decomposition A = U * Sigma * V^T\nA_reconstructed = U @ Sigma_matrix @ VT\nprint(\"\\nReconstructed A (U * Sigma * V^T):\")\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[3 1]\n [1 3]]\n\nU matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nSigma matrix:\n[[4. 0.]\n [0. 2.]]\n\nV^T matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nReconstructed A (U * Sigma * V^T):\n[[3. 1.]\n [1. 3.]]\n```\n:::\n:::\n\n\n## Reconstructing Matrix $A$ Using SVD\n\nGiven the Singular Value Decomposition (SVD) of a matrix $A$, the matrix can be reconstructed as a linear combination of low-rank matrices using the left singular vectors $u_i$, singular values $\\sigma_i$, and the right singular vectors $v_i^T$.\n\nThe formula to reconstruct the matrix $A$ is:\n\n$$\nA = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n$$\n\nwhere:\n- $r$ is the rank of the matrix $A$ (i.e., the number of non-zero singular values),\n- $\\sigma_i$ is the $i$-th singular value from the diagonal matrix $\\Sigma$,\n- $u_i$ is the $i$-th column of the matrix $U$ (left singular vectors),\n- $v_i^T$ is the transpose of the $i$-th row of the matrix $V^T$ (right singular vectors).\n\n### Breakdown of Terms:\n- $u_i \\in \\mathbb{R}^m$ is a column vector from the matrix $U$ (size $m \\times 1$),\n- $v_i^T \\in \\mathbb{R}^n$ is a row vector from the matrix $V^T$ (size $1 \\times n$),\n- $\\sigma_i \\in \\mathbb{R}$ is a scalar representing the $i$-th singular value.\n\nEach term $\\sigma_i u_i v_i^T$ represents a **rank-1 matrix** (the outer product of two vectors). The sum of these rank-1 matrices reconstructs the original matrix $A$.\n\n### Example:\n\nFor a matrix $A$, its SVD is represented as:\n\n$$\nA = U \\Sigma V^T = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n$$\n\nIf the rank of $A$ is 2, then the reconstructed form of $A$ would be:\n\n$$\nA = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T\n$$\n\nEach term $\\sigma_i u_i v_i^T$ corresponds to a **low-rank approximation** that contributes to the final matrix. By summing these terms, the full matrix $A$ is obtained.\n\nPython code demonstrating reconstruction is given below:\n\n::: {#d3ca7e99 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the matrix A and convert it to float64\nA = np.array([[3, 1], \n              [1, 3]], dtype=np.float64)\n\n# Perform SVD\nU, Sigma, VT = np.linalg.svd(A)\n\n# Reconstruct A using the singular values and singular vectors\nA_reconstructed = np.zeros_like(A)  # This will be float64 now\nfor i in range(len(Sigma)-1):\n    A_reconstructed += Sigma[i] * np.outer(U[:, i], VT[i, :])\n\nprint(\"Original matrix A:\")\nprint(A)\n\nprint(\"\\nReconstructed A from rank-1 matrices:\")\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal matrix A:\n[[3. 1.]\n [1. 3.]]\n\nReconstructed A from rank-1 matrices:\n[[2. 2.]\n [2. 2.]]\n```\n:::\n:::\n\n\n## Takeaway\n\nSingular Value Decomposition provides a general framework for decomposing any matrix into orthogonal components, revealing the underlying structure of the matrix. SVD has numerous applications in machine learning, signal processing, and more. The method to find the matrices $U$, $\\Sigma$, and $V^T$ involves using the eigenvalues and eigenvectors of $A^T A$ and $A A^T$.\n\n",
    "supporting": [
      "module_5_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}