{
  "hash": "e1e48f6ce36f4083cdae04b44ff601b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Algebra for Advanced Applications\"\nexecute: \n  enabled: true\njupyter: python3\n---\n\n\n\n\n\n\n\n\n\n\n\n## Singular Value Decomposition (SVD) â€“ An Intuitive and Mathematical Approach\n\nSingular Value Decomposition (SVD) is one of the most powerful matrix factorization tools in linear algebra, extensively used in areas like data compression, signal processing, machine learning, and more. SVD generalizes the concept of diagonalization to non-square matrices, decomposing any $m \\times n$ matrix $A$ into three matrices with well-defined geometric interpretations.\n\n## The SVD Theorem\n\nFor any real or complex $m \\times n$ matrix $A$, SVD states that:\n\n$$\nA = U \\Sigma V^T\n$$\n\nWhere:\n- $U$ is an $m \\times m$ orthogonal matrix (or unitary in the complex case),\n- $\\Sigma$ is an $m \\times n$ diagonal matrix, with non-negative real numbers (the singular values of $A$) on the diagonal,\n- $V^T$ is the transpose (or conjugate transpose in the complex case) of an $n \\times n$ orthogonal matrix $V$.\n\nThese are range and null spaces for both the column and the row spaces.\n\n\\begin{align}\n%\n  \\mathbf{C}^{n} &= \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} \\oplus\n    \\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} \\\\\n%\n  \\mathbf{C}^{m} &= \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} \\oplus\n    \\color{red} {\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)}\n%\n\\end{align}\n\nThe singular value decomposition provides an orthonormal basis for the four fundamental subspaces.\n\n\n\n## Intuition Behind SVD\n\nThe SVD can be understood geometrically:\n- The columns of $V$ form an orthonormal basis of the input space.\n- The matrix $\\Sigma$ scales and transforms this space along the principal axes.\n- The columns of $U$ form an orthonormal basis of the output space, representing how the transformed vectors in the input space map to the output space.\n\nSVD essentially performs three steps on any vector $x$:\n1. **Rotation**: $V^T$ aligns $x$ with the principal axes.\n\n2. **Scaling**: $\\Sigma$ scales along these axes.\n\n3. **Rotation**: $U$ maps the result back to the output space.\n\n## Spectral Decomposition vs. SVD\n\n- **Spectral Decomposition** (also known as **Eigendecomposition**) applies to **square** matrices and decomposes a matrix $A$ into $A = Q \\Lambda Q^{-1}$, where $Q$ is an orthogonal matrix of eigenvectors, and $\\Lambda$ is a diagonal matrix of eigenvalues.\n- **SVD**, on the other hand, applies to **any** matrix (square or rectangular) and generalizes this idea by using singular values (which are always non-negative) instead of eigenvalues.\n\n### Comparison:\n\n- **Eigenvectors and Eigenvalues**: Spectral decomposition only works if $A$ is square and diagonalizable. It gives insight into the properties of a matrix (e.g., whether it is invertible).\n- **Singular Vectors and Singular Values**: SVD works for any matrix and provides a more general and stable decomposition, useful even for non-square matrices.\n\n## Steps to Find $U$, $\\Sigma$, and $V^T$\n\nGiven a matrix $A$, the SVD factors $U$, $\\Sigma$, and $V^T$ can be computed as follows:\n\n1. **Compute $A^T A$ and find the eigenvalues and eigenvectors:**\n\n   - The matrix $V$ is formed from the eigenvectors of $A^T A$.\n\n   - The singular values $\\sigma_i$ are the square roots of the eigenvalues of $A^T A$.\n   \n2. **Construct $\\Sigma$:**\n\n   - $\\Sigma$ is a diagonal matrix where the non-zero entries are the singular values $\\sigma_1, \\sigma_2, \\dots$, arranged in decreasing order.\n   \n3. **Compute $A A^T$ and find the eigenvectors:**\n\n   - The matrix $U$ is formed from the eigenvectors of $A A^T$.\n   \n4. **Transpose $V$:**\n\n   - The matrix $V^T$ is simply the transpose of $V$.\n\n## Example\n\nLet's consider a simple example where $A$ is a $2 \\times 2$ matrix:\n\n$$\nA = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\n\n### Step 1: Compute $A^T A$\n\n$$\nA^T A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n$$\n\nFind the eigenvalues of $A^T A$:\n\n$$\n\\det(A^T A - \\lambda I) = \\det\\begin{pmatrix} 10 - \\lambda & 6 \\\\ 6 & 10 - \\lambda \\end{pmatrix} = 0\n$$\n\n$$\n(10 - \\lambda)^2 - 36 = 0 \\quad \\Rightarrow \\quad \\lambda = 16, \\lambda = 4\n$$\n\nThe eigenvalues of $A^T A$ are $16$ and $4$, so the singular values of $A$ are $\\sigma_1 = 4$ and $\\sigma_2 = 2$.\n\n### Step 2: Find $V$ from the eigenvectors of $A^T A$\n\nSolve $(A^T A - \\lambda I)v = 0$ for each eigenvalue:\n\n- For $\\lambda = 16$, the eigenvector is $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- For $\\lambda = 4$, the eigenvector is $v_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n\nThus, \n\n$$\nV = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n### Step 3: Construct $\\Sigma$\n\nThe singular values $\\sigma_1 = 4$ and $\\sigma_2 = 2$, so:\n\n$$\n\\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\n\n### Step 4: Find $U$ from the eigenvectors of $A A^T$\n\nSimilarly, compute $A A^T$:\n\n$$\nA A^T = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 10 & 6 \\\\ 6 & 10 \\end{pmatrix}\n$$\n\nSolve for the eigenvectors of $A A^T$ (same as $A^T A$):\n\nThe eigenvectors are $u_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$ and $u_2 = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$.\n\nThus,\n\n$$\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n### Step 5: Final SVD\n\nWe can now write the SVD of $A$ as:\n\n$$\nA = U \\Sigma V^T\n$$\n\nWhere:\n\n$$\nU = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad V^T = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\n$$\n\n`Python` code to find SVD of this example is given below.\n\n::: {#2460af31 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the matrix A\nA = np.array([[3, 1],\n              [1, 3]])\n\n# Perform SVD decomposition\nU, Sigma, VT = np.linalg.svd(A)\n\n# Create Sigma matrix from singular values\nSigma_matrix = np.zeros((A.shape[0], A.shape[1]))\nnp.fill_diagonal(Sigma_matrix, Sigma)\n\n# Print results\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nU matrix:\")\nprint(U)\nprint(\"\\nSigma matrix:\")\nprint(Sigma_matrix)\nprint(\"\\nV^T matrix:\")\nprint(VT)\n\n# Verify the decomposition A = U * Sigma * V^T\nA_reconstructed = U @ Sigma_matrix @ VT\nprint(\"\\nReconstructed A (U * Sigma * V^T):\")\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[3 1]\n [1 3]]\n\nU matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nSigma matrix:\n[[4. 0.]\n [0. 2.]]\n\nV^T matrix:\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\n\nReconstructed A (U * Sigma * V^T):\n[[3. 1.]\n [1. 3.]]\n```\n:::\n:::\n\n\n## Reconstructing Matrix $A$ Using SVD\n\nGiven the Singular Value Decomposition (SVD) of a matrix $A$, the matrix can be reconstructed as a linear combination of low-rank matrices using the left singular vectors $u_i$, singular values $\\sigma_i$, and the right singular vectors $v_i^T$.\n\nThe formula to reconstruct the matrix $A$ is:\n\n$$\nA = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n$$\n\nwhere:\n- $r$ is the rank of the matrix $A$ (i.e., the number of non-zero singular values),\n- $\\sigma_i$ is the $i$-th singular value from the diagonal matrix $\\Sigma$,\n- $u_i$ is the $i$-th column of the matrix $U$ (left singular vectors),\n- $v_i^T$ is the transpose of the $i$-th row of the matrix $V^T$ (right singular vectors).\n\n### Breakdown of Terms:\n- $u_i \\in \\mathbb{R}^m$ is a column vector from the matrix $U$ (size $m \\times 1$),\n- $v_i^T \\in \\mathbb{R}^n$ is a row vector from the matrix $V^T$ (size $1 \\times n$),\n- $\\sigma_i \\in \\mathbb{R}$ is a scalar representing the $i$-th singular value.\n\nEach term $\\sigma_i u_i v_i^T$ represents a **rank-1 matrix** (the outer product of two vectors). The sum of these rank-1 matrices reconstructs the original matrix $A$.\n\n### Example:\n\nFor a matrix $A$, its SVD is represented as:\n\n$$\nA = U \\Sigma V^T = \\sum_{i=1}^{r} \\sigma_i \\, u_i \\, v_i^T\n$$\n\nIf the rank of $A$ is 2, then the reconstructed form of $A$ would be:\n\n$$\nA = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T\n$$\n\nEach term $\\sigma_i u_i v_i^T$ corresponds to a **low-rank approximation** that contributes to the final matrix. By summing these terms, the full matrix $A$ is obtained.\n\nPython code demonstrating reconstruction is given below:\n\n::: {#fd0aedd7 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the matrix A and convert it to float64\nA = np.array([[3, 1], \n              [1, 3]], dtype=np.float64)\n\n# Perform SVD\nU, Sigma, VT = np.linalg.svd(A)\n\n# Reconstruct A using the singular values and singular vectors\nA_reconstructed = np.zeros_like(A)  # This will be float64 now\nfor i in range(len(Sigma)-1):\n    A_reconstructed += Sigma[i] * np.outer(U[:, i], VT[i, :])\n\nprint(\"Original matrix A:\")\nprint(A)\n\nprint(\"\\nReconstructed A from rank-1 matrices:\")\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal matrix A:\n[[3. 1.]\n [1. 3.]]\n\nReconstructed A from rank-1 matrices:\n[[2. 2.]\n [2. 2.]]\n```\n:::\n:::\n\n\n## Singular Value Decomposition in Image Processing\n\n###  Image Compression\n\nSVD is widely used for compressing images. By approximating an image with a lower rank matrix, significant amounts of data can be reduced without a substantial loss in quality. The largest singular values and their corresponding singular vectors are retained, allowing for effective storage and transmission.\n\n###  Noise Reduction\n\nSVD helps in denoising images by separating noise from the original image data. By reconstructing the image using only the most significant singular values and vectors, the impact of noise (often associated with smaller singular values) can be minimized, resulting in a clearer image.\n\n###  Image Reconstruction\nIn applications where parts of an image are missing or corrupted, SVD can facilitate reconstruction. By analyzing the singular values and vectors, missing data can be inferred and filled in, preserving the structural integrity of the image.\n\n###  Facial Recognition\nSVD is employed in facial recognition systems as a means to extract features. By decomposing facial images into their constituent parts, SVD helps identify key features that distinguish different faces, enhancing recognition accuracy.\n\n###  Image Segmentation\nIn image segmentation, SVD can aid in clustering pixels based on their attributes. By reducing dimensionality, it helps identify distinct regions in an image, facilitating the separation of objects and backgrounds.\n\n###  Color Image Processing\nSVD can be applied to color images by treating each color channel separately. This allows for efficient manipulation, compression, and analysis of color images, improving overall processing performance.\n\n###  Pattern Recognition\nSVD is utilized in pattern recognition tasks where it helps to identify and classify patterns within images. By simplifying the data representation, SVD enhances the efficiency and accuracy of recognition algorithms.\n\n### Example\n\n::: {#5602bf52 .cell execution_count=3}\n``` {.python .cell-code}\nfrom PIL import Image\nimport urllib.request\nimport matplotlib.pyplot as plt\nurllib.request.urlretrieve(\n  'http://lenna.org/len_top.jpg',\n   \"input.jpg\")\n\nimg = Image.open(\"input.jpg\")\n```\n:::\n\n\n::: {#1bdf364f .cell execution_count=4}\n``` {.python .cell-code}\n# convert to grayscale\nimggray = img.convert('LA')\nplt.figure(figsize=(8,6))\nplt.imshow(imggray);\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-5-output-1.png){width=649 height=381}\n:::\n:::\n\n\n::: {#a7dd4db0 .cell execution_count=5}\n``` {.python .cell-code}\n# creating image histogram\nimport pandas as pd\nimport numpy as np\nimgmat = np.array(list(imggray.getdata(band=0)), float)\nA=pd.Series(imgmat)\nA.hist(bins=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-6-output-1.png){width=592 height=411}\n:::\n:::\n\n\n::: {#c6c20117 .cell execution_count=6}\n``` {.python .cell-code}\n# printing the pixel values\nprint(imgmat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 80.  80.  79. ... 100.  94.  99.]\n```\n:::\n:::\n\n\n::: {#149c493f .cell execution_count=7}\n``` {.python .cell-code}\n# dimension of the gray scale image matrix\nimgmat.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(90000,)\n```\n:::\n:::\n\n\n::: {#1d9759ab .cell execution_count=8}\n``` {.python .cell-code}\n##loading an image and show it using matrices of pixel values\nfrom skimage import io\nf = \"http://lenna.org/len_top.jpg\" #url of the image\na = io.imread(f) # read the image to a tensor\nc1=a[:,:,0] # channel 1\nc2=a[:,:,1] # channel 2\nc3=a[:,:,2] # channel 3\nprint(c1)\n# dimension of channel-1\nc1.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[109 109 108 ...  54  60  67]\n [112 111 107 ...  52  55  61]\n [111 110 107 ...  51  54  60]\n ...\n [130 129 133 ... 122 119 125]\n [128 127 132 ... 125 119 123]\n [133 130 127 ... 139 133 140]]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(225, 400)\n```\n:::\n:::\n\n\n::: {#33a944ea .cell execution_count=9}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(12, 3))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nax1.imshow(c1, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax2.imshow(c2, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax3.imshow(c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-10-output-1.png){width=947 height=194}\n:::\n:::\n\n\n::: {#850ae180 .cell execution_count=10}\n``` {.python .cell-code}\nc1_array=np.array(list(c1)).reshape(-1)\npd.Series(c1_array).hist()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-11-output-1.png){width=592 height=411}\n:::\n:::\n\n\n::: {#c16cf91d .cell execution_count=11}\n``` {.python .cell-code}\n## an application of matrix addition\nplt.imshow(0.34*c1-0.2*c2-0.01*c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-12-output-1.png){width=575 height=339}\n:::\n:::\n\n\n::: {#77dd1ac9 .cell execution_count=12}\n``` {.python .cell-code}\n#converting a grayscale image to numpy array\nimgmat = np.array(list(imggray.getdata(band=0)), float)\nimgmat.shape = (imggray.size[1], imggray.size[0])\nimgmat = np.matrix(imgmat)\nplt.figure(figsize=(8,6))\nplt.imshow(imgmat, cmap='gray');\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-13-output-1.png){width=649 height=381}\n:::\n:::\n\n\nAs promised, one line of command is enough to get the singular value decomposition. $U$ and $V$ are the left-hand side and the right-hand side matrices, respectively. 'sigma' is a vector containing the diagonal entries of the matrix $\\Sigma$ The other two lines reconstruct the matrix using the first singular value only. You can already guess the rough shape of the original image.\n\n::: {#89084a09 .cell execution_count=13}\n``` {.python .cell-code}\nU, sigma, V = np.linalg.svd(imgmat)\n\nreconstimg = np.matrix(U[:, :1]) * np.diag(sigma[:1]) * np.matrix(V[:1, :])\nplt.imshow(reconstimg, cmap='gray');\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-14-output-1.png){width=575 height=339}\n:::\n:::\n\n\nLet's see what we get when we use the second, third and fourth singular value as well.\n\n::: {#9a36ca11 .cell execution_count=14}\n``` {.python .cell-code}\nfor i in range(2, 4):\n    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n    plt.imshow(reconstimg, cmap='gray')\n    title = \"n = %s\" % i\n    plt.title(title)\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-15-output-1.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-15-output-2.png){width=575 height=354}\n:::\n:::\n\n\nNow we let $i$ run from 5 to 51, using a step width of 5. For $i=50$, we already get a pretty good image!\n\n::: {#dfeb50d8 .cell execution_count=15}\n``` {.python .cell-code}\nfor i in range(5, 51, 5):\n    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n    plt.imshow(reconstimg, cmap='gray')\n    title = \"n = %s\" % i\n    plt.title(title)\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-1.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-2.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-3.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-4.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-5.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-6.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-7.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-8.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-9.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-16-output-10.png){width=575 height=354}\n:::\n:::\n\n\nBut how many singular values do we have after all? The following command gives us the number of entries in sigma. As it is the diagonal matrix, it is stored as a vector and we do not save the zero entries. We now output the number of singular values (the length of the vector sigma, containing the diagonal entries), as well as the size of the matrices $U$ and $V$.\n\n::: {#344d08cb .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"We have %d singular values.\" % sigma.shape)\nprint(\"U is of size\", U.shape, \".\")\nprint(\"V is of size\", V.shape, \".\")\nprint(\"The last, or smallest entry in sigma is\", sigma[224])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe have 225 singular values.\nU is of size (225, 225) .\nV is of size (400, 400) .\nThe last, or smallest entry in sigma is 9.637679189276597\n```\n:::\n:::\n\n\nAs Python stores the whole singular value decomposition, we do not really save space. But as you saw in the first theoretical exercise of the 10th series, we do not have to compute the whole matrices $U$ and $V$ if we know that we only want to reconstruct the rank $k$ approximation. How many numbers do you have to store for the initial matrix of the picture? How many numbers do you have to store if you want to reconstruct the rank $k$ approximation only?\n\nUse the following Cell to find an $i$ large enough that you are satisfied with the quality of the image. Check, how much percent of the initial size you have to store. If your picture has a different resolution, you will have to correct the terms.\n\n::: {#3eac7ac7 .cell execution_count=17}\n``` {.python .cell-code}\ni = 10\nreconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\nplt.imshow(reconstimg, cmap='gray')\ntitle = \"n = %s\" % i\nplt.title(title)\nplt.show()\n\nnumbers = 400*i + i + 225* i\nprint(\"For this quality, we have to store %d numbers.\" % numbers)\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-18-output-1.png){width=575 height=354}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFor this quality, we have to store 6260 numbers.\n```\n:::\n:::\n\n\nIf you really want to have a good quality, say you want to reconstruct using $r - 1$ singular values, where $r$ is the total number of singular values, is it still a good idea to use the singular value decomposition?\n\n::: {#fd20a8b7 .cell execution_count=18}\n``` {.python .cell-code}\ni = 30\nreconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\nplt.imshow(reconstimg, cmap='gray')\ntitle = \"n = %s\" % i\nplt.title(title)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](module_5_files/figure-html/cell-19-output-1.png){width=575 height=354}\n:::\n:::\n\n\n## Takeaway\n\nSingular Value Decomposition provides a general framework for decomposing any matrix into orthogonal components, revealing the underlying structure of the matrix. SVD has numerous applications in machine learning, signal processing, and more. The method to find the matrices $U$, $\\Sigma$, and $V^T$ involves using the eigenvalues and eigenvectors of $A^T A$ and $A A^T$.\n\n",
    "supporting": [
      "module_5_files"
    ],
    "filters": [],
    "includes": {}
  }
}