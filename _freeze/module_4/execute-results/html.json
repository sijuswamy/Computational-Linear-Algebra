{
  "hash": "44119849076aa28f6175c338710a18a2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Algebra for Advanced Applications\"\nexecute: \n  enabled: true\njupyter: python3\n---\n\n\n\n\n\n\n\n\n## Introduction\n\nMatrix decomposition plays a pivotal role in computational linear algebra, forming the backbone of numerous modern applications in fields such as data science, machine learning, computer vision, and signal processing. The core idea behind matrix decomposition is to break down complex matrices into simpler, structured components that allow for more efficient computation. Techniques such as LU, QR, Singular Value Decomposition (SVD), and Eigenvalue decompositions not only reduce computational complexity but also provide deep insights into the geometry and structure of data. These methods are essential in solving systems of linear equations, performing dimensionality reduction, and extracting meaningful features from data. For instance, LU decomposition is widely used to solve large linear systems, while QR decomposition plays a key role in solving least squares problems—a fundamental task in machine learning models.\n\nIn emerging fields like big data analytics and artificial intelligence, matrix decomposition techniques are indispensable for processing and analyzing high-dimensional datasets. SVD and Principal Component Analysis (PCA), for example, are extensively used for data compression and noise reduction, making machine learning algorithms more efficient by reducing the number of variables while retaining key information. Additionally, sparse matrix decompositions allow for the handling of enormous datasets where most entries are zero, optimizing memory usage and computation time. As data science and machine learning continue to evolve, mastering these matrix decomposition techniques provides not only a computational advantage but also deeper insights into the structure and relationships within data, enhancing the performance of algorithms in real-world applications.\n\n## LU Decomposition\n\nLU decomposition is a powerful tool in linear algebra that elegantly unravels the complexity of solving systems of linear equations. At its core, LU decomposition expresses a matrix $A$ as the product of two distinct matrices: $L$ (a lower triangular matrix with ones on the diagonal) and $U$ (an upper triangular matrix). This decomposition transforms the problem of solving $Ax = b$ into a two-step process: first, solving $Ly = b$ for $y$, followed by $Ux = y$ for $x$. This systematic approach not only simplifies computations but also provides insightful perspectives on the relationships between the equations involved.\n\nThe magic of LU decomposition lies in its utilization of elementary transformations—operations that allow us to manipulate the rows of a matrix to achieve a row-reduced echelon form. These transformations include row swaps, scaling, and adding multiples of one row to another. By applying these operations, we can gradually transform the original matrix $A$ into the upper triangular matrix $U$, while simultaneously capturing the essence of these transformations in the lower triangular matrix $L$. This interplay of $L$ and $U$ not only enhances computational efficiency but also unveils the deeper structural relationships within the matrix. \n\nMoreover, the beauty of matrix multiplication shines through in LU decomposition. The product $A = LU$ showcases how two simpler matrices can combine to reconstruct a more complex one, demonstrating the power of linear combinations in solving equations. As we delve into LU decomposition, we embark on a journey that highlights the synergy between algebraic manipulation and geometric interpretation, empowering us to tackle intricate problems with grace and precision.\nGiven a square matrix $A$, the LU decomposition expresses $A$ as a product of a lower triangular matrix $L$ and an upper triangular matrix $U$:\n$$A = LU$$\n\nWhere:\n- $L$ is a lower triangular matrix with 1's on the diagonal and other elements like $l_{21}, l_{31}, l_{32}, \\dots$,\n- $U$ is an upper triangular matrix with elements $u_{11}, u_{12}, u_{13}, u_{22}, u_{23}, u_{33}, \\dots$.\n\n### Step-by-Step Procedure\n\nLet’s assume $A$ is a $3 \\times 3$ matrix for simplicity:\n$$A = \\begin{pmatrix}a_{11} & a_{12} & a_{13} \\\\a_{21} & a_{22} & a_{23} \\\\a_{31} & a_{32} & a_{33}\\end{pmatrix}$$\n\nWe need to find matrices $L$ and $U$, where:\n\n- $L = \\begin{pmatrix} \n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}$\n\n- $U = \\begin{pmatrix} \nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}$\n\nThe product of $L$ and $U$ gives:\n$$LU = \\begin{pmatrix} 1 & 0 & 0 \\\\l_{21} & 1 & 0 \\\\l_{31} & l_{32} & 1\\end{pmatrix}\\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\0 & u_{22} & u_{23} \\\\0 & 0 & u_{33}\\end{pmatrix}=\\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\l_{21}u_{11} & l_{21}u_{12} + u_{22} & l_{21}u_{13} + u_{23} \\\\l_{31}u_{11} & l_{31}u_{12} + l_{32}u_{22} & l_{31}u_{13} + l_{32}u_{23} + u_{33}\\end{pmatrix}$$\n\nBy equating this with $A$, we can set up a system of equations to solve for $l_{ij}$ and $u_{ij}$.\n\n>Step 1: Solve for $u_{11}, u_{12}, u_{13}$\n\nFrom the first row of $A = LU$, we have:\n$$u_{11} = a_{11}$$\n$$u_{12} = a_{12}$$\n$$u_{13} = a_{13}$$\n\n>Step 2: Solve for $l_{21}$ and $u_{22}, u_{23}$\n\nFrom the second row, we get:\n$$l_{21}u_{11} = a_{21} \\quad \\Rightarrow \\quad l_{21} = \\frac{a_{21}}{u_{11}}$$\n$$l_{21}u_{12} + u_{22} = a_{22} \\quad \\Rightarrow \\quad u_{22} = a_{22} - l_{21}u_{12}$$\n$$l_{21}u_{13} + u_{23} = a_{23} \\quad \\Rightarrow \\quad u_{23} = a_{23} - l_{21}u_{13}$$\n\n>Step 3: Solve for $l_{31}, l_{32}$ and $u_{33}$\n\nFrom the third row, we get:\n$$l_{31}u_{11} = a_{31} \\quad \\Rightarrow \\quad l_{31} = \\frac{a_{31}}{u_{11}}$$\n$$l_{31}u_{12} + l_{32}u_{22} = a_{32} \\quad \\Rightarrow \\quad l_{32} = \\frac{a_{32} - l_{31}u_{12}}{u_{22}}$$\n$$l_{31}u_{13} + l_{32}u_{23} + u_{33} = a_{33} \\quad \\Rightarrow \\quad u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}$$\n\n**Final Result**\n\nThus, the LU decomposition is given by the matrices:\n- $L = \\begin{pmatrix} \n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}$\n- $U = \\begin{pmatrix} \nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}$\n\nWhere:\n- $u_{11} = a_{11}, u_{12} = a_{12}, u_{13} = a_{13}$\n- $l_{21} = \\frac{a_{21}}{u_{11}}, u_{22} = a_{22} - l_{21}u_{12}, u_{23} = a_{23} - l_{21}u_{13}$\n- $l_{31} = \\frac{a_{31}}{u_{11}}, l_{32} = \\frac{a_{32} - l_{31}u_{12}}{u_{22}}, u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}$\n\n### Example\n\nLet’s decompose the following matrix:\n$$A = \\begin{pmatrix} 4 & 3 & 2 \\\\6 & 3 & 1 \\\\2 & 1 & 3\\end{pmatrix}$$\n\nFollowing the steps outlined above:\n\n- $u_{11} = 4, u_{12} = 3, u_{13} = 2$\n- $l_{21} = \\frac{6}{4} = 1.5$, so:\n  - $u_{22} = 3 - 1.5 \\times 3 = -1.5$\n  - $u_{23} = 1 - 1.5 \\times 2 = -2$\n- $l_{31} = \\frac{2}{4} = 0.5$, so:\n  - $l_{32} = \\frac{1 - 0.5 \\times 3}{-1.5} = 0.67$\n  - $u_{33} = 3 - 0.5 \\times 2 - 0.67 \\times (-2) = 2.67$\n\nThus, the decomposition is:\n- $L = \\begin{pmatrix} \n1 & 0 & 0 \\\\\n1.5 & 1 & 0 \\\\\n0.5 & 0.67 & 1\n\\end{pmatrix}$\n- $U = \\begin{pmatrix} \n4 & 3 & 2 \\\\\n0 & -1.5 & -2 \\\\\n0 & 0 & 2.67\n\\end{pmatrix}$\n\n### Python Implementation\n\n::: {#ea3421d1 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.linalg import lu\n\n# Define matrix A\nA = np.array([[4, 3, 2],\n              [6, 3, 1],\n              [2, 1, 3]])\n\n# Perform LU decomposition\nP, L, U = lu(A)\n\n# Print the results\nprint(\"L = \\n\", L)\nprint(\"U = \\n\", U)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL = \n [[1.         0.         0.        ]\n [0.66666667 1.         0.        ]\n [0.33333333 0.         1.        ]]\nU = \n [[6.         3.         1.        ]\n [0.         1.         1.33333333]\n [0.         0.         2.66666667]]\n```\n:::\n:::\n\n\n:::{.callout-note}\n\n### Note\n\nSince there are many row transformations that reduce a given matrix into row echelon form. So the LU decomposition is not unique.\n\n:::\n\n\n\n### LU Decomposition Practice Problems with Solutions\n\n**Problem 1:**\nDecompose the matrix \n$$ A = \\begin{pmatrix} 4 & 3 \\\\ 6 & 3 \\end{pmatrix} $$\ninto the product of a lower triangular matrix $L$ and an upper triangular matrix $U$.\n\n**Solution:**\n\nLet \n$$ L = \\begin{pmatrix} 1 & 0 \\\\ l_{21} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{pmatrix}. $$\n\nWe have:\n\n1. From the first row: $u_{11} = 4$ and $u_{12} = 3$.\n2. From the second row: $6 = l_{21} \\cdot 4$ gives $l_{21} = \\frac{6}{4} = 1.5$.\n3. Finally, $3 = 1.5 \\cdot 3 + u_{22}$ gives $u_{22} = 3 - 4.5 = -1.5$.\n\nThus, we have:\n$$ L = \\begin{pmatrix} 1 & 0 \\\\ 1.5 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 4 & 3 \\\\ 0 & -1.5 \\end{pmatrix}. $$\n\n**Problem 2:**\nGiven the matrix \n$$ A = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 5 & 8 \\\\ 4 & 5 & 6 \\end{pmatrix}, $$\nperform LU decomposition to find matrices $L$ and $U$.\n\n**Solution:**\n\nLet \n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. $$\n\nWe have:\n\n1. From Row 1: $u_{11} = 1, u_{12} = 2, u_{13} = 3$.\n2. From Row 2: $2 = l_{21} \\cdot 1$ gives $l_{21} = 2$.\n   - For Row 2: $5 = l_{21} \\cdot 2 + u_{22}$ gives $5 = 4 + u_{22} \\Rightarrow u_{22} = 1$.\n   - $8 = l_{21} \\cdot 3 + u_{23} \\Rightarrow 8 = 6 + u_{23} \\Rightarrow u_{23} = 2$.\n3. From Row 3: $4 = l_{31} \\cdot 1 \\Rightarrow l_{31} = 4$.\n   - $5 = l_{31} \\cdot 2 + l_{32} \\cdot 1 \\Rightarrow 5 = 8 + l_{32} \\Rightarrow l_{32} = -3$.\n   - Finally, $6 = l_{31} \\cdot 3 + l_{32} \\cdot 2 + u_{33} \\Rightarrow 6 = 12 - 6 + u_{33} \\Rightarrow u_{33} = 0$.\n\nThus,\n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 4 & -3 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix}. $$\n\n**Problem 3:**\nPerform LU decomposition of the matrix \n$$ A = \\begin{pmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{pmatrix}, $$\nand verify the decomposition by checking $A = LU$.\n\n**Solution:**\n\nLet \n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. $$\n\nWe have:\n\n1. From Row 1: $u_{11} = 2, u_{12} = 1, u_{13} = 1$.\n2. From Row 2: $4 = l_{21} \\cdot 2 \\Rightarrow l_{21} = 2$.\n   - $-6 = 2 \\cdot 1 + u_{22} \\Rightarrow u_{22} = -8$.\n   - $0 = 2 \\cdot 1 + u_{23} \\Rightarrow u_{23} = -2$.\n3. From Row 3: $-2 = l_{31} \\cdot 2 \\Rightarrow l_{31} = -1$.\n   - $7 = -1 \\cdot 1 + l_{32} \\cdot -8 \\Rightarrow 7 = -1 - 8l_{32} \\Rightarrow l_{32} = -1$.\n   - Finally, $2 = -1 \\cdot 1 + -1 \\cdot -2 + u_{33} \\Rightarrow 2 = 1 + u_{33} \\Rightarrow u_{33} = 1$.\n\nThus,\n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{pmatrix}. $$\n\n**Problem 4:**\nFor the matrix \n$$ A = \\begin{pmatrix} 3 & 1 & 6 \\\\ 2 & 1 & 1 \\\\ 1 & 2 & 2 \\end{pmatrix}, $$\nfind the LU decomposition and use it to solve the system $Ax = b$ where $b = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix}$.\n\n**Solution:**\n\nLet \n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ l_{31} & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & u_{13} \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}. $$\n\nWe have:\n\n1. From Row 1: $u_{11} = 3, u_{12} = 1, u_{13} = 6$.\n2. From Row 2: $2 = l_{21} \\cdot 3 \\Rightarrow l_{21} = \\frac{2}{3}$.\n   - $1 = \\frac{2}{3} \\cdot 1 + u_{22} \\Rightarrow 1 = \\frac{2}{3} + u_{22} \\Rightarrow u_{22} = \\frac{1}{3}$.\n   - $1 = \\frac{2}{3} \\cdot 6 + u_{23} \\Rightarrow 1 = 4 + u_{23} \\Rightarrow u_{23} = -3$.\n3. From Row 3: $1 = l_{31} \\cdot 3 \\Rightarrow l_{31} = \\frac{1}{3}$.\n   - $2 = \\frac{1}{3} \\cdot 1 + l_{32} \\cdot \\frac{1}{3} \\Rightarrow 2 = \\frac{1}{3} + \\frac{1}{3} l_{32} \\Rightarrow l_{32} = 6$.\n   - Finally, $2 = \\frac{1}{3} \\cdot 6 + 6 \\cdot -3 + u_{33} \\Rightarrow 2 = 2 - 18 + u_{33} \\Rightarrow u_{33} = 18$.\n\nThus,\n$$ L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{2}{3} & 1 & 0 \\\\ \\frac{1}{3} & 6 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 3 & 1 & 6 \\\\ 0 & \\frac{1}{3} & -3 \\\\ 0 & 0 & 18 \\end{pmatrix}. $$\n\nNow, to solve $Ax = b$, we first solve $Ly = b$:\n$$ \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{2}{3} & 1 & 0 \\\\ \\frac{1}{3} & 6 & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix} $$\n\nSolving this gives:\n1. $y_1 = 9$\n2. $\\frac{2}{3} \\cdot 9 + y_2 = 5 \\Rightarrow 6 + y_2 = 5 \\Rightarrow y_2 = -1$\n3. $\\frac{1}{3} \\cdot 9 + 6 \\cdot -1 + y_3 = 4 \\Rightarrow 3 - 6 + y_3 = 4 \\Rightarrow y_3 = 7$\n\nNext, solve $Ux = y$:\n$$ \\begin{pmatrix} 3 & 1 & 6 \\\\ 0 & \\frac{1}{3} & -3 \\\\ 0 & 0 & 18 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ -1 \\\\ 7 \\end{pmatrix} $$\n\n1. From Row 3: $18x_3 = 7 \\Rightarrow x_3 = \\frac{7}{18}$\n2. From Row 2: $\\frac{1}{3}x_2 - 3x_3 = -1 \\Rightarrow \\frac{1}{3}x_2 - \\frac{21}{18} = -1 \\Rightarrow \\frac{1}{3}x_2 = -\\frac{18}{18} + \\frac{21}{18} = \\frac{3}{18} \\Rightarrow x_2 = \\frac{1}{3}$\n3. From Row 1: $3x_1 + x_2 + 6x_3 = 9 \\Rightarrow 3x_1 + \\frac{1}{3} + \\frac{42}{18} = 9 \\Rightarrow 3x_1 + \\frac{1}{3} + \\frac{7}{3} = 9 \\Rightarrow 3x_1 = 9 - \\frac{8}{3} = \\frac{27 - 8}{3} = \\frac{19}{3} \\Rightarrow x_1 = \\frac{19}{9}$\n\nThus, the solution to $Ax = b$ is \n$$ x = \\begin{pmatrix} \\frac{19}{9} \\\\ \\frac{1}{3} \\\\ \\frac{7}{18} \\end{pmatrix}. $$\n\n## LU Decomposition Practice Problems\n\n**Problem 1:**\nDecompose the matrix \n$$ A = \\begin{pmatrix} 4 & 3 \\\\ 6 & 3 \\end{pmatrix} $$\ninto the product of a lower triangular matrix $L$ and an upper triangular matrix $U$.\n\n**Problem 2:**\nGiven the matrix \n$$ A = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 5 & 8 \\\\ 4 & 5 & 6 \\end{pmatrix}, $$\nperform LU decomposition to find matrices $L$ and $U$.\n\n**Problem 3:**\nPerform LU decomposition of the matrix \n$$ A = \\begin{pmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{pmatrix}, $$\nand verify the decomposition by checking $A = LU$.\n\n**Problem 4:**\nFor the matrix \n$$ A = \\begin{pmatrix} 3 & 1 & 6 \\\\ 2 & 1 & 1 \\\\ 1 & 2 & 2 \\end{pmatrix}, $$\nfind the LU decomposition and use it to solve the system $Ax = b$ where $b = \\begin{pmatrix} 9 \\\\ 5 \\\\ 4 \\end{pmatrix}$.\n\n**Problem 5:**\nDecompose the matrix \n$$ A = \\begin{pmatrix} 1 & 3 & 1 \\\\ 2 & 6 & 1 \\\\ 1 & 1 & 4 \\end{pmatrix} $$\ninto $L$ and $U$, and solve the system $Ax = \\begin{pmatrix} 5 \\\\ 9 \\\\ 6 \\end{pmatrix}$.\n\n**Problem 6:**\nGiven the matrix \n$$ A = \\begin{pmatrix} 7 & 3 \\\\ 2 & 5 \\end{pmatrix}, $$\nperform LU decomposition and use the result to solve $Ax = b$ for $b = \\begin{pmatrix} 10 \\\\ 7 \\end{pmatrix}$.\n\n**Problem 7:**\nFind the LU decomposition of the matrix \n$$ A = \\begin{pmatrix} 2 & -1 & 1 \\\\ -2 & 2 & -1 \\\\ 4 & -1 & 3 \\end{pmatrix}, $$\nand use it to solve $Ax = b$ where $b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 7 \\end{pmatrix}$.\n\n**Problem 8:**\nPerform LU decomposition of the matrix \n$$ A = \\begin{pmatrix} 5 & 2 & 1 \\\\ 10 & 4 & 3 \\\\ 15 & 8 & 6 \\end{pmatrix}. $$\n\n**Problem 9:**\nUse LU decomposition to find the solution to the system $Ax = b$ where\n$$ A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 2 & 3 & 5 \\\\ 4 & 6 & 8 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 15 \\\\ 30 \\end{pmatrix}. $$\n\n**Problem 10:**\nDecompose the matrix \n$$ A = \\begin{pmatrix} 6 & -2 & 2 \\\\ 12 & -8 & 6 \\\\ -6 & 3 & -3 \\end{pmatrix} $$\ninto $L$ and $U$, and verify that $A = LU$.\n\n## Matrix Approach to Create LU Decomposition\n\nLU decomposition can be performed using _elementary matrix operations_. In this method, we iteratively apply elementary matrices to reduce the given matrix $A$ into an upper triangular matrix $U$, while keeping track of the transformations to form the lower triangular matrix $L$.\n\nThe LU decomposition can be written as:\n$$A = LU$$\n\n\nwhere:\n- $L$ is the product of the inverses of the elementary matrices.\n- $U$ is the upper triangular matrix obtained after applying the row operations.\n\n**Example: LU Decomposition of a 3x3 Matrix**\n\nGiven the matrix:\n$$\nA = \\begin{pmatrix} \n2 & 1 & 1 \\\\\n4 & -6 & 0 \\\\\n-2 & 7 & 2 \n\\end{pmatrix}\n$$\n\nWe will decompose $A$ into $L$ and $U$ using elementary row operations.\n\n>Step 1: Applying Elementary Matrices\n\nWe want to perform row operations to reduce $A$ into upper triangular form.\n\n>Step 1.1: Eliminate the $a_{21}$ entry (below the pivot in column 1)\n\nTo eliminate the $4$ in position $a_{21}$, perform the operation:\n$$R_2 \\rightarrow R_2 - 2R_1$$\n\nThis corresponds to multiplying $A$ by the elementary matrix:\n$$E_1 = \\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1\\end{pmatrix}$$\n\nAfter this row operation, the matrix becomes:\n$$\nE_1 A = \\begin{pmatrix} \n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n-2 & 7 & 2 \n\\end{pmatrix}\n$$\n\n>Step 1.2: Eliminate the $a_{31}$ entry\n\nTo eliminate the $-2$ in position $a_{31}$, perform the operation:\n$$R_3 \\rightarrow R_3 + R_1$$\n\nThis corresponds to multiplying the matrix by another elementary matrix:\n$$\nE_2 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\n\nNow, the matrix becomes:\n$$\nE_2 E_1 A = \\begin{pmatrix} \n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 8 & 3\n\\end{pmatrix}\n$$\n\n>Step 1.3: Eliminate the $a_{32}$ entry\n\nFinally, to eliminate the $8$ in position $a_{32}$, perform the operation:\n$$\nR_3 \\rightarrow R_3 + R_2\n$$\n\nThis corresponds to multiplying the matrix by the third elementary matrix:\n\n$$\nE_3 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\n\nAfter applying this operation, the matrix becomes:\n$$\nE_3 E_2 E_1 A = \\begin{pmatrix} \n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\n\nThis is the upper triangular matrix $U$.\n\n>Step 2: Construct the Lower Triangular Matrix $L$\n\nThe lower triangular matrix $L$ is formed by taking the inverses of the elementary matrices $E_1, E_2, E_3$. Each inverse corresponds to the inverse of the row operations we applied.\n\n- $E_1^{-1}$ corresponds to adding back $2R_1$ to $R_2$, so:\n$$\nE_1^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\n\n- $E_2^{-1}$ corresponds to subtracting $R_1$ from $R_3$, so:\n$$\nE_2^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n-1 & 0 & 1\n\\end{pmatrix}\n$$\n\n- $E_3^{-1}$ corresponds to subtracting $R_2$ from $R_3$, so:\n$$\nE_3^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & -1 & 1\n\\end{pmatrix}\n$$\n\nNow, the lower triangular matrix $L$ is obtained by multiplying these inverses in reverse order:\n$$\nL = E_3^{-1} E_2^{-1} E_1^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{pmatrix}\n$$\n\nThus, the LU decomposition of $A$ is:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{pmatrix}, \n\\quad U = \\begin{pmatrix} \n2 & 1 & 1 \\\\\n0 & -8 & -2 \\\\\n0 & 0 & 1 \n\\end{pmatrix}\n$$\n\n**Verification**\n\nNow, we check if $A = LU$.\n\nMultiply $L$ and $U$:\n\n::: {#bead0ff8 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\nL = np.array([[1, 0, 0],\n              [2, 1, 0],\n              [-1, -1, 1]])\n\nU = np.array([[2, 1, 1],\n              [0, -8, -2],\n              [0, 0, 1]])\n\nA = L @ U\nA\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([[ 2,  1,  1],\n       [ 4, -6,  0],\n       [-2,  7,  2]])\n```\n:::\n:::\n\n\n# Spectral Decomposition\n\n## Background\n\nImagine encountering a low-resolution image of a familiar scene. The human brain excels at recognizing familiar objects by relying on essential features, often extracting the most significant details while discarding the less important information. This cognitive process mirrors the power of eigenvalue decomposition, where eigenvectors represent the ``nectar'' of a matrix, capturing its most important characteristics.\n\nAs an example, try to identify this image. If you can do it, then your brain  know this place!\n\n![](gitsRC.jpg)\n\n\nBefore proceeding further just compare the size of its' original clean image and the low-quality image shown in Figure\n\n```{.matlab}\nOriginal image size: 985.69 KB\nReconstructed image size: 1.12 KB\n```\n\nThe reconstructed image is just 0.2\\% of the original in size! This is the core principle of optimizing image storage of CCTV system. This resizing can be done and execute with optimal scaling with the help of Linear Algebra. This module mainly focuses on such engineering applications.\n\n## Introduction\n\nSpectral decomposition, also known as eigenvalue decomposition, is a powerful tool in computational linear algebra that breaks down a matrix into its eigenvalues and eigenvectors. This technique allows matrices to be represented in terms of their fundamental components, making it easier to analyze and manipulate them. It is especially useful for symmetric matrices, which are common in various applications. Spectral decomposition facilitates solving systems of equations, optimizing functions, and performing transformations in a simplified, structured manner, as it allows operations to be performed on the eigenvalues, which often leads to more efficient computations.\n\nThe importance of spectral decomposition extends across a wide range of fields, including computer science, engineering, and data science. In machine learning, for instance, it forms the backbone of algorithms like Principal Component Analysis (PCA), which is used for dimensionality reduction. It also plays a vital role in numerical stability when dealing with large matrices and is central to many optimization problems, such as those found in machine learning and physics. Spectral decomposition not only provides a deeper understanding of the properties of matrices but also offers practical benefits in improving the efficiency and accuracy of numerical algorithms.\n\n## Spectral Decomposition: Detailed Concepts\n\n###  Eigenvalues and Eigenvectors\nThe core idea behind spectral decomposition is that it expresses a matrix in terms of its eigenvalues and eigenvectors. For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, an eigenvalue $\\lambda \\in \\mathbb{R}$ and an eigenvector $v \\in \\mathbb{R}^{n}$ satisfy the following equation:\n\n$$\nA v = \\lambda v\n$$\n\nThis implies that when the matrix $A$ acts on the vector $v$, it only scales the vector by $\\lambda$, but does not change its direction. The eigenvector $v$ represents the direction of this scaling, while the eigenvalue $\\lambda$ represents the magnitude of the scaling.\n\n:::{.callout-note}\n## Properties of Eigen values\n\n- If $\\lambda$ is an eigenvalue of $A$, then it satisfies the characteristic polynomial:\n\n  $$\n  p(\\lambda) = \\text{det}(A - \\lambda I) = 0.\n  $$\n\n- The sum of the eigenvalues (counted with algebraic multiplicity) is equal to the trace of the matrix:\n\n  $$\n  \\sum_{i=1}^{n} \\lambda_i = \\text{trace}(A).\n  $$\n\n- The product of the eigenvalues (counted with algebraic multiplicity) is equal to the determinant of the matrix:\n\n  $$\n  \\prod_{i=1}^{n} \\lambda_i = \\text{det}(A).\n  $$\n\n- If$A$ is symmetric, then:\n  - All eigenvalues $\\lambda$ are real.\n  - If $\\lambda_i$ and $\\lambda_j$ are distinct eigenvalues, then their corresponding eigenvectors $\\mathbf{v}_i$ and $\\mathbf{v}_j$ satisfy:\n\n  $$\n  \\mathbf{v}_i^T \\mathbf{v}_j = 0.\n  $$\n\n- If $A$ is a scalar multiple of $k$, then:\n\n  $$\n  \\lambda_i \\text{ of } kA = k \\cdot \\lambda_i \\text{ of } A.\n  $$\n\n- If $A$ is invertible, then:\n\n  $$\n  \\lambda_i \\text{ of } A^{-1} = \\frac{1}{\\lambda_i \\text{ of } A}.\n  $$\n\n- If $A$ and $B$ are similar, then:\n\n  $$\n  B = P^{-1} A P \\implies \\lambda_i \\text{ of } B = \\lambda_i \\text{ of } A.\n  $$\n\n- If $\\lambda$ is an eigenvalue, it has:\n  - **Algebraic Multiplicity**: The number of times $\\lambda$ appears as a root of $p(\\lambda)$.\n  - **Geometric Multiplicity**: The dimension of the eigenspace $E_{\\lambda} = \\{\\mathbf{v} : A\\mathbf{v} = \\lambda \\mathbf{v}\\}$.\n\n- If$A$ is symmetric and all eigenvalues $\\lambda$ are positive, then $A$ is positive definite:\n\n  $$\n  \\lambda_i > 0 \\implies A \\text{ is positive definite.}\n  $$\n\n- A square matrix $A$ has an eigenvalue $\\lambda = 0$ if and only if $A$ is singular:\n\n  $$\n  \\text{det}(A) = 0 \\iff \\lambda = 0.\n  $$\n\n:::\n\n:::{.callout-important}\n## Eigen Vectors\n\nEigen vectors are the non-trivial solutions of $det(A-\\lambda I)=0$ for distinct $\\lambda$.\n:::\n\n:::{.callout-note}\n## Properties of Eigen vectors\n- If $\\mathbf{v}$ is an eigenvector of a square matrix $A$ corresponding to the eigenvalue $\\lambda$, then:\n\n  $$\n  A\\mathbf{v} = \\lambda \\mathbf{v}.\n  $$\n\n- Eigenvectors corresponding to distinct eigenvalues are linearly independent. If $\\lambda_1$ and $\\lambda_2$ are distinct eigenvalues of $A$, with corresponding eigenvectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$, then:\n\n  $$\n  c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 = \\mathbf{0} \\implies c_1 = 0 \\text{ and } c_2 = 0.\n  $$\n\n- If $\\mathbf{v}$ is an eigenvector corresponding to the eigenvalue $\\lambda$, then any non-zero scalar multiple of $\\mathbf{v}$ is also an eigenvector corresponding to $\\lambda$:\n\n  $$\n  \\text{If } \\mathbf{v} \\text{ is an eigenvector, then } c\\mathbf{v} \\text{ is an eigenvector for any non-zero scalar } c.\n  $$\n\n- The eigenspace $E_{\\lambda}$ associated with an eigenvalue $\\lambda$ is defined as:\n\n  $$\n  E_{\\lambda} = \\{ \\mathbf{v} : A\\mathbf{v} = \\lambda \\mathbf{v} \\} = \\text{Null}(A - \\lambda I).\n  $$\n\n- The dimension of the eigenspace $E_{\\lambda}$ is equal to the geometric multiplicity of the eigenvalue $\\lambda$.\n\n- If $A$ is a symmetric matrix, then eigenvectors corresponding to distinct eigenvalues are orthogonal:\n\n  $$\n  \\mathbf{v}_i^T \\mathbf{v}_j = 0 \\text{ for distinct eigenvalues } \\lambda_i \\text{ and } \\lambda_j.\n  $$\n\n- For any square matrix $A$, if $\\lambda = 0$ is an eigenvalue, the eigenvectors corresponding to this eigenvalue form the null space of $A$:\n\n  $$\n  E_{0} = \\{ \\mathbf{v} : A\\mathbf{v} = \\mathbf{0} \\} = \\text{Null}(A).\n  $$\n\n- If$A$ is invertible, then $A$ has no eigenvalue equal to zero, meaning all eigenvectors correspond to non-zero eigenvalues.\n\n- For$A$ as a scalar multiple of $k$:\n\n  $$\n  A\\mathbf{v} = k \\lambda \\mathbf{v} \\text{ for eigenvalue } \\lambda.\n  $$\n\n:::\n###  Eigenvalue Decomposition (Spectral Decomposition)\nFor matrices that are diagonalizable (including symmetric matrices), spectral decomposition expresses the matrix as a combination of its eigenvalues and eigenvectors. Specifically, for a matrix $A$, spectral decomposition is represented as:\n\n$$\nA = V \\Lambda V^{-1}\n$$\n\nwhere:\n- $V$ is the matrix of eigenvectors of $A$,\n- $\\Lambda$ is a diagonal matrix of eigenvalues of $A$,\n- $V^{-1}$ is the inverse of the matrix of eigenvectors (if $V$ is invertible).\n\nFor symmetric matrices $A$, the decomposition becomes simpler:\n\n$$\nA = Q \\Lambda Q^\\top\n$$\n\nHere, $Q$ is an orthogonal matrix of eigenvectors (i.e., $Q^\\top Q = I$), and $\\Lambda$ is a diagonal matrix of eigenvalues.\n\n###  Geometric Interpretation\nEigenvalues and eigenvectors provide insights into the geometry of linear transformations represented by matrices. Eigenvectors represent directions that remain invariant under the transformation, while eigenvalues indicate how these directions are stretched or compressed.\n\nFor example, in the case of a transformation matrix that scales or rotates data points, eigenvalues show the magnitude of scaling along the principal axes (directions defined by eigenvectors).\n\n###  Importance of Diagonalization\nThe key advantage of spectral decomposition is that it simplifies matrix operations. When a matrix is diagonalized as $A = Q \\Lambda Q^\\top$, any function of the matrix $A$ (such as powers, exponentials, or inverses) can be easily computed by operating on the diagonal matrix $\\Lambda$. For example:\n\n$$\nA^k = Q \\Lambda^k Q^\\top\n$$\n\nSince $\\Lambda$ is diagonal, raising $\\Lambda$ to any power $k$ is straightforward, involving only raising each eigenvalue to the power $k$.\n\n###  Properties of Symmetric Matrices\nSpectral decomposition applies particularly well to symmetric matrices, which satisfy $A = A^\\top$. Symmetric matrices have the following key properties:\n\n- **Real eigenvalues**: The eigenvalues of a symmetric matrix are always real numbers.\n\n- **Orthogonal eigenvectors**: The eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal to each other.\n\n- **Diagonalizability**: Every symmetric matrix can be diagonalized by an orthogonal matrix.\n\nThese properties make symmetric matrices highly desirable in computational applications.\n\n## Mathematical Requirements for Spectral Decomposition\n\n###  Determining Eigenvalues and Eigenvectors\n\nThe eigenvalues of a matrix $A$ are the solutions to the characteristic equation:\n\n$$\n\\text{det}(A - \\lambda I) = 0\n$$\n\nHere, $I$ is the identity matrix, and $\\lambda$ represents the eigenvalues. Solving this polynomial equation provides the eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$. Once the eigenvalues are determined, the eigenvectors can be computed by solving the equation $(A - \\lambda I)v = 0$ for each eigenvalue.\n\n### Characteristic Polynomial of $2 \\times 2$ Matrices\n\nFor a $2 \\times 2$ matrix:\n$$\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n$$\nthe characteristic polynomial is derived from the determinant of $A - \\lambda I$, where $I$ is the identity matrix:\n\n$$\n\\det(A - \\lambda I) = 0\n$$\n\nThis leads to:\n$$\n\\det\\begin{pmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{pmatrix} = (a - \\lambda)(d - \\lambda) - bc = 0\n$$\n\n\\textbf{Short-cut Method:} The characteristic polynomial can be simplified to:\n$$\n\\lambda^2 - (a + d)\\lambda + (ad - bc) = 0\n$$\n\nThis polynomial can be solved using the quadratic formula:\n$$\n\\lambda = \\frac{(a + d) \\pm \\sqrt{(a + d)^2 - 4(ad - bc)}}{2}\n$$\n\n:::{.callout-important}\n## Shortcut to write Characteristic polynomial of a $2\\times 2$ matrix\n\nIf $A=\\begin{bmatrix} a & b\\\\ c& d\\end{bmatrix}$, then the characteristic polynomial is\n$$\\lambda^2-(\\text{Trace}(A))\\lambda+det(A)=0$$\n\nEigen vectors can be found by using the formula:\n\\begin{equation*}\nEV(\\lambda=\\lambda_1)=\\begin{bmatrix}\\lambda_1-d\\\\ c\\end{bmatrix}\n\\end{equation*}\n\n:::\n\n### Problems\n\n**Example 1:** Find Eigenvalues and Eigenvectors of the matrix, $$A = \\begin{pmatrix} 3 & 2 \\\\ 4 & 1 \\end{pmatrix}$$\n\n*Solution:*\n\nThe characteristic equation is given by $$det(A-\\lambda I)=0$$\n\n\\begin{align*}\n\\lambda^2 - 4\\lambda - 5 &= 0\\\\\n(\\lambda-5)(\\lambda+1)&=0\\\\\n\\end{align*}\n\nHence the eigen values are $\\lambda_1=5,\\quad \\lambda_2=-1$.\n\nSo the eigen vectors are:\n\\begin{align*}\nEV(\\lambda=\\lambda_1)&=\\begin{bmatrix}\\lambda_1-d\\\\ c\\end{bmatrix}\\\\\n\\therefore EV(\\lambda=5)&=\\begin{bmatrix}4\\\\ 4\\end{bmatrix}=\\begin{bmatrix}1\\\\ 1\\end{bmatrix}\\\\\n\\therefore EV(\\lambda=-1)&=\\begin{bmatrix}-2\\\\ 4\\end{bmatrix}=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}\n\\end{align*}\n\n**Problem 2:** Calculate the eigenvalues and eigenvectors of the matrix: $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n\n*Solution:*\n\nTo find the eigenvalues and eigenvectors of a $2 \\times 2$ matrix, we can use the shortcut formula for the characteristic polynomial:\n\n$$\n\\lambda^2 - \\text{trace}(A)\\lambda + \\det(A) = 0,\n$$\n\nwhere $A$ is the matrix. Let's apply this to the matrix \n\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\n\nFirst, we calculate the trace and determinant of $A$:\n\n- The trace is the sum of the diagonal elements:\n\n$$\n\\text{trace}(A) = 2 + 2 = 4.\n$$\n\n- The determinant is calculated as follows:\n\n$$\n\\det(A) = (2)(2) - (1)(1) = 4 - 1 = 3.\n$$\n\nNext, substituting the trace and determinant into the characteristic polynomial gives:\n\n$$\n\\lambda^2 - (4)\\lambda + 3 = 0,\n$$\n\nwhich simplifies to:\n\n$$\n\\lambda^2 - 4\\lambda + 3 = 0.\n$$\n\nWe can factor this quadratic equation:\n\n$$\n(\\lambda - 1)(\\lambda - 3) = 0.\n$$\n\nSetting each factor to zero gives the eigenvalues:\n\n$$\n\\lambda_1 = 1, \\quad \\lambda_2 = 3.\n$$\n\nTo find the eigenvectors corresponding to each eigenvalue, we use the shortcut for the eigenvector of a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$:\n\n$$\nEV(\\lambda) = \\begin{pmatrix} \\lambda - d \\\\ c \\end{pmatrix}.\n$$\n\nFor the eigenvalue $\\lambda_1 = 1$:\n\n$$\nEV(1) = \\begin{pmatrix} 1 - 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}.\n$$\n\nThis eigenvector can be simplified (up to a scalar multiple) to:\n\n$$\n\\mathbf{v_1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\n\nFor the eigenvalue $\\lambda_2 = 3$:\n\n$$\nEV(3) = \\begin{pmatrix} 3 - 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\nThis eigenvector is already in a simple form:\n\n$$\n\\mathbf{v_2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n**Problem 3:** For the matrix:\n    $A = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$, find the eigenvalues and eigenvectors.\n\n*Solution:*\n\nWe are given the matrix \n$$\nA = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}\n$$\n\nand we aim to find its eigenvalues using the characteristic polynomial.\n\nThe shortcut formula for the characteristic polynomial of a $3 \\times 3$ matrix is given by:\n$$\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors of } A)\\lambda - \\det(A) = 0.\n$$\n\n\nThe trace of a matrix is the sum of its diagonal elements. For matrix $A$, we have:\n$$\n\\text{tr}(A) = 1 + 1 + 1 = 3.\n$$\n\n\nThe principal minors are the determinants of the $2 \\times 2$ submatrices obtained by deleting one row and one column of $A$.\n\n The first minor is obtained by deleting the third row and third column:\n$$\n\\det\\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = (1)(1) - (2)(0) = 1.\n$$\n\n The second minor is obtained by deleting the second row and second column:\n$$\n\\det\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = (1)(1) - (1)(1) = 0.\n$$\n\nThe third minor is obtained by deleting the first row and first column:\n$$\n\\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = (1)(1) - (0)(0) = 1.\n$$\n\nThus, the sum of the principal minors is:\n$$\n1 + 0 + 1 = 2.\n$$\n\n\nThe determinant of $A$ can be calculated using cofactor expansion along the first row:\n$$\n\\det(A) = 1 \\cdot \\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix} + 1 \\cdot \\det\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\n= 1 \\cdot (1) - 2 \\cdot (0) + 1 \\cdot (-1) = 1 - 0 - 1 = 0.\n$$\n\n\nNow, we substitute these values into the characteristic polynomial formula:\n$$\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0\n$$\n$$\n\\lambda^3 - 3\\lambda^2 + 2\\lambda - 0 = 0.\n$$\n\n\nWe now solve the equation:\n$$\n\\lambda^3 - 3\\lambda^2 + 2\\lambda = 0.\n$$\nFactoring out $\\lambda$ and apply factor theorem, we get:\n\n\\begin{align*}\n   \\lambda(\\lambda^2 - 3\\lambda + 2) &= 0\\\\\n   \\lambda(\\lambda-2)(\\lambda-1)&=0\n\\end{align*}\n\nThis gives one eigenvalue:\n$$\n\\lambda_1 = 0;\\quad \\lambda_2=2;\\quad \\lambda_3=1\n$$\n\n\nNow we find the eigenvectors corresponding to each eigenvalue.\n\n For $\\lambda_1 = 0$, solve $(A - 0I)\\mathbf{v} = 0$:\n$$\n\\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis gives the system:\n$$\nx + 2y + z = 0, \\quad y = 0, \\quad x + z = 0.\n$$\nThus, $x = -z$, and the eigenvector is:\n$$\n\\mathbf{v}_1 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\n For $\\lambda_2 = 2$, solve $(A - 2I)\\mathbf{v} = 0$:\n$$\n\\begin{pmatrix} -1 & 2 & 1 \\\\ 0 & -1 & 0 \\\\ 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis gives the system:\n$$\n-x + 2y + z = 0, \\quad -y = 0, \\quad x - z = 0.\n$$\nThus, $x = z$, and the eigenvector is:\n$$\n\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nFor $\\lambda_3 = 1$, solve $(A - I)\\mathbf{v} = 0$:\n$$\n\\begin{pmatrix} 0 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis gives the system:\n$$\n2y + z = 0, \\quad x = 0.\n$$\nThus, $z = -2y$, and the eigenvector is:\n$$\n\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix}.\n$$\n\n**Problem 3:** If $A=\\begin{bmatrix}1&2&4\\\\ 0&3&4\\\\ 1&-1&-1 \\end{bmatrix}$, compute the eigen values and eigen vectors and left eigen vectors of $A$.\n\n*Solution:*\n\nWe are given the matrix\n$$\nA = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix}\n$$\n\nand need to find its eigenvalues and eigenvectors.\n\n\n\nThe characteristic polynomial for a $3 \\times 3$ matrix is given by:\n$$\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0.\n$$\n\nThe trace is the sum of the diagonal elements:\n$$\n\\text{tr}(A) = 1 + 3 + (-1) = 3.\n$$\n\n\nWe now compute the $2 \\times 2$ principal minors:\n\n- Minor by removing the third row and third column:\n$$\n\\det\\begin{pmatrix} 1 & 2 \\\\ 0 & 3 \\end{pmatrix} = (1)(3) - (2)(0) = 3.\n$$\n\n- Minor by removing the second row and second column:\n$$\n\\det\\begin{pmatrix} 1 & 4 \\\\ 1 & -1 \\end{pmatrix} = (1)(-1) - (4)(1) = -1 - 4 = -5.\n$$\n\n- Minor by removing the first row and first column:\n$$\n\\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} = (3)(-1) - (4)(-1) = -3 + 4 = 1.\n$$\n\nThus, the sum of the principal minors is:\n$$\n3 + (-5) + 1 = -1.\n$$\n\n\nWe calculate the determinant of $A$ by cofactor expansion along the first row:\n$$\n\\det(A) = 1 \\cdot \\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 0 & 4 \\\\ 1 & -1 \\end{pmatrix} + 4 \\cdot \\det\\begin{pmatrix} 0 & 3 \\\\ 1 & -1 \\end{pmatrix}.\n$$\nThe $2 \\times 2$ determinants are:\n$$\n\\det\\begin{pmatrix} 3 & 4 \\\\ -1 & -1 \\end{pmatrix} = -3 + 4 = 1, \\quad \\det\\begin{pmatrix} 0 & 4 \\\\ 1 & -1 \\end{pmatrix} = -4,\n$$\n$$\n\\det\\begin{pmatrix} 0 & 3 \\\\ 1 & -1 \\end{pmatrix} = -3.\n$$\n\nThus:\n$$\n\\det(A) = 1 \\cdot 1 - 2 \\cdot (-4) + 4 \\cdot (-3) = 1 + 8 - 12 = -3.\n$$\n\n\nSubstituting into the characteristic polynomial:\n$$\n\\lambda^3 - \\text{tr}(A)\\lambda^2 + (\\text{sum of principal minors})\\lambda - \\det(A) = 0,\n$$\n\nwe get:\n$$\n\\lambda^3 - 3\\lambda^2 - \\lambda + 3 = 0.\n$$\n\n\nWe now solve the cubic equation:\n\\begin{align*}\n   \\lambda^3 - 3\\lambda^2 - \\lambda + 3& = 0. \\\\\n   (\\lambda-1)(\\lambda+1)(\\lambda -3)&=0\n\\end{align*}\n\n$$\\lambda_1 = 1, \\quad \\lambda_2 = -1, \\quad \\lambda_3 = 3.$$\n\n\nTo find the eigenvector corresponding to $\\lambda_1 = 3$, solve $(A - 3I)\\mathbf{v} = 0$:\n$$\nA - 3I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} - 3\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -2 & 2 & 4 \\\\ 0 & 0 & 4 \\\\ 1 & -1 & -4 \\end{pmatrix}.\n$$\n\nSolving this system gives the eigenvector:\n$$\n\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n\n\nFor $\\lambda_2 = -1$, solve $(A +I)\\mathbf{v} = 0$:\n$$\nA +I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} +\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 4 \\\\ 0 & 4 & 4 \\\\ 1 & -1 & 0 \\end{pmatrix}.\n$$\n\nNote that the third row is depending on first and second rows. So by finding the cross product of first two rows,\n\n$$\n\\mathbf{v}_2 = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n$$\n\nFor $\\lambda_3 = 1$, solve $(A -I)\\mathbf{v} = 0$:\n$$\nA - I = \\begin{pmatrix} 1 & 2 & 4 \\\\ 0 & 3 & 4 \\\\ 1 & -1 & -1 \\end{pmatrix} -\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 2 & 4 \\\\ 0 & 2 & 4 \\\\ 1 & -1 & -2\\end{pmatrix}.\n$$\n\nNote that the second row is same as first row. So by finding the cross product of first and third rows,\n$$\n\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ -2 \\\\ 1 \\end{pmatrix}.\n$$\n\nThus, the eigenvalues of the matrix are:\n$$\n\\lambda_1 = 3, \\quad \\lambda_2 = -1, \\quad \\lambda_3 = 1\n$$\n\nwith corresponding eigenvectors $\\mathbf{v}_1=\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{v}_2=\\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}$, and $\\mathbf{v}_3=\\begin{pmatrix} 0 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\n\nLeft eigen vectors of the matrix $A$ are eigen vectors of $A^T$.\n\nHere $A^T=\\begin{bmatrix}\n    1&0&1\\\\ 2&3&-1\\\\ 4&4&-1\n\\end{bmatrix}$.\n\nSince $A$ and $A^T$ have same eigen values, it is enough to find corresponding eigen vectors.\nWhen $\\lambda=3$, the coefficient matrix of $(A-\\lambda I)X=0$ reduced into $\\begin{bmatrix}\n    -2&0&1\\\\ 2&0&-1\\\\ 4&4&-4\n\\end{bmatrix}$\n\nHere the only independent rows are first and last. So the eigen vector can be found as the cross product of these two rows.\n$\\therefore v_1=\\begin{bmatrix}\n    1\\\\1\\\\2\n\\end{bmatrix}$.\n\nWhen $\\lambda=-1$, the coefficient matrix of $(A-\\lambda I)X=0$ reduced into $\\begin{bmatrix}\n    2&0&1\\\\ 2&4&-1\\\\ 4&4&0\n\\end{bmatrix}$\n\nHere the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows.\n$\\therefore v_2=\\begin{bmatrix}\n    -1\\\\1\\\\2\n\\end{bmatrix}$.\nWhen $\\lambda=1$, the coefficient matrix of $(A-\\lambda I)X=0$ reduced into $\\begin{bmatrix}\n    0&0&1\\\\ 2&2&-1\\\\ 4&4&-2\n\\end{bmatrix}$\n\nHere the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows.\n$\\therefore v_2=\\begin{bmatrix}\n    -1\\\\1\\\\0\n\\end{bmatrix}$.\n\n\n### `Python` code to find eigen values and eigen vectors\n\n1. Find eigen values and eigen vectors of $A=\\begin{bmatrix} 2&1\\\\ 1&2\\end{bmatrix}$.\n\n::: {#98aed40b .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.linalg import null_space\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Find eigenvalues\neigenvalues, _ = np.linalg.eig(A)\n\n# Define identity matrix I\nI = np.eye(A.shape[0])\n\n# Iterate over eigenvalues to find corresponding eigenvectors\nfor i, eigenvalue in enumerate(eigenvalues):\n    # Compute A - lambda * I\n    A_lambda_I = A - eigenvalue * I\n    \n    # Find the null space (which gives the eigenvector)\n    eig_vector = null_space(A_lambda_I)\n    \n    print(f\"Eigenvalue {i+1}: {eigenvalue}\")\n    print(f\"Eigenvector {i+1}:\\n{eig_vector}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEigenvalue 1: 3.0\nEigenvector 1:\n[[0.70710678]\n [0.70710678]]\n\nEigenvalue 2: 1.0\nEigenvector 2:\n[[-0.70710678]\n [ 0.70710678]]\n\n```\n:::\n:::\n\n\nSame can be done using direct approach. Code for this task is given below.\n\n::: {#a5231dfb .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Find eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Display the results\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\\n\", eigenvectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEigenvalues: [3. 1.]\nEigenvectors:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n```\n:::\n:::\n\n\n###  Diagonalization of Symmetric Matrices\n\nFor a symmetric matrix $A$, the process of diagonalization can be summarized as follows:\n\n1. **Compute eigenvalues**: Solve the characteristic equation $\\text{det}(A - \\lambda I) = 0$ to find the eigenvalues.\n\n2. **Find eigenvectors**: For each eigenvalue $\\lambda_i$, solve $(A - \\lambda_i I)v_i = 0$ to find the corresponding eigenvector $v_i$.\n\n3. **Form the eigenvector matrix**: Arrange the eigenvectors into a matrix $Q$, with each eigenvector as a column.\n\n4. **Form the diagonal matrix of eigenvalues**: Construct $\\Lambda$ by placing the eigenvalues along the diagonal of the matrix.\n\nThus, the matrix can be expressed as $A = Q \\Lambda Q^\\top$.\n\n1. Diagonalize the matrix $A=\\begin{bmatrix} 2&1\\\\ 1&2\\end{bmatrix}$.\n\n`Python` code for this task is given below.\n\n::: {#b3905548 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define matrix A\nA = np.array([[2, 1], \n              [1, 2]])\n\n# Step 1: Find eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Step 2: Construct the diagonal matrix D (eigenvalues)\nD = np.diag(eigenvalues)\n\n# Step 3: Construct the matrix P (eigenvectors)\nP = eigenvectors\n\n# Step 4: Calculate the inverse of P\nP_inv = np.linalg.inv(P)\n\n# Verify the diagonalization: A = P D P_inv\nA_reconstructed = P @ D @ P_inv\n\nprint(\"Matrix A:\")\nprint(A)\n\nprint(\"\\nEigenvalues (Diagonal matrix D):\")\nprint(D)\n\nprint(\"\\nEigenvectors (Matrix P):\")\nprint(P)\n\nprint(\"\\nInverse of P:\")\nprint(P_inv)\n\nprint(\"\\nReconstructed matrix A (P D P^(-1)):\")\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[2 1]\n [1 2]]\n\nEigenvalues (Diagonal matrix D):\n[[3. 0.]\n [0. 1.]]\n\nEigenvectors (Matrix P):\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\nInverse of P:\n[[ 0.70710678  0.70710678]\n [-0.70710678  0.70710678]]\n\nReconstructed matrix A (P D P^(-1)):\n[[2. 1.]\n [1. 2.]]\n```\n:::\n:::\n\n\n###  Matrix Functions and Spectral Theorem\n\nOnce a matrix is diagonalized, various matrix functions become easier to compute. For a function $f(A)$, such as the exponential of a matrix or any power, the function can be applied to the diagonal matrix of eigenvalues:\n\n$$\nf(A) = Q f(\\Lambda) Q^\\top\n$$\n\nwhere $f(\\Lambda)$ is the function applied element-wise to the eigenvalues in the diagonal matrix $\\Lambda$.\n\n\n# QR Decomposition\n\nThe **QR decomposition** of a matrix is a factorization technique that expresses a matrix $A$ as the product of an orthogonal matrix $Q$ and an upper triangular matrix $R$:\n\n$$\nA = QR\n$$\n\nwhere:\n- $Q$ is an orthogonal matrix ($Q^T Q = I$), meaning its columns are orthonormal vectors.\n- $R$ is an upper triangular matrix.\n\n:::{.callout-note}\n### Properties\n1. **Orthogonality**: The columns of $Q$ are orthonormal, which implies that $Q^T = Q^{-1}$.\n2. **Uniqueness**: The QR decomposition is unique if the columns of $A$ are linearly independent.\n\n:::\n\n:::{.callout-tip}\n### Relation with Other Decompositions\n\n1. **LU Decomposition**:\n   - **LU decomposition** factors a matrix $A$ into a lower triangular matrix $L$ and an upper triangular matrix $U$:\n   $$\n   A = LU\n   $$\n   - Unlike QR, LU decomposition does not require the columns of $A$ to be orthogonal.\n   - QR decomposition is often used when $A$ is not square or when numerical stability is a concern, as it can be computed using Gram-Schmidt or Householder reflections.\n\n2. **Cholesky Decomposition (CR)**:\n   - The **Cholesky decomposition** is a specific case of LU decomposition applicable to symmetric positive-definite matrices:\n   $$\n   A = LL^T\n   $$\n   - It is more efficient than LU decomposition for suitable matrices but does not provide orthogonality like QR.\n\n3. **Spectral Decomposition**:\n   - The **spectral decomposition** expresses a symmetric matrix $A$ in terms of its eigenvalues and eigenvectors:\n   $$\n   A = Q \\Lambda Q^T\n   $$\n   - While QR decomposition provides an orthogonal basis for any matrix, spectral decomposition is specifically used for symmetric matrices, providing insights into the matrix's properties through its eigenvalues and eigenvectors.\n\n4. **Singular Value Decomposition (SVD)**:\n   - The **SVD** decomposes a matrix $A$ into three matrices:\n   $$\n   A = U \\Sigma V^T\n   $$\n   - $U$ and $V$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix of singular values. \n   - SVD is more general than QR and is particularly useful in applications involving rank-deficient matrices, dimensionality reduction, and noise reduction.\n\n\n:::\n\n### Practical Uses of QR Decomposition\n\n1. **Solving Linear Systems**:\n   - QR decomposition is used to solve linear systems of equations, especially over-determined systems where there are more equations than unknowns. The least squares solution can be efficiently obtained via QR.\n\n2. **Eigenvalue Problems**:\n   - QR algorithms are often used in iterative methods for finding eigenvalues and eigenvectors of matrices, especially for large matrices.\n\n3. **Numerical Stability**:\n   - QR decomposition is numerically stable, making it suitable for computations involving floating-point arithmetic, particularly when dealing with ill-conditioned matrices.\n\n4. **Computer Graphics**:\n   - In computer graphics, QR decomposition can be used in perspective projection, where 3D points are projected onto a 2D plane, often needing orthogonal transformations.\n\n5. **Signal Processing**:\n   - In signal processing, QR decomposition is utilized in adaptive filtering algorithms and for solving problems related to estimation theory.\n\n### `Python` method for DR decomposition\n\n\n1. Find the QR decomposition of the matrix, $A=\\begin{bmatrix}12&-51&4\\\\ 6&167&-68\\\\ -4&24&-41\\end{bmatrix}$. Verify the decomposition using the reconstruction.\n\n::: {#99aa0b78 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define  A\nA = np.array([[12, -51, 4],\n              [6, 167, -68],\n              [-4, 24, -41]])\n\n# Perform QR decomposition\nQ, R = np.linalg.qr(A)\n\n# Display the results\nprint(\"Matrix A:\")\nprint(A)\n\nprint(\"\\nOrthogonal Matrix Q:\")\nprint(Q)\n\nprint(\"\\nUpper Triangular Matrix R:\")\nprint(R)\n\n# Verify the decomposition\nprint(\"\\nVerification (Q @ R):\")\nprint(np.dot(Q, R))\n\n# Check if Q is orthogonal (Q^T @ Q should be the identity matrix)\nprint(\"\\nQ^T @ Q (should be identity):\")\nprint(np.dot(Q.T, Q))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[ 12 -51   4]\n [  6 167 -68]\n [ -4  24 -41]]\n\nOrthogonal Matrix Q:\n[[-0.85714286  0.39428571  0.33142857]\n [-0.42857143 -0.90285714 -0.03428571]\n [ 0.28571429 -0.17142857  0.94285714]]\n\nUpper Triangular Matrix R:\n[[ -14.  -21.   14.]\n [   0. -175.   70.]\n [   0.    0.  -35.]]\n\nVerification (Q @ R):\n[[ 12. -51.   4.]\n [  6. 167. -68.]\n [ -4.  24. -41.]]\n\nQ^T @ Q (should be identity):\n[[ 1.00000000e+00 -5.04131884e-17 -3.39864191e-17]\n [-5.04131884e-17  1.00000000e+00  2.30881074e-17]\n [-3.39864191e-17  2.30881074e-17  1.00000000e+00]]\n```\n:::\n:::\n\n\n## Overdetermined Systems\n\nAn **overdetermined system** of linear equations is a system in which there are more equations than unknowns. Mathematically, if we have a matrix $A$ of size $m \\times n$ where $m > n$, the system can be represented as:\n\n$$\nA \\mathbf{x} = \\mathbf{b}\n$$\n\nwhere:\n- $A$ is the coefficient matrix,\n- $\\mathbf{x}$ is the vector of unknowns (with size $n$),\n- $\\mathbf{b}$ is the vector of constants (with size $m$).\n\n### Example of an Overdetermined System\n\nConsider the following system of equations:\n\n\\begin{align*}\n2x_1 + 3x_2 &= 5 \\\\\n4x_1 + 6x_2 &= 10 \\\\\n1x_1 + 2x_2 &= 3 \\\\\n\\end{align*}\n\n\n\nHere, we have three equations but only two unknowns ($x_1$ and $x_2$). This system is overdetermined.\n\n### Challenges in Solving Overdetermined Systems\n\n1. **No Exact Solutions**: In most cases, an overdetermined system does not have an exact solution because the equations may be inconsistent. For example, if one equation contradicts another, no value of $\\mathbf{x}$ can satisfy all equations simultaneously.\n\n2. **Finding Best Approximation**: When the system is consistent, it may still be that no single solution satisfies all equations perfectly. Therefore, the goal is often to find an approximate solution that minimizes the error.\n\n### Why We Need QR Decomposition\n\nQR decomposition is particularly useful for solving overdetermined systems for the following reasons:\n\n1. **Least Squares Solution**:\n   - The primary goal in solving an overdetermined system is to find the least squares solution, which minimizes the sum of the squared residuals (the differences between the left and right sides of the equations). QR decomposition allows us to efficiently compute this solution.\n\n2. **Orthogonality**:\n   - The QR decomposition expresses the matrix $A$ as the product of an orthogonal matrix $Q$ and an upper triangular matrix $R$:\n   $$\n   A = QR\n   $$\n   - The orthogonality of $Q$ ensures numerical stability and helps in reducing the problem to solving triangular systems.\n\n3. **Stability**:\n   - QR decomposition is more stable than other methods, such as Gaussian elimination, especially when dealing with ill-conditioned matrices. This is crucial in applications where precision is important.\n\n4. **Computational Efficiency**:\n   - The process of obtaining the QR decomposition can be performed using efficient algorithms, such as Gram-Schmidt orthogonalization or Householder reflections, which makes it suitable for large systems.\n\n### Solving an Overdetermined System using QR Decomposition\n\nGiven an overdetermined system represented as $A \\mathbf{x} = \\mathbf{b}$, the steps to find the least squares solution using QR decomposition are as follows:\n\n1. **Compute QR Decomposition**:\n   - Decompose the matrix $A$ into $Q$ and $R$.\n\n2. **Formulate the Normal Equations**:\n   - The least squares solution can be found from the equation:\n   $$\n   R \\mathbf{x} = Q^T \\mathbf{b}\n   $$\n\n3. **Solve the Triangular System**:\n   - Solve for $\\mathbf{x}$ using back substitution, as $R$ is an upper triangular matrix.\n\n\n`Python` code for solving the above system of equations is given below.\n\n::: {#7d5295e1 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the coefficient matrix A and the constant vector b\nA = np.array([[2, 3],\n              [4, 6],\n              [1, 2]])\n\nb = np.array([5, 10, 3])\n\n# Perform QR decomposition\nQ, R = np.linalg.qr(A)\n\n# Calculate the least squares solution\n# Solve the equation R * x = Q^T * b\nQ_b = np.dot(Q.T, b)\nx = np.linalg.solve(R, Q_b)\n\nprint(\"The least squares solution is:\")\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe least squares solution is:\n[1. 1.]\n```\n:::\n:::\n\n\n### Problems\n\n> Problem 1: Simple Overdetermined System\n\n**Problem Statement:**  \nSolve the overdetermined system given by the equations:\n\n\n\\begin{align*}\n2x + 3y &= 5 \\\\\n4x + 6y &= 10 \\\\\n1x + 2y &= 2\n\\end{align*}\n\n::: {#fe98e81a .cell execution_count=8}\n``` {.python .cell-code}\n# Problem 1: Simple Overdetermined System\n\nimport numpy as np\n\n# Define the coefficient matrix A and the vector b\nA1 = np.array([[2, 3],\n               [4, 6],\n               [1, 2]])\n\nb1 = np.array([5, 10, 2])\n\n# QR decomposition\nQ1, R1 = np.linalg.qr(A1)\nx_qr1 = np.linalg.solve(R1, Q1.T @ b1)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols1, residuals1, rank1, s1 = np.linalg.lstsq(A1, b1, rcond=None)\n\nprint(\"Problem 1 - QR Decomposition Solution:\", x_qr1)\nprint(\"Problem 1 - Ordinary Least Squares Solution:\", x_ols1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem 1 - QR Decomposition Solution: [ 4. -1.]\nProblem 1 - Ordinary Least Squares Solution: [ 4. -1.]\n```\n:::\n:::\n\n\n>Problem 2: Overdetermined System with No Exact Solution\n\n**Problem Statement:**  \nSolve the following system:\n\n\n\\begin{align*}\nx + 2y &= 3 \\\\\n2x + 4y &= 6 \\\\\n3x + 1y &= 5\n\\end{align*}\n\n::: {#133ad404 .cell execution_count=9}\n``` {.python .cell-code}\n# Problem 2: Overdetermined System with No Exact Solution\n\n# Define the coefficient matrix A and the vector b\nA2 = np.array([[1, 2],\n               [2, 4],\n               [3, 1]])\n\nb2 = np.array([3, 6, 5])\n\n# QR decomposition\nQ2, R2 = np.linalg.qr(A2)\nx_qr2 = np.linalg.solve(R2, Q2.T @ b2)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols2, residuals2, rank2, s2 = np.linalg.lstsq(A2, b2, rcond=None)\n\nprint(\"Problem 2 - QR Decomposition Solution:\", x_qr2)\nprint(\"Problem 2 - Ordinary Least Squares Solution:\", x_ols2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem 2 - QR Decomposition Solution: [1.4 0.8]\nProblem 2 - Ordinary Least Squares Solution: [1.4 0.8]\n```\n:::\n:::\n\n\n> Problem 3: Overdetermined System with Random Data\n\n**Problem Statement:**  \nGenerate a random overdetermined system and solve it:\n\n$$\nAx = b\n$$\n\nWhere \\(A\\) is a random \\(6 \\times 3\\) matrix and \\(b\\) is generated accordingly.\n\n::: {#1a0f2ba9 .cell execution_count=10}\n``` {.python .cell-code}\n# Problem 3: Overdetermined System with Random Data\n\n# Generate a random overdetermined system\nnp.random.seed(0)  # For reproducibility\nA3 = np.random.rand(6, 3)\nx_true = np.array([1, 2, 3])  # True solution\nb3 = A3 @ x_true + np.random.normal(0, 0.1, 6)  # Adding some noise\n\n# QR decomposition\nQ3, R3 = np.linalg.qr(A3)\nx_qr3 = np.linalg.solve(R3, Q3.T @ b3)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols3, residuals3, rank3, s3 = np.linalg.lstsq(A3, b3, rcond=None)\n\nprint(\"Problem 3 - QR Decomposition Solution:\", x_qr3)\nprint(\"Problem 3 - Ordinary Least Squares Solution:\", x_ols3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem 3 - QR Decomposition Solution: [0.94791379 2.10331498 2.98999875]\nProblem 3 - Ordinary Least Squares Solution: [0.94791379 2.10331498 2.98999875]\n```\n:::\n:::\n\n\n> Problem 4: Real-World Data Fitting\n\n**Problem Statement:**  \nFit a linear model to the following data points:\n\n\n\\begin{align*}\n(1, 2), (2, 3), (3, 5), (4, 7), (5, 11)\n\\end{align*}\n\n::: {#01383219 .cell execution_count=11}\n``` {.python .cell-code}\n# Problem 4: Real-World Data Fitting\n\n# Data points\nx_data = np.array([1, 2, 3, 4, 5])\ny_data = np.array([2, 3, 5, 7, 11])\n\n# Create the design matrix A\nA4 = np.vstack([x_data, np.ones(len(x_data))]).T  # Add intercept\n\n# QR decomposition\nQ4, R4 = np.linalg.qr(A4)\nx_qr4 = np.linalg.solve(R4, Q4.T @ y_data)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols4, residuals4, rank4, s4 = np.linalg.lstsq(A4, y_data, rcond=None)\n\nprint(\"Problem 4 - QR Decomposition Solution:\", x_qr4)\nprint(\"Problem 4 - Ordinary Least Squares Solution:\", x_ols4)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem 4 - QR Decomposition Solution: [ 2.2 -1. ]\nProblem 4 - Ordinary Least Squares Solution: [ 2.2 -1. ]\n```\n:::\n:::\n\n\n>Problem 5: Polynomial Fit (Higher Degree)\n\n**Problem Statement:**  \nFit a quadratic polynomial to the following data points:\n\n\n\\begin{align*}\n(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)\n\\end{align*}\n\n::: {#decf2005 .cell execution_count=12}\n``` {.python .cell-code}\n# Problem 5: Polynomial Fit (Higher Degree)\n\n# Data points for polynomial fitting\nx_data_poly = np.array([1, 2, 3, 4, 5])\ny_data_poly = np.array([1, 4, 9, 16, 25])\n\n# Create the design matrix for a quadratic polynomial\nA5 = np.vstack([x_data_poly**2, x_data_poly, np.ones(len(x_data_poly))]).T\n\n# QR decomposition\nQ5, R5 = np.linalg.qr(A5)\nx_qr5 = np.linalg.solve(R5, Q5.T @ y_data_poly)\n\n# Ordinary Least Squares solution using np.linalg.lstsq\nx_ols5, residuals5, rank5, s5 = np.linalg.lstsq(A5, y_data_poly, rcond=None)\n\nprint(\"Problem 5 - QR Decomposition Solution:\", x_qr5)\nprint(\"Problem 5 - Ordinary Least Squares Solution:\", x_ols5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem 5 - QR Decomposition Solution: [ 1.00000000e+00 -2.73316113e-15  3.80986098e-15]\nProblem 5 - Ordinary Least Squares Solution: [ 1.00000000e+00 -6.77505297e-15  1.06049542e-14]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "module_4_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}