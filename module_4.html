<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Linear Algebra for Advanced Applications – Computational Linear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./module_5.html" rel="next">
<link href="./module_3.html" rel="prev">
<link href="./CME_logo.JPG" rel="icon" type="image/jpeg">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./module_4.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra for Advanced Applications</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Linear Algebra</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sijuswamy/Computational-Linear-Algebra" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Computational-Linear-Algebra.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Computational-Linear-Algebra.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Python for Linear Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Transforming Linear Algebra to Computational Language</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python Libraries for Computational Linear Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra for Advanced Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module_5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Practical Uses Cases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="CME_logo.JPG" class="img-fluid figure-img"></p>
<figcaption>Computational Mathematics</figcaption>
</figure>
</div>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
  <li><a href="#lu-decomposition" id="toc-lu-decomposition" class="nav-link" data-scroll-target="#lu-decomposition"><span class="header-section-number">4.2</span> LU Decomposition</a>
  <ul class="collapse">
  <li><a href="#step-by-step-procedure" id="toc-step-by-step-procedure" class="nav-link" data-scroll-target="#step-by-step-procedure"><span class="header-section-number">4.2.1</span> Step-by-Step Procedure</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">4.2.2</span> Example</a></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation"><span class="header-section-number">4.2.3</span> Python Implementation</a></li>
  <li><a href="#lu-decomposition-practice-problems-with-solutions" id="toc-lu-decomposition-practice-problems-with-solutions" class="nav-link" data-scroll-target="#lu-decomposition-practice-problems-with-solutions"><span class="header-section-number">4.2.4</span> LU Decomposition Practice Problems with Solutions</a></li>
  </ul></li>
  <li><a href="#lu-decomposition-practice-problems" id="toc-lu-decomposition-practice-problems" class="nav-link" data-scroll-target="#lu-decomposition-practice-problems"><span class="header-section-number">4.3</span> LU Decomposition Practice Problems</a></li>
  <li><a href="#matrix-approach-to-create-lu-decomposition" id="toc-matrix-approach-to-create-lu-decomposition" class="nav-link" data-scroll-target="#matrix-approach-to-create-lu-decomposition"><span class="header-section-number">4.4</span> Matrix Approach to Create LU Decomposition</a></li>
  <li><a href="#spectral-decomposition" id="toc-spectral-decomposition" class="nav-link" data-scroll-target="#spectral-decomposition"><span class="header-section-number">5</span> Spectral Decomposition</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">5.1</span> Background</a></li>
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">5.2</span> Introduction</a></li>
  <li><a href="#spectral-decomposition-detailed-concepts" id="toc-spectral-decomposition-detailed-concepts" class="nav-link" data-scroll-target="#spectral-decomposition-detailed-concepts"><span class="header-section-number">5.3</span> Spectral Decomposition: Detailed Concepts</a>
  <ul class="collapse">
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors"><span class="header-section-number">5.3.1</span> Eigenvalues and Eigenvectors</a></li>
  <li><a href="#eigenvalue-decomposition-spectral-decomposition" id="toc-eigenvalue-decomposition-spectral-decomposition" class="nav-link" data-scroll-target="#eigenvalue-decomposition-spectral-decomposition"><span class="header-section-number">5.3.2</span> Eigenvalue Decomposition (Spectral Decomposition)</a></li>
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation"><span class="header-section-number">5.3.3</span> Geometric Interpretation</a></li>
  <li><a href="#importance-of-diagonalization" id="toc-importance-of-diagonalization" class="nav-link" data-scroll-target="#importance-of-diagonalization"><span class="header-section-number">5.3.4</span> Importance of Diagonalization</a></li>
  <li><a href="#properties-of-symmetric-matrices" id="toc-properties-of-symmetric-matrices" class="nav-link" data-scroll-target="#properties-of-symmetric-matrices"><span class="header-section-number">5.3.5</span> Properties of Symmetric Matrices</a></li>
  </ul></li>
  <li><a href="#mathematical-requirements-for-spectral-decomposition" id="toc-mathematical-requirements-for-spectral-decomposition" class="nav-link" data-scroll-target="#mathematical-requirements-for-spectral-decomposition"><span class="header-section-number">5.4</span> Mathematical Requirements for Spectral Decomposition</a>
  <ul class="collapse">
  <li><a href="#determining-eigenvalues-and-eigenvectors" id="toc-determining-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#determining-eigenvalues-and-eigenvectors"><span class="header-section-number">5.4.1</span> Determining Eigenvalues and Eigenvectors</a></li>
  <li><a href="#characteristic-polynomial-of-2-times-2-matrices" id="toc-characteristic-polynomial-of-2-times-2-matrices" class="nav-link" data-scroll-target="#characteristic-polynomial-of-2-times-2-matrices"><span class="header-section-number">5.4.2</span> Characteristic Polynomial of <span class="math inline">\(2 \times 2\)</span> Matrices</a></li>
  <li><a href="#problems" id="toc-problems" class="nav-link" data-scroll-target="#problems"><span class="header-section-number">5.4.3</span> Problems</a></li>
  <li><a href="#python-code-to-find-eigen-values-and-eigen-vectors" id="toc-python-code-to-find-eigen-values-and-eigen-vectors" class="nav-link" data-scroll-target="#python-code-to-find-eigen-values-and-eigen-vectors"><span class="header-section-number">5.4.4</span> <code>Python</code> code to find eigen values and eigen vectors</a></li>
  <li><a href="#diagonalization-of-symmetric-matrices" id="toc-diagonalization-of-symmetric-matrices" class="nav-link" data-scroll-target="#diagonalization-of-symmetric-matrices"><span class="header-section-number">5.4.5</span> Diagonalization of Symmetric Matrices</a></li>
  <li><a href="#matrix-functions-and-spectral-theorem" id="toc-matrix-functions-and-spectral-theorem" class="nav-link" data-scroll-target="#matrix-functions-and-spectral-theorem"><span class="header-section-number">5.4.6</span> Matrix Functions and Spectral Theorem</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#qr-decomposition" id="toc-qr-decomposition" class="nav-link" data-scroll-target="#qr-decomposition"><span class="header-section-number">6</span> QR Decomposition</a>
  <ul class="collapse">
  <li><a href="#practical-uses-of-qr-decomposition" id="toc-practical-uses-of-qr-decomposition" class="nav-link" data-scroll-target="#practical-uses-of-qr-decomposition"><span class="header-section-number">6.0.1</span> Practical Uses of QR Decomposition</a></li>
  <li><a href="#python-method-for-dr-decomposition" id="toc-python-method-for-dr-decomposition" class="nav-link" data-scroll-target="#python-method-for-dr-decomposition"><span class="header-section-number">6.0.2</span> <code>Python</code> method for DR decomposition</a></li>
  <li><a href="#overdetermined-systems" id="toc-overdetermined-systems" class="nav-link" data-scroll-target="#overdetermined-systems"><span class="header-section-number">6.1</span> Overdetermined Systems</a>
  <ul class="collapse">
  <li><a href="#example-of-an-overdetermined-system" id="toc-example-of-an-overdetermined-system" class="nav-link" data-scroll-target="#example-of-an-overdetermined-system"><span class="header-section-number">6.1.1</span> Example of an Overdetermined System</a></li>
  <li><a href="#challenges-in-solving-overdetermined-systems" id="toc-challenges-in-solving-overdetermined-systems" class="nav-link" data-scroll-target="#challenges-in-solving-overdetermined-systems"><span class="header-section-number">6.1.2</span> Challenges in Solving Overdetermined Systems</a></li>
  <li><a href="#why-we-need-qr-decomposition" id="toc-why-we-need-qr-decomposition" class="nav-link" data-scroll-target="#why-we-need-qr-decomposition"><span class="header-section-number">6.1.3</span> Why We Need QR Decomposition</a></li>
  <li><a href="#solving-an-overdetermined-system-using-qr-decomposition" id="toc-solving-an-overdetermined-system-using-qr-decomposition" class="nav-link" data-scroll-target="#solving-an-overdetermined-system-using-qr-decomposition"><span class="header-section-number">6.1.4</span> Solving an Overdetermined System using QR Decomposition</a></li>
  <li><a href="#problems-1" id="toc-problems-1" class="nav-link" data-scroll-target="#problems-1"><span class="header-section-number">6.1.5</span> Problems</a></li>
  </ul></li>
  <li><a href="#module-review" id="toc-module-review" class="nav-link" data-scroll-target="#module-review"><span class="header-section-number">6.2</span> Module review</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra for Advanced Applications</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>Matrix decomposition plays a pivotal role in computational linear algebra, forming the backbone of numerous modern applications in fields such as data science, machine learning, computer vision, and signal processing. The core idea behind matrix decomposition is to break down complex matrices into simpler, structured components that allow for more efficient computation. Techniques such as LU, QR, Singular Value Decomposition (SVD), and Eigenvalue decompositions not only reduce computational complexity but also provide deep insights into the geometry and structure of data. These methods are essential in solving systems of linear equations, performing dimensionality reduction, and extracting meaningful features from data. For instance, LU decomposition is widely used to solve large linear systems, while QR decomposition plays a key role in solving least squares problems—a fundamental task in machine learning models.</p>
<p>In emerging fields like big data analytics and artificial intelligence, matrix decomposition techniques are indispensable for processing and analyzing high-dimensional datasets. SVD and Principal Component Analysis (PCA), for example, are extensively used for data compression and noise reduction, making machine learning algorithms more efficient by reducing the number of variables while retaining key information. Additionally, sparse matrix decompositions allow for the handling of enormous datasets where most entries are zero, optimizing memory usage and computation time. As data science and machine learning continue to evolve, mastering these matrix decomposition techniques provides not only a computational advantage but also deeper insights into the structure and relationships within data, enhancing the performance of algorithms in real-world applications.</p>
</section>
<section id="lu-decomposition" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="lu-decomposition"><span class="header-section-number">4.2</span> LU Decomposition</h2>
<p>LU decomposition is a powerful tool in linear algebra that elegantly unravels the complexity of solving systems of linear equations. At its core, LU decomposition expresses a matrix <span class="math inline">\(A\)</span> as the product of two distinct matrices: <span class="math inline">\(L\)</span> (a lower triangular matrix with ones on the diagonal) and <span class="math inline">\(U\)</span> (an upper triangular matrix). This decomposition transforms the problem of solving <span class="math inline">\(Ax = b\)</span> into a two-step process: first, solving <span class="math inline">\(Ly = b\)</span> for <span class="math inline">\(y\)</span>, followed by <span class="math inline">\(Ux = y\)</span> for <span class="math inline">\(x\)</span>. This systematic approach not only simplifies computations but also provides insightful perspectives on the relationships between the equations involved.</p>
<p>The magic of LU decomposition lies in its utilization of elementary transformations—operations that allow us to manipulate the rows of a matrix to achieve a row-reduced echelon form. These transformations include row swaps, scaling, and adding multiples of one row to another. By applying these operations, we can gradually transform the original matrix <span class="math inline">\(A\)</span> into the upper triangular matrix <span class="math inline">\(U\)</span>, while simultaneously capturing the essence of these transformations in the lower triangular matrix <span class="math inline">\(L\)</span>. This interplay of <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> not only enhances computational efficiency but also unveils the deeper structural relationships within the matrix.</p>
<p>Moreover, the beauty of matrix multiplication shines through in LU decomposition. The product <span class="math inline">\(A = LU\)</span> showcases how two simpler matrices can combine to reconstruct a more complex one, demonstrating the power of linear combinations in solving equations. As we delve into LU decomposition, we embark on a journey that highlights the synergy between algebraic manipulation and geometric interpretation, empowering us to tackle intricate problems with grace and precision. Given a square matrix <span class="math inline">\(A\)</span>, the LU decomposition expresses <span class="math inline">\(A\)</span> as a product of a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span>: <span class="math display">\[A = LU\]</span></p>
<p>Where: - <span class="math inline">\(L\)</span> is a lower triangular matrix with 1’s on the diagonal and other elements like <span class="math inline">\(l_{21}, l_{31}, l_{32}, \dots\)</span>, - <span class="math inline">\(U\)</span> is an upper triangular matrix with elements <span class="math inline">\(u_{11}, u_{12}, u_{13}, u_{22}, u_{23}, u_{33}, \dots\)</span>.</p>
<section id="step-by-step-procedure" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="step-by-step-procedure"><span class="header-section-number">4.2.1</span> Step-by-Step Procedure</h3>
<p>Let’s assume <span class="math inline">\(A\)</span> is a <span class="math inline">\(3 \times 3\)</span> matrix for simplicity: <span class="math display">\[A = \begin{pmatrix}a_{11} &amp; a_{12} &amp; a_{13} \\a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}\end{pmatrix}\]</span></p>
<p>We need to find matrices <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(L = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
l_{21} &amp; 1 &amp; 0 \\
l_{31} &amp; l_{32} &amp; 1
\end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(U = \begin{pmatrix}
u_{11} &amp; u_{12} &amp; u_{13} \\
0 &amp; u_{22} &amp; u_{23} \\
0 &amp; 0 &amp; u_{33}
\end{pmatrix}\)</span></p></li>
</ul>
<p>The product of <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> gives: <span class="math display">\[LU = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\l_{21} &amp; 1 &amp; 0 \\l_{31} &amp; l_{32} &amp; 1\end{pmatrix}\begin{pmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\0 &amp; u_{22} &amp; u_{23} \\0 &amp; 0 &amp; u_{33}\end{pmatrix}=\begin{pmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\l_{21}u_{11} &amp; l_{21}u_{12} + u_{22} &amp; l_{21}u_{13} + u_{23} \\l_{31}u_{11} &amp; l_{31}u_{12} + l_{32}u_{22} &amp; l_{31}u_{13} + l_{32}u_{23} + u_{33}\end{pmatrix}\]</span></p>
<p>By equating this with <span class="math inline">\(A\)</span>, we can set up a system of equations to solve for <span class="math inline">\(l_{ij}\)</span> and <span class="math inline">\(u_{ij}\)</span>.</p>
<blockquote class="blockquote">
<p>Step 1: Solve for <span class="math inline">\(u_{11}, u_{12}, u_{13}\)</span></p>
</blockquote>
<p>From the first row of <span class="math inline">\(A = LU\)</span>, we have: <span class="math display">\[u_{11} = a_{11}\]</span> <span class="math display">\[u_{12} = a_{12}\]</span> <span class="math display">\[u_{13} = a_{13}\]</span></p>
<blockquote class="blockquote">
<p>Step 2: Solve for <span class="math inline">\(l_{21}\)</span> and <span class="math inline">\(u_{22}, u_{23}\)</span></p>
</blockquote>
<p>From the second row, we get: <span class="math display">\[l_{21}u_{11} = a_{21} \quad \Rightarrow \quad l_{21} = \frac{a_{21}}{u_{11}}\]</span> <span class="math display">\[l_{21}u_{12} + u_{22} = a_{22} \quad \Rightarrow \quad u_{22} = a_{22} - l_{21}u_{12}\]</span> <span class="math display">\[l_{21}u_{13} + u_{23} = a_{23} \quad \Rightarrow \quad u_{23} = a_{23} - l_{21}u_{13}\]</span></p>
<blockquote class="blockquote">
<p>Step 3: Solve for <span class="math inline">\(l_{31}, l_{32}\)</span> and <span class="math inline">\(u_{33}\)</span></p>
</blockquote>
<p>From the third row, we get: <span class="math display">\[l_{31}u_{11} = a_{31} \quad \Rightarrow \quad l_{31} = \frac{a_{31}}{u_{11}}\]</span> <span class="math display">\[l_{31}u_{12} + l_{32}u_{22} = a_{32} \quad \Rightarrow \quad l_{32} = \frac{a_{32} - l_{31}u_{12}}{u_{22}}\]</span> <span class="math display">\[l_{31}u_{13} + l_{32}u_{23} + u_{33} = a_{33} \quad \Rightarrow \quad u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}\]</span></p>
<p><strong>Final Result</strong></p>
<p>Thus, the LU decomposition is given by the matrices: - <span class="math inline">\(L = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
l_{21} &amp; 1 &amp; 0 \\
l_{31} &amp; l_{32} &amp; 1
\end{pmatrix}\)</span> - <span class="math inline">\(U = \begin{pmatrix}
u_{11} &amp; u_{12} &amp; u_{13} \\
0 &amp; u_{22} &amp; u_{23} \\
0 &amp; 0 &amp; u_{33}
\end{pmatrix}\)</span></p>
<p>Where: - <span class="math inline">\(u_{11} = a_{11}, u_{12} = a_{12}, u_{13} = a_{13}\)</span> - <span class="math inline">\(l_{21} = \frac{a_{21}}{u_{11}}, u_{22} = a_{22} - l_{21}u_{12}, u_{23} = a_{23} - l_{21}u_{13}\)</span> - <span class="math inline">\(l_{31} = \frac{a_{31}}{u_{11}}, l_{32} = \frac{a_{32} - l_{31}u_{12}}{u_{22}}, u_{33} = a_{33} - l_{31}u_{13} - l_{32}u_{23}\)</span></p>
</section>
<section id="example" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="example"><span class="header-section-number">4.2.2</span> Example</h3>
<p>Let’s decompose the following matrix: <span class="math display">\[A = \begin{pmatrix} 4 &amp; 3 &amp; 2 \\6 &amp; 3 &amp; 1 \\2 &amp; 1 &amp; 3\end{pmatrix}\]</span></p>
<p>Following the steps outlined above:</p>
<ul>
<li><span class="math inline">\(u_{11} = 4, u_{12} = 3, u_{13} = 2\)</span></li>
<li><span class="math inline">\(l_{21} = \frac{6}{4} = 1.5\)</span>, so:
<ul>
<li><span class="math inline">\(u_{22} = 3 - 1.5 \times 3 = -1.5\)</span></li>
<li><span class="math inline">\(u_{23} = 1 - 1.5 \times 2 = -2\)</span></li>
</ul></li>
<li><span class="math inline">\(l_{31} = \frac{2}{4} = 0.5\)</span>, so:
<ul>
<li><span class="math inline">\(l_{32} = \frac{1 - 0.5 \times 3}{-1.5} = 0.67\)</span></li>
<li><span class="math inline">\(u_{33} = 3 - 0.5 \times 2 - 0.67 \times (-2) = 2.67\)</span></li>
</ul></li>
</ul>
<p>Thus, the decomposition is: - <span class="math inline">\(L = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1.5 &amp; 1 &amp; 0 \\
0.5 &amp; 0.67 &amp; 1
\end{pmatrix}\)</span> - <span class="math inline">\(U = \begin{pmatrix}
4 &amp; 3 &amp; 2 \\
0 &amp; -1.5 &amp; -2 \\
0 &amp; 0 &amp; 2.67
\end{pmatrix}\)</span></p>
</section>
<section id="python-implementation" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="python-implementation"><span class="header-section-number">4.2.3</span> Python Implementation</h3>
<div id="479d8cb7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> lu</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define matrix A</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform LU decomposition</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>P, L, U <span class="op">=</span> lu(A)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L = </span><span class="ch">\n</span><span class="st">"</span>, L)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"U = </span><span class="ch">\n</span><span class="st">"</span>, U)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>L = 
 [[1.         0.         0.        ]
 [0.66666667 1.         0.        ]
 [0.33333333 0.         1.        ]]
U = 
 [[6.         3.         1.        ]
 [0.         1.         1.33333333]
 [0.         0.         2.66666667]]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since there are many row transformations that reduce a given matrix into row echelon form. So the LU decomposition is not unique.</p>
</div>
</div>
</section>
<section id="lu-decomposition-practice-problems-with-solutions" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="lu-decomposition-practice-problems-with-solutions"><span class="header-section-number">4.2.4</span> LU Decomposition Practice Problems with Solutions</h3>
<p><strong>Problem 1:</strong> Decompose the matrix <span class="math display">\[ A = \begin{pmatrix} 4 &amp; 3 \\ 6 &amp; 3 \end{pmatrix} \]</span> into the product of a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span>.</p>
<p><strong>Solution:</strong></p>
<p>Let <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 \\ l_{21} &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} u_{11} &amp; u_{12} \\ 0 &amp; u_{22} \end{pmatrix}. \]</span></p>
<p>We have:</p>
<ol type="1">
<li>From the first row: <span class="math inline">\(u_{11} = 4\)</span> and <span class="math inline">\(u_{12} = 3\)</span>.</li>
<li>From the second row: <span class="math inline">\(6 = l_{21} \cdot 4\)</span> gives <span class="math inline">\(l_{21} = \frac{6}{4} = 1.5\)</span>.</li>
<li>Finally, <span class="math inline">\(3 = 1.5 \cdot 3 + u_{22}\)</span> gives <span class="math inline">\(u_{22} = 3 - 4.5 = -1.5\)</span>.</li>
</ol>
<p>Thus, we have: <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 \\ 1.5 &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} 4 &amp; 3 \\ 0 &amp; -1.5 \end{pmatrix}. \]</span></p>
<p><strong>Problem 2:</strong> Given the matrix <span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 5 &amp; 8 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}, \]</span> perform LU decomposition to find matrices <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>.</p>
<p><strong>Solution:</strong></p>
<p>Let <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ l_{21} &amp; 1 &amp; 0 \\ l_{31} &amp; l_{32} &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\ 0 &amp; u_{22} &amp; u_{23} \\ 0 &amp; 0 &amp; u_{33} \end{pmatrix}. \]</span></p>
<p>We have:</p>
<ol type="1">
<li>From Row 1: <span class="math inline">\(u_{11} = 1, u_{12} = 2, u_{13} = 3\)</span>.</li>
<li>From Row 2: <span class="math inline">\(2 = l_{21} \cdot 1\)</span> gives <span class="math inline">\(l_{21} = 2\)</span>.
<ul>
<li>For Row 2: <span class="math inline">\(5 = l_{21} \cdot 2 + u_{22}\)</span> gives <span class="math inline">\(5 = 4 + u_{22} \Rightarrow u_{22} = 1\)</span>.</li>
<li><span class="math inline">\(8 = l_{21} \cdot 3 + u_{23} \Rightarrow 8 = 6 + u_{23} \Rightarrow u_{23} = 2\)</span>.</li>
</ul></li>
<li>From Row 3: <span class="math inline">\(4 = l_{31} \cdot 1 \Rightarrow l_{31} = 4\)</span>.
<ul>
<li><span class="math inline">\(5 = l_{31} \cdot 2 + l_{32} \cdot 1 \Rightarrow 5 = 8 + l_{32} \Rightarrow l_{32} = -3\)</span>.</li>
<li>Finally, <span class="math inline">\(6 = l_{31} \cdot 3 + l_{32} \cdot 2 + u_{33} \Rightarrow 6 = 12 - 6 + u_{33} \Rightarrow u_{33} = 0\)</span>.</li>
</ul></li>
</ol>
<p>Thus, <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 2 &amp; 1 &amp; 0 \\ 4 &amp; -3 &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}. \]</span></p>
<p><strong>Problem 3:</strong> Perform LU decomposition of the matrix <span class="math display">\[ A = \begin{pmatrix} 2 &amp; 1 &amp; 1 \\ 4 &amp; -6 &amp; 0 \\ -2 &amp; 7 &amp; 2 \end{pmatrix}, \]</span> and verify the decomposition by checking <span class="math inline">\(A = LU\)</span>.</p>
<p><strong>Solution:</strong></p>
<p>Let <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ l_{21} &amp; 1 &amp; 0 \\ l_{31} &amp; l_{32} &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\ 0 &amp; u_{22} &amp; u_{23} \\ 0 &amp; 0 &amp; u_{33} \end{pmatrix}. \]</span></p>
<p>We have:</p>
<ol type="1">
<li>From Row 1: <span class="math inline">\(u_{11} = 2, u_{12} = 1, u_{13} = 1\)</span>.</li>
<li>From Row 2: <span class="math inline">\(4 = l_{21} \cdot 2 \Rightarrow l_{21} = 2\)</span>.
<ul>
<li><span class="math inline">\(-6 = 2 \cdot 1 + u_{22} \Rightarrow u_{22} = -8\)</span>.</li>
<li><span class="math inline">\(0 = 2 \cdot 1 + u_{23} \Rightarrow u_{23} = -2\)</span>.</li>
</ul></li>
<li>From Row 3: <span class="math inline">\(-2 = l_{31} \cdot 2 \Rightarrow l_{31} = -1\)</span>.
<ul>
<li><span class="math inline">\(7 = -1 \cdot 1 + l_{32} \cdot -8 \Rightarrow 7 = -1 - 8l_{32} \Rightarrow l_{32} = -1\)</span>.</li>
<li>Finally, <span class="math inline">\(2 = -1 \cdot 1 + -1 \cdot -2 + u_{33} \Rightarrow 2 = 1 + u_{33} \Rightarrow u_{33} = 1\)</span>.</li>
</ul></li>
</ol>
<p>Thus, <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 2 &amp; 1 &amp; 0 \\ -1 &amp; -1 &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} 2 &amp; 1 &amp; 1 \\ 0 &amp; -8 &amp; -2 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \]</span></p>
<p><strong>Problem 4:</strong> For the matrix <span class="math display">\[ A = \begin{pmatrix} 3 &amp; 1 &amp; 6 \\ 2 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 2 \end{pmatrix}, \]</span> find the LU decomposition and use it to solve the system <span class="math inline">\(Ax = b\)</span> where <span class="math inline">\(b = \begin{pmatrix} 9 \\ 5 \\ 4 \end{pmatrix}\)</span>.</p>
<p><strong>Solution:</strong></p>
<p>Let <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ l_{21} &amp; 1 &amp; 0 \\ l_{31} &amp; l_{32} &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\ 0 &amp; u_{22} &amp; u_{23} \\ 0 &amp; 0 &amp; u_{33} \end{pmatrix}. \]</span></p>
<p>We have:</p>
<ol type="1">
<li>From Row 1: <span class="math inline">\(u_{11} = 3, u_{12} = 1, u_{13} = 6\)</span>.</li>
<li>From Row 2: <span class="math inline">\(2 = l_{21} \cdot 3 \Rightarrow l_{21} = \frac{2}{3}\)</span>.
<ul>
<li><span class="math inline">\(1 = \frac{2}{3} \cdot 1 + u_{22} \Rightarrow 1 = \frac{2}{3} + u_{22} \Rightarrow u_{22} = \frac{1}{3}\)</span>.</li>
<li><span class="math inline">\(1 = \frac{2}{3} \cdot 6 + u_{23} \Rightarrow 1 = 4 + u_{23} \Rightarrow u_{23} = -3\)</span>.</li>
</ul></li>
<li>From Row 3: <span class="math inline">\(1 = l_{31} \cdot 3 \Rightarrow l_{31} = \frac{1}{3}\)</span>.
<ul>
<li><span class="math inline">\(2 = \frac{1}{3} \cdot 1 + l_{32} \cdot \frac{1}{3} \Rightarrow 2 = \frac{1}{3} + \frac{1}{3} l_{32} \Rightarrow l_{32} = 6\)</span>.</li>
<li>Finally, <span class="math inline">\(2 = \frac{1}{3} \cdot 6 + 6 \cdot -3 + u_{33} \Rightarrow 2 = 2 - 18 + u_{33} \Rightarrow u_{33} = 18\)</span>.</li>
</ul></li>
</ol>
<p>Thus, <span class="math display">\[ L = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ \frac{2}{3} &amp; 1 &amp; 0 \\ \frac{1}{3} &amp; 6 &amp; 1 \end{pmatrix}, \quad U = \begin{pmatrix} 3 &amp; 1 &amp; 6 \\ 0 &amp; \frac{1}{3} &amp; -3 \\ 0 &amp; 0 &amp; 18 \end{pmatrix}. \]</span></p>
<p>Now, to solve <span class="math inline">\(Ax = b\)</span>, we first solve <span class="math inline">\(Ly = b\)</span>: <span class="math display">\[ \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ \frac{2}{3} &amp; 1 &amp; 0 \\ \frac{1}{3} &amp; 6 &amp; 1 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix} = \begin{pmatrix} 9 \\ 5 \\ 4 \end{pmatrix} \]</span></p>
<p>Solving this gives: 1. <span class="math inline">\(y_1 = 9\)</span> 2. <span class="math inline">\(\frac{2}{3} \cdot 9 + y_2 = 5 \Rightarrow 6 + y_2 = 5 \Rightarrow y_2 = -1\)</span> 3. <span class="math inline">\(\frac{1}{3} \cdot 9 + 6 \cdot -1 + y_3 = 4 \Rightarrow 3 - 6 + y_3 = 4 \Rightarrow y_3 = 7\)</span></p>
<p>Next, solve <span class="math inline">\(Ux = y\)</span>: <span class="math display">\[ \begin{pmatrix} 3 &amp; 1 &amp; 6 \\ 0 &amp; \frac{1}{3} &amp; -3 \\ 0 &amp; 0 &amp; 18 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 9 \\ -1 \\ 7 \end{pmatrix} \]</span></p>
<ol type="1">
<li>From Row 3: <span class="math inline">\(18x_3 = 7 \Rightarrow x_3 = \frac{7}{18}\)</span></li>
<li>From Row 2: <span class="math inline">\(\frac{1}{3}x_2 - 3x_3 = -1 \Rightarrow \frac{1}{3}x_2 - \frac{21}{18} = -1 \Rightarrow \frac{1}{3}x_2 = -\frac{18}{18} + \frac{21}{18} = \frac{3}{18} \Rightarrow x_2 = \frac{1}{3}\)</span></li>
<li>From Row 1: <span class="math inline">\(3x_1 + x_2 + 6x_3 = 9 \Rightarrow 3x_1 + \frac{1}{3} + \frac{42}{18} = 9 \Rightarrow 3x_1 + \frac{1}{3} + \frac{7}{3} = 9 \Rightarrow 3x_1 = 9 - \frac{8}{3} = \frac{27 - 8}{3} = \frac{19}{3} \Rightarrow x_1 = \frac{19}{9}\)</span></li>
</ol>
<p>Thus, the solution to <span class="math inline">\(Ax = b\)</span> is <span class="math display">\[ x = \begin{pmatrix} \frac{19}{9} \\ \frac{1}{3} \\ \frac{7}{18} \end{pmatrix}. \]</span></p>
</section>
</section>
<section id="lu-decomposition-practice-problems" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="lu-decomposition-practice-problems"><span class="header-section-number">4.3</span> LU Decomposition Practice Problems</h2>
<p><strong>Problem 1:</strong> Decompose the matrix <span class="math display">\[ A = \begin{pmatrix} 4 &amp; 3 \\ 6 &amp; 3 \end{pmatrix} \]</span> into the product of a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span>.</p>
<p><strong>Problem 2:</strong> Given the matrix <span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 5 &amp; 8 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}, \]</span> perform LU decomposition to find matrices <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>.</p>
<p><strong>Problem 3:</strong> Perform LU decomposition of the matrix <span class="math display">\[ A = \begin{pmatrix} 2 &amp; 1 &amp; 1 \\ 4 &amp; -6 &amp; 0 \\ -2 &amp; 7 &amp; 2 \end{pmatrix}, \]</span> and verify the decomposition by checking <span class="math inline">\(A = LU\)</span>.</p>
<p><strong>Problem 4:</strong> For the matrix <span class="math display">\[ A = \begin{pmatrix} 3 &amp; 1 &amp; 6 \\ 2 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 2 \end{pmatrix}, \]</span> find the LU decomposition and use it to solve the system <span class="math inline">\(Ax = b\)</span> where <span class="math inline">\(b = \begin{pmatrix} 9 \\ 5 \\ 4 \end{pmatrix}\)</span>.</p>
<p><strong>Problem 5:</strong> Decompose the matrix <span class="math display">\[ A = \begin{pmatrix} 1 &amp; 3 &amp; 1 \\ 2 &amp; 6 &amp; 1 \\ 1 &amp; 1 &amp; 4 \end{pmatrix} \]</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>, and solve the system <span class="math inline">\(Ax = \begin{pmatrix} 5 \\ 9 \\ 6 \end{pmatrix}\)</span>.</p>
<p><strong>Problem 6:</strong> Given the matrix <span class="math display">\[ A = \begin{pmatrix} 7 &amp; 3 \\ 2 &amp; 5 \end{pmatrix}, \]</span> perform LU decomposition and use the result to solve <span class="math inline">\(Ax = b\)</span> for <span class="math inline">\(b = \begin{pmatrix} 10 \\ 7 \end{pmatrix}\)</span>.</p>
<p><strong>Problem 7:</strong> Find the LU decomposition of the matrix <span class="math display">\[ A = \begin{pmatrix} 2 &amp; -1 &amp; 1 \\ -2 &amp; 2 &amp; -1 \\ 4 &amp; -1 &amp; 3 \end{pmatrix}, \]</span> and use it to solve <span class="math inline">\(Ax = b\)</span> where <span class="math inline">\(b = \begin{pmatrix} 1 \\ -1 \\ 7 \end{pmatrix}\)</span>.</p>
<p><strong>Problem 8:</strong> Perform LU decomposition of the matrix <span class="math display">\[ A = \begin{pmatrix} 5 &amp; 2 &amp; 1 \\ 10 &amp; 4 &amp; 3 \\ 15 &amp; 8 &amp; 6 \end{pmatrix}. \]</span></p>
<p><strong>Problem 9:</strong> Use LU decomposition to find the solution to the system <span class="math inline">\(Ax = b\)</span> where <span class="math display">\[ A = \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 2 &amp; 3 &amp; 5 \\ 4 &amp; 6 &amp; 8 \end{pmatrix}, \quad b = \begin{pmatrix} 6 \\ 15 \\ 30 \end{pmatrix}. \]</span></p>
<p><strong>Problem 10:</strong> Decompose the matrix <span class="math display">\[ A = \begin{pmatrix} 6 &amp; -2 &amp; 2 \\ 12 &amp; -8 &amp; 6 \\ -6 &amp; 3 &amp; -3 \end{pmatrix} \]</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>, and verify that <span class="math inline">\(A = LU\)</span>.</p>
</section>
<section id="matrix-approach-to-create-lu-decomposition" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="matrix-approach-to-create-lu-decomposition"><span class="header-section-number">4.4</span> Matrix Approach to Create LU Decomposition</h2>
<p>LU decomposition can be performed using <em>elementary matrix operations</em>. In this method, we iteratively apply elementary matrices to reduce the given matrix <span class="math inline">\(A\)</span> into an upper triangular matrix <span class="math inline">\(U\)</span>, while keeping track of the transformations to form the lower triangular matrix <span class="math inline">\(L\)</span>.</p>
<p>The LU decomposition can be written as: <span class="math display">\[A = LU\]</span></p>
<p>where: - <span class="math inline">\(L\)</span> is the product of the inverses of the elementary matrices. - <span class="math inline">\(U\)</span> is the upper triangular matrix obtained after applying the row operations.</p>
<p><strong>Example: LU Decomposition of a 3x3 Matrix</strong></p>
<p>Given the matrix: <span class="math display">\[
A = \begin{pmatrix}
2 &amp; 1 &amp; 1 \\
4 &amp; -6 &amp; 0 \\
-2 &amp; 7 &amp; 2
\end{pmatrix}
\]</span></p>
<p>We will decompose <span class="math inline">\(A\)</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> using elementary row operations.</p>
<blockquote class="blockquote">
<p>Step 1: Applying Elementary Matrices</p>
</blockquote>
<p>We want to perform row operations to reduce <span class="math inline">\(A\)</span> into upper triangular form.</p>
<blockquote class="blockquote">
<p>Step 1.1: Eliminate the <span class="math inline">\(a_{21}\)</span> entry (below the pivot in column 1)</p>
</blockquote>
<p>To eliminate the <span class="math inline">\(4\)</span> in position <span class="math inline">\(a_{21}\)</span>, perform the operation: <span class="math display">\[R_2 \rightarrow R_2 - 2R_1\]</span></p>
<p>This corresponds to multiplying <span class="math inline">\(A\)</span> by the elementary matrix: <span class="math display">\[E_1 = \begin{pmatrix}1 &amp; 0 &amp; 0 \\-2 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1\end{pmatrix}\]</span></p>
<p>After this row operation, the matrix becomes: <span class="math display">\[
E_1 A = \begin{pmatrix}
2 &amp; 1 &amp; 1 \\
0 &amp; -8 &amp; -2 \\
-2 &amp; 7 &amp; 2
\end{pmatrix}
\]</span></p>
<blockquote class="blockquote">
<p>Step 1.2: Eliminate the <span class="math inline">\(a_{31}\)</span> entry</p>
</blockquote>
<p>To eliminate the <span class="math inline">\(-2\)</span> in position <span class="math inline">\(a_{31}\)</span>, perform the operation: <span class="math display">\[R_3 \rightarrow R_3 + R_1\]</span></p>
<p>This corresponds to multiplying the matrix by another elementary matrix: <span class="math display">\[
E_2 = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Now, the matrix becomes: <span class="math display">\[
E_2 E_1 A = \begin{pmatrix}
2 &amp; 1 &amp; 1 \\
0 &amp; -8 &amp; -2 \\
0 &amp; 8 &amp; 3
\end{pmatrix}
\]</span></p>
<blockquote class="blockquote">
<p>Step 1.3: Eliminate the <span class="math inline">\(a_{32}\)</span> entry</p>
</blockquote>
<p>Finally, to eliminate the <span class="math inline">\(8\)</span> in position <span class="math inline">\(a_{32}\)</span>, perform the operation: <span class="math display">\[
R_3 \rightarrow R_3 + R_2
\]</span></p>
<p>This corresponds to multiplying the matrix by the third elementary matrix:</p>
<p><span class="math display">\[
E_3 = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>After applying this operation, the matrix becomes: <span class="math display">\[
E_3 E_2 E_1 A = \begin{pmatrix}
2 &amp; 1 &amp; 1 \\
0 &amp; -8 &amp; -2 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<p>This is the upper triangular matrix <span class="math inline">\(U\)</span>.</p>
<blockquote class="blockquote">
<p>Step 2: Construct the Lower Triangular Matrix <span class="math inline">\(L\)</span></p>
</blockquote>
<p>The lower triangular matrix <span class="math inline">\(L\)</span> is formed by taking the inverses of the elementary matrices <span class="math inline">\(E_1, E_2, E_3\)</span>. Each inverse corresponds to the inverse of the row operations we applied.</p>
<ul>
<li><p><span class="math inline">\(E_1^{-1}\)</span> corresponds to adding back <span class="math inline">\(2R_1\)</span> to <span class="math inline">\(R_2\)</span>, so: <span class="math display">\[
E_1^{-1} = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p></li>
<li><p><span class="math inline">\(E_2^{-1}\)</span> corresponds to subtracting <span class="math inline">\(R_1\)</span> from <span class="math inline">\(R_3\)</span>, so: <span class="math display">\[
E_2^{-1} = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
-1 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p></li>
<li><p><span class="math inline">\(E_3^{-1}\)</span> corresponds to subtracting <span class="math inline">\(R_2\)</span> from <span class="math inline">\(R_3\)</span>, so: <span class="math display">\[
E_3^{-1} = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 1
\end{pmatrix}
\]</span></p></li>
</ul>
<p>Now, the lower triangular matrix <span class="math inline">\(L\)</span> is obtained by multiplying these inverses in reverse order: <span class="math display">\[
L = E_3^{-1} E_2^{-1} E_1^{-1} = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
-1 &amp; -1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Thus, the LU decomposition of <span class="math inline">\(A\)</span> is: <span class="math display">\[
L = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
-1 &amp; -1 &amp; 1
\end{pmatrix},
\quad U = \begin{pmatrix}
2 &amp; 1 &amp; 1 \\
0 &amp; -8 &amp; -2 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<p><strong>Verification</strong></p>
<p>Now, we check if <span class="math inline">\(A = LU\)</span>.</p>
<p>Multiply <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>:</p>
<div id="bb386db2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="op">-</span><span class="dv">8</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> L <span class="op">@</span> U</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>array([[ 2,  1,  1],
       [ 4, -6,  0],
       [-2,  7,  2]])</code></pre>
</div>
</div>
</section>
<section id="spectral-decomposition" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Spectral Decomposition</h1>
<section id="background" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="background"><span class="header-section-number">5.1</span> Background</h2>
<p>Imagine encountering a low-resolution image of a familiar scene. The human brain excels at recognizing familiar objects by relying on essential features, often extracting the most significant details while discarding the less important information. This cognitive process mirrors the power of eigenvalue decomposition, where eigenvectors represent the ``nectar’’ of a matrix, capturing its most important characteristics.</p>
<p>As an example, try to identify this image. If you can do it, then your brain know this place!</p>
<p><img src="gitsRC.jpg" class="img-fluid"></p>
<p>Before proceeding further just compare the size of its’ original clean image and the low-quality image shown in Figure</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="va">Original</span> <span class="va">image</span> <span class="va">size</span><span class="op">:</span> <span class="fl">985.69</span> <span class="va">KB</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="va">Reconstructed</span> <span class="va">image</span> <span class="va">size</span><span class="op">:</span> <span class="fl">1.12</span> <span class="va">KB</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The reconstructed image is just 0.2% of the original in size! This is the core principle of optimizing image storage of CCTV system. This resizing can be done and execute with optimal scaling with the help of Linear Algebra. This module mainly focuses on such engineering applications.</p>
</section>
<section id="introduction-1" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">5.2</span> Introduction</h2>
<p>Spectral decomposition, also known as eigenvalue decomposition, is a powerful tool in computational linear algebra that breaks down a matrix into its eigenvalues and eigenvectors. This technique allows matrices to be represented in terms of their fundamental components, making it easier to analyze and manipulate them. It is especially useful for symmetric matrices, which are common in various applications. Spectral decomposition facilitates solving systems of equations, optimizing functions, and performing transformations in a simplified, structured manner, as it allows operations to be performed on the eigenvalues, which often leads to more efficient computations.</p>
<p>The importance of spectral decomposition extends across a wide range of fields, including computer science, engineering, and data science. In machine learning, for instance, it forms the backbone of algorithms like Principal Component Analysis (PCA), which is used for dimensionality reduction. It also plays a vital role in numerical stability when dealing with large matrices and is central to many optimization problems, such as those found in machine learning and physics. Spectral decomposition not only provides a deeper understanding of the properties of matrices but also offers practical benefits in improving the efficiency and accuracy of numerical algorithms.</p>
</section>
<section id="spectral-decomposition-detailed-concepts" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="spectral-decomposition-detailed-concepts"><span class="header-section-number">5.3</span> Spectral Decomposition: Detailed Concepts</h2>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">5.3.1</span> Eigenvalues and Eigenvectors</h3>
<p>The core idea behind spectral decomposition is that it expresses a matrix in terms of its eigenvalues and eigenvectors. For a square matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, an eigenvalue <span class="math inline">\(\lambda \in \mathbb{R}\)</span> and an eigenvector <span class="math inline">\(v \in \mathbb{R}^{n}\)</span> satisfy the following equation:</p>
<p><span class="math display">\[
A v = \lambda v
\]</span></p>
<p>This implies that when the matrix <span class="math inline">\(A\)</span> acts on the vector <span class="math inline">\(v\)</span>, it only scales the vector by <span class="math inline">\(\lambda\)</span>, but does not change its direction. The eigenvector <span class="math inline">\(v\)</span> represents the direction of this scaling, while the eigenvalue <span class="math inline">\(\lambda\)</span> represents the magnitude of the scaling.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Eigen values
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>If <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(A\)</span>, then it satisfies the characteristic polynomial:</p>
<p><span class="math display">\[
p(\lambda) = \text{det}(A - \lambda I) = 0.
\]</span></p></li>
<li><p>The sum of the eigenvalues (counted with algebraic multiplicity) is equal to the trace of the matrix:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} \lambda_i = \text{trace}(A).
\]</span></p></li>
<li><p>The product of the eigenvalues (counted with algebraic multiplicity) is equal to the determinant of the matrix:</p>
<p><span class="math display">\[
\prod_{i=1}^{n} \lambda_i = \text{det}(A).
\]</span></p></li>
<li><p>If<span class="math inline">\(A\)</span> is symmetric, then:</p>
<ul>
<li>All eigenvalues <span class="math inline">\(\lambda\)</span> are real.</li>
<li>If <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\lambda_j\)</span> are distinct eigenvalues, then their corresponding eigenvectors <span class="math inline">\(\mathbf{v}_i\)</span> and <span class="math inline">\(\mathbf{v}_j\)</span> satisfy:</li>
</ul>
<p><span class="math display">\[
\mathbf{v}_i^T \mathbf{v}_j = 0.
\]</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is a scalar multiple of <span class="math inline">\(k\)</span>, then:</p>
<p><span class="math display">\[
\lambda_i \text{ of } kA = k \cdot \lambda_i \text{ of } A.
\]</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then:</p>
<p><span class="math display">\[
\lambda_i \text{ of } A^{-1} = \frac{1}{\lambda_i \text{ of } A}.
\]</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, then:</p>
<p><span class="math display">\[
B = P^{-1} A P \implies \lambda_i \text{ of } B = \lambda_i \text{ of } A.
\]</span></p></li>
<li><p>If <span class="math inline">\(\lambda\)</span> is an eigenvalue, it has:</p>
<ul>
<li><strong>Algebraic Multiplicity</strong>: The number of times <span class="math inline">\(\lambda\)</span> appears as a root of <span class="math inline">\(p(\lambda)\)</span>.</li>
<li><strong>Geometric Multiplicity</strong>: The dimension of the eigenspace <span class="math inline">\(E_{\lambda} = \{\mathbf{v} : A\mathbf{v} = \lambda \mathbf{v}\}\)</span>.</li>
</ul></li>
<li><p>If<span class="math inline">\(A\)</span> is symmetric and all eigenvalues <span class="math inline">\(\lambda\)</span> are positive, then <span class="math inline">\(A\)</span> is positive definite:</p>
<p><span class="math display">\[
\lambda_i &gt; 0 \implies A \text{ is positive definite.}
\]</span></p></li>
<li><p>A square matrix <span class="math inline">\(A\)</span> has an eigenvalue <span class="math inline">\(\lambda = 0\)</span> if and only if <span class="math inline">\(A\)</span> is singular:</p>
<p><span class="math display">\[
\text{det}(A) = 0 \iff \lambda = 0.
\]</span></p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Eigen Vectors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Eigen vectors are the non-trivial solutions of <span class="math inline">\(det(A-\lambda I)=0\)</span> for distinct <span class="math inline">\(\lambda\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Eigen vectors
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>If <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector of a square matrix <span class="math inline">\(A\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda\)</span>, then:</p>
<p><span class="math display">\[
A\mathbf{v} = \lambda \mathbf{v}.
\]</span></p></li>
<li><p>Eigenvectors corresponding to distinct eigenvalues are linearly independent. If <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are distinct eigenvalues of <span class="math inline">\(A\)</span>, with corresponding eigenvectors <span class="math inline">\(\mathbf{v}_1\)</span> and <span class="math inline">\(\mathbf{v}_2\)</span>, then:</p>
<p><span class="math display">\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 = \mathbf{0} \implies c_1 = 0 \text{ and } c_2 = 0.
\]</span></p></li>
<li><p>If <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector corresponding to the eigenvalue <span class="math inline">\(\lambda\)</span>, then any non-zero scalar multiple of <span class="math inline">\(\mathbf{v}\)</span> is also an eigenvector corresponding to <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[
\text{If } \mathbf{v} \text{ is an eigenvector, then } c\mathbf{v} \text{ is an eigenvector for any non-zero scalar } c.
\]</span></p></li>
<li><p>The eigenspace <span class="math inline">\(E_{\lambda}\)</span> associated with an eigenvalue <span class="math inline">\(\lambda\)</span> is defined as:</p>
<p><span class="math display">\[
E_{\lambda} = \{ \mathbf{v} : A\mathbf{v} = \lambda \mathbf{v} \} = \text{Null}(A - \lambda I).
\]</span></p></li>
<li><p>The dimension of the eigenspace <span class="math inline">\(E_{\lambda}\)</span> is equal to the geometric multiplicity of the eigenvalue <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>If <span class="math inline">\(A\)</span> is a symmetric matrix, then eigenvectors corresponding to distinct eigenvalues are orthogonal:</p>
<p><span class="math display">\[
\mathbf{v}_i^T \mathbf{v}_j = 0 \text{ for distinct eigenvalues } \lambda_i \text{ and } \lambda_j.
\]</span></p></li>
<li><p>For any square matrix <span class="math inline">\(A\)</span>, if <span class="math inline">\(\lambda = 0\)</span> is an eigenvalue, the eigenvectors corresponding to this eigenvalue form the null space of <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
E_{0} = \{ \mathbf{v} : A\mathbf{v} = \mathbf{0} \} = \text{Null}(A).
\]</span></p></li>
<li><p>If<span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(A\)</span> has no eigenvalue equal to zero, meaning all eigenvectors correspond to non-zero eigenvalues.</p></li>
<li><p>For<span class="math inline">\(A\)</span> as a scalar multiple of <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
A\mathbf{v} = k \lambda \mathbf{v} \text{ for eigenvalue } \lambda.
\]</span></p></li>
</ul>
</div>
</div>
</section>
<section id="eigenvalue-decomposition-spectral-decomposition" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="eigenvalue-decomposition-spectral-decomposition"><span class="header-section-number">5.3.2</span> Eigenvalue Decomposition (Spectral Decomposition)</h3>
<p>For matrices that are diagonalizable (including symmetric matrices), spectral decomposition expresses the matrix as a combination of its eigenvalues and eigenvectors. Specifically, for a matrix <span class="math inline">\(A\)</span>, spectral decomposition is represented as:</p>
<p><span class="math display">\[
A = V \Lambda V^{-1}
\]</span></p>
<p>where: - <span class="math inline">\(V\)</span> is the matrix of eigenvectors of <span class="math inline">\(A\)</span>, - <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of eigenvalues of <span class="math inline">\(A\)</span>, - <span class="math inline">\(V^{-1}\)</span> is the inverse of the matrix of eigenvectors (if <span class="math inline">\(V\)</span> is invertible).</p>
<p>For symmetric matrices <span class="math inline">\(A\)</span>, the decomposition becomes simpler:</p>
<p><span class="math display">\[
A = Q \Lambda Q^\top
\]</span></p>
<p>Here, <span class="math inline">\(Q\)</span> is an orthogonal matrix of eigenvectors (i.e., <span class="math inline">\(Q^\top Q = I\)</span>), and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of eigenvalues.</p>
</section>
<section id="geometric-interpretation" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="geometric-interpretation"><span class="header-section-number">5.3.3</span> Geometric Interpretation</h3>
<p>Eigenvalues and eigenvectors provide insights into the geometry of linear transformations represented by matrices. Eigenvectors represent directions that remain invariant under the transformation, while eigenvalues indicate how these directions are stretched or compressed.</p>
<p>For example, in the case of a transformation matrix that scales or rotates data points, eigenvalues show the magnitude of scaling along the principal axes (directions defined by eigenvectors).</p>
</section>
<section id="importance-of-diagonalization" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="importance-of-diagonalization"><span class="header-section-number">5.3.4</span> Importance of Diagonalization</h3>
<p>The key advantage of spectral decomposition is that it simplifies matrix operations. When a matrix is diagonalized as <span class="math inline">\(A = Q \Lambda Q^\top\)</span>, any function of the matrix <span class="math inline">\(A\)</span> (such as powers, exponentials, or inverses) can be easily computed by operating on the diagonal matrix <span class="math inline">\(\Lambda\)</span>. For example:</p>
<p><span class="math display">\[
A^k = Q \Lambda^k Q^\top
\]</span></p>
<p>Since <span class="math inline">\(\Lambda\)</span> is diagonal, raising <span class="math inline">\(\Lambda\)</span> to any power <span class="math inline">\(k\)</span> is straightforward, involving only raising each eigenvalue to the power <span class="math inline">\(k\)</span>.</p>
</section>
<section id="properties-of-symmetric-matrices" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5" class="anchored" data-anchor-id="properties-of-symmetric-matrices"><span class="header-section-number">5.3.5</span> Properties of Symmetric Matrices</h3>
<p>Spectral decomposition applies particularly well to symmetric matrices, which satisfy <span class="math inline">\(A = A^\top\)</span>. Symmetric matrices have the following key properties:</p>
<ul>
<li><p><strong>Real eigenvalues</strong>: The eigenvalues of a symmetric matrix are always real numbers.</p></li>
<li><p><strong>Orthogonal eigenvectors</strong>: The eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal to each other.</p></li>
<li><p><strong>Diagonalizability</strong>: Every symmetric matrix can be diagonalized by an orthogonal matrix.</p></li>
</ul>
<p>These properties make symmetric matrices highly desirable in computational applications.</p>
</section>
</section>
<section id="mathematical-requirements-for-spectral-decomposition" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="mathematical-requirements-for-spectral-decomposition"><span class="header-section-number">5.4</span> Mathematical Requirements for Spectral Decomposition</h2>
<section id="determining-eigenvalues-and-eigenvectors" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="determining-eigenvalues-and-eigenvectors"><span class="header-section-number">5.4.1</span> Determining Eigenvalues and Eigenvectors</h3>
<p>The eigenvalues of a matrix <span class="math inline">\(A\)</span> are the solutions to the characteristic equation:</p>
<p><span class="math display">\[
\text{det}(A - \lambda I) = 0
\]</span></p>
<p>Here, <span class="math inline">\(I\)</span> is the identity matrix, and <span class="math inline">\(\lambda\)</span> represents the eigenvalues. Solving this polynomial equation provides the eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_n\)</span>. Once the eigenvalues are determined, the eigenvectors can be computed by solving the equation <span class="math inline">\((A - \lambda I)v = 0\)</span> for each eigenvalue.</p>
</section>
<section id="characteristic-polynomial-of-2-times-2-matrices" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="characteristic-polynomial-of-2-times-2-matrices"><span class="header-section-number">5.4.2</span> Characteristic Polynomial of <span class="math inline">\(2 \times 2\)</span> Matrices</h3>
<p>For a <span class="math inline">\(2 \times 2\)</span> matrix: <span class="math display">\[
A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}
\]</span> the characteristic polynomial is derived from the determinant of <span class="math inline">\(A - \lambda I\)</span>, where <span class="math inline">\(I\)</span> is the identity matrix:</p>
<p><span class="math display">\[
\det(A - \lambda I) = 0
\]</span></p>
<p>This leads to: <span class="math display">\[
\det\begin{pmatrix} a - \lambda &amp; b \\ c &amp; d - \lambda \end{pmatrix} = (a - \lambda)(d - \lambda) - bc = 0
\]</span></p>
<p> The characteristic polynomial can be simplified to: <span class="math display">\[
\lambda^2 - (a + d)\lambda + (ad - bc) = 0
\]</span></p>
<p>This polynomial can be solved using the quadratic formula: <span class="math display">\[
\lambda = \frac{(a + d) \pm \sqrt{(a + d)^2 - 4(ad - bc)}}{2}
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Shortcut to write Characteristic polynomial of a <span class="math inline">\(2\times 2\)</span> matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(A=\begin{bmatrix} a &amp; b\\ c&amp; d\end{bmatrix}\)</span>, then the characteristic polynomial is <span class="math display">\[\lambda^2-(\text{Trace}(A))\lambda+det(A)=0\]</span></p>
<p>Eigen vectors can be found by using the formula: <span class="math display">\[\begin{equation*}
EV(\lambda=\lambda_1)=\begin{bmatrix}\lambda_1-d\\ c\end{bmatrix}
\end{equation*}\]</span></p>
</div>
</div>
</section>
<section id="problems" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="problems"><span class="header-section-number">5.4.3</span> Problems</h3>
<p><strong>Example 1:</strong> Find Eigenvalues and Eigenvectors of the matrix, <span class="math display">\[A = \begin{pmatrix} 3 &amp; 2 \\ 4 &amp; 1 \end{pmatrix}\]</span></p>
<p><em>Solution:</em></p>
<p>The characteristic equation is given by <span class="math display">\[det(A-\lambda I)=0\]</span></p>
<p><span class="math display">\[\begin{align*}
\lambda^2 - 4\lambda - 5 &amp;= 0\\
(\lambda-5)(\lambda+1)&amp;=0\\
\end{align*}\]</span></p>
<p>Hence the eigen values are <span class="math inline">\(\lambda_1=5,\quad \lambda_2=-1\)</span>.</p>
<p>So the eigen vectors are: <span class="math display">\[\begin{align*}
EV(\lambda=\lambda_1)&amp;=\begin{bmatrix}\lambda_1-d\\ c\end{bmatrix}\\
\therefore EV(\lambda=5)&amp;=\begin{bmatrix}4\\ 4\end{bmatrix}=\begin{bmatrix}1\\ 1\end{bmatrix}\\
\therefore EV(\lambda=-1)&amp;=\begin{bmatrix}-2\\ 4\end{bmatrix}=\begin{bmatrix}-1\\ 2\end{bmatrix}
\end{align*}\]</span></p>
<p><strong>Problem 2:</strong> Calculate the eigenvalues and eigenvectors of the matrix: <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span></p>
<p><em>Solution:</em></p>
<p>To find the eigenvalues and eigenvectors of a <span class="math inline">\(2 \times 2\)</span> matrix, we can use the shortcut formula for the characteristic polynomial:</p>
<p><span class="math display">\[
\lambda^2 - \text{trace}(A)\lambda + \det(A) = 0,
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is the matrix. Let’s apply this to the matrix</p>
<p><span class="math display">\[
A = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}.
\]</span></p>
<p>First, we calculate the trace and determinant of <span class="math inline">\(A\)</span>:</p>
<ul>
<li>The trace is the sum of the diagonal elements:</li>
</ul>
<p><span class="math display">\[
\text{trace}(A) = 2 + 2 = 4.
\]</span></p>
<ul>
<li>The determinant is calculated as follows:</li>
</ul>
<p><span class="math display">\[
\det(A) = (2)(2) - (1)(1) = 4 - 1 = 3.
\]</span></p>
<p>Next, substituting the trace and determinant into the characteristic polynomial gives:</p>
<p><span class="math display">\[
\lambda^2 - (4)\lambda + 3 = 0,
\]</span></p>
<p>which simplifies to:</p>
<p><span class="math display">\[
\lambda^2 - 4\lambda + 3 = 0.
\]</span></p>
<p>We can factor this quadratic equation:</p>
<p><span class="math display">\[
(\lambda - 1)(\lambda - 3) = 0.
\]</span></p>
<p>Setting each factor to zero gives the eigenvalues:</p>
<p><span class="math display">\[
\lambda_1 = 1, \quad \lambda_2 = 3.
\]</span></p>
<p>To find the eigenvectors corresponding to each eigenvalue, we use the shortcut for the eigenvector of a <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\)</span>:</p>
<p><span class="math display">\[
EV(\lambda) = \begin{pmatrix} \lambda - d \\ c \end{pmatrix}.
\]</span></p>
<p>For the eigenvalue <span class="math inline">\(\lambda_1 = 1\)</span>:</p>
<p><span class="math display">\[
EV(1) = \begin{pmatrix} 1 - 2 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ 1 \end{pmatrix}.
\]</span></p>
<p>This eigenvector can be simplified (up to a scalar multiple) to:</p>
<p><span class="math display">\[
\mathbf{v_1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}.
\]</span></p>
<p>For the eigenvalue <span class="math inline">\(\lambda_2 = 3\)</span>:</p>
<p><span class="math display">\[
EV(3) = \begin{pmatrix} 3 - 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\]</span></p>
<p>This eigenvector is already in a simple form:</p>
<p><span class="math display">\[
\mathbf{v_2} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\]</span></p>
<p><strong>Problem 3:</strong> For the matrix: <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix}\)</span>, find the eigenvalues and eigenvectors.</p>
<p><em>Solution:</em></p>
<p>We are given the matrix <span class="math display">\[
A = \begin{pmatrix} 1 &amp; 2 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix}
\]</span></p>
<p>and we aim to find its eigenvalues using the characteristic polynomial.</p>
<p>The shortcut formula for the characteristic polynomial of a <span class="math inline">\(3 \times 3\)</span> matrix is given by: <span class="math display">\[
\lambda^3 - \text{tr}(A)\lambda^2 + (\text{sum of principal minors of } A)\lambda - \det(A) = 0.
\]</span></p>
<p>The trace of a matrix is the sum of its diagonal elements. For matrix <span class="math inline">\(A\)</span>, we have: <span class="math display">\[
\text{tr}(A) = 1 + 1 + 1 = 3.
\]</span></p>
<p>The principal minors are the determinants of the <span class="math inline">\(2 \times 2\)</span> submatrices obtained by deleting one row and one column of <span class="math inline">\(A\)</span>.</p>
<p>The first minor is obtained by deleting the third row and third column: <span class="math display">\[
\det\begin{pmatrix} 1 &amp; 2 \\ 0 &amp; 1 \end{pmatrix} = (1)(1) - (2)(0) = 1.
\]</span></p>
<p>The second minor is obtained by deleting the second row and second column: <span class="math display">\[
\det\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} = (1)(1) - (1)(1) = 0.
\]</span></p>
<p>The third minor is obtained by deleting the first row and first column: <span class="math display">\[
\det\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = (1)(1) - (0)(0) = 1.
\]</span></p>
<p>Thus, the sum of the principal minors is: <span class="math display">\[
1 + 0 + 1 = 2.
\]</span></p>
<p>The determinant of <span class="math inline">\(A\)</span> can be calculated using cofactor expansion along the first row: <span class="math display">\[
\det(A) = 1 \cdot \det\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} - 2 \cdot \det\begin{pmatrix} 0 &amp; 0 \\ 1 &amp; 1 \end{pmatrix} + 1 \cdot \det\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}
\]</span> <span class="math display">\[
= 1 \cdot (1) - 2 \cdot (0) + 1 \cdot (-1) = 1 - 0 - 1 = 0.
\]</span></p>
<p>Now, we substitute these values into the characteristic polynomial formula: <span class="math display">\[
\lambda^3 - \text{tr}(A)\lambda^2 + (\text{sum of principal minors})\lambda - \det(A) = 0
\]</span> <span class="math display">\[
\lambda^3 - 3\lambda^2 + 2\lambda - 0 = 0.
\]</span></p>
<p>We now solve the equation: <span class="math display">\[
\lambda^3 - 3\lambda^2 + 2\lambda = 0.
\]</span> Factoring out <span class="math inline">\(\lambda\)</span> and apply factor theorem, we get:</p>
<p><span class="math display">\[\begin{align*}
   \lambda(\lambda^2 - 3\lambda + 2) &amp;= 0\\
   \lambda(\lambda-2)(\lambda-1)&amp;=0
\end{align*}\]</span></p>
<p>This gives one eigenvalue: <span class="math display">\[
\lambda_1 = 0;\quad \lambda_2=2;\quad \lambda_3=1
\]</span></p>
<p>Now we find the eigenvectors corresponding to each eigenvalue.</p>
<p>For <span class="math inline">\(\lambda_1 = 0\)</span>, solve <span class="math inline">\((A - 0I)\mathbf{v} = 0\)</span>: <span class="math display">\[
\begin{pmatrix} 1 &amp; 2 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\]</span> This gives the system: <span class="math display">\[
x + 2y + z = 0, \quad y = 0, \quad x + z = 0.
\]</span> Thus, <span class="math inline">\(x = -z\)</span>, and the eigenvector is: <span class="math display">\[
\mathbf{v}_1 = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}.
\]</span></p>
<p>For <span class="math inline">\(\lambda_2 = 2\)</span>, solve <span class="math inline">\((A - 2I)\mathbf{v} = 0\)</span>: <span class="math display">\[
\begin{pmatrix} -1 &amp; 2 &amp; 1 \\ 0 &amp; -1 &amp; 0 \\ 1 &amp; 0 &amp; -1 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\]</span> This gives the system: <span class="math display">\[
-x + 2y + z = 0, \quad -y = 0, \quad x - z = 0.
\]</span> Thus, <span class="math inline">\(x = z\)</span>, and the eigenvector is: <span class="math display">\[
\mathbf{v}_2 = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}.
\]</span></p>
<p>For <span class="math inline">\(\lambda_3 = 1\)</span>, solve <span class="math inline">\((A - I)\mathbf{v} = 0\)</span>: <span class="math display">\[
\begin{pmatrix} 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\]</span> This gives the system: <span class="math display">\[
2y + z = 0, \quad x = 0.
\]</span> Thus, <span class="math inline">\(z = -2y\)</span>, and the eigenvector is: <span class="math display">\[
\mathbf{v}_3 = \begin{pmatrix} 0 \\ 1 \\ -2 \end{pmatrix}.
\]</span></p>
<p><strong>Problem 3:</strong> If <span class="math inline">\(A=\begin{bmatrix}1&amp;2&amp;4\\ 0&amp;3&amp;4\\ 1&amp;-1&amp;-1 \end{bmatrix}\)</span>, compute the eigen values and eigen vectors and left eigen vectors of <span class="math inline">\(A\)</span>.</p>
<p><em>Solution:</em></p>
<p>We are given the matrix <span class="math display">\[
A = \begin{pmatrix} 1 &amp; 2 &amp; 4 \\ 0 &amp; 3 &amp; 4 \\ 1 &amp; -1 &amp; -1 \end{pmatrix}
\]</span></p>
<p>and need to find its eigenvalues and eigenvectors.</p>
<p>The characteristic polynomial for a <span class="math inline">\(3 \times 3\)</span> matrix is given by: <span class="math display">\[
\lambda^3 - \text{tr}(A)\lambda^2 + (\text{sum of principal minors})\lambda - \det(A) = 0.
\]</span></p>
<p>The trace is the sum of the diagonal elements: <span class="math display">\[
\text{tr}(A) = 1 + 3 + (-1) = 3.
\]</span></p>
<p>We now compute the <span class="math inline">\(2 \times 2\)</span> principal minors:</p>
<ul>
<li><p>Minor by removing the third row and third column: <span class="math display">\[
\det\begin{pmatrix} 1 &amp; 2 \\ 0 &amp; 3 \end{pmatrix} = (1)(3) - (2)(0) = 3.
\]</span></p></li>
<li><p>Minor by removing the second row and second column: <span class="math display">\[
\det\begin{pmatrix} 1 &amp; 4 \\ 1 &amp; -1 \end{pmatrix} = (1)(-1) - (4)(1) = -1 - 4 = -5.
\]</span></p></li>
<li><p>Minor by removing the first row and first column: <span class="math display">\[
\det\begin{pmatrix} 3 &amp; 4 \\ -1 &amp; -1 \end{pmatrix} = (3)(-1) - (4)(-1) = -3 + 4 = 1.
\]</span></p></li>
</ul>
<p>Thus, the sum of the principal minors is: <span class="math display">\[
3 + (-5) + 1 = -1.
\]</span></p>
<p>We calculate the determinant of <span class="math inline">\(A\)</span> by cofactor expansion along the first row: <span class="math display">\[
\det(A) = 1 \cdot \det\begin{pmatrix} 3 &amp; 4 \\ -1 &amp; -1 \end{pmatrix} - 2 \cdot \det\begin{pmatrix} 0 &amp; 4 \\ 1 &amp; -1 \end{pmatrix} + 4 \cdot \det\begin{pmatrix} 0 &amp; 3 \\ 1 &amp; -1 \end{pmatrix}.
\]</span> The <span class="math inline">\(2 \times 2\)</span> determinants are: <span class="math display">\[
\det\begin{pmatrix} 3 &amp; 4 \\ -1 &amp; -1 \end{pmatrix} = -3 + 4 = 1, \quad \det\begin{pmatrix} 0 &amp; 4 \\ 1 &amp; -1 \end{pmatrix} = -4,
\]</span> <span class="math display">\[
\det\begin{pmatrix} 0 &amp; 3 \\ 1 &amp; -1 \end{pmatrix} = -3.
\]</span></p>
<p>Thus: <span class="math display">\[
\det(A) = 1 \cdot 1 - 2 \cdot (-4) + 4 \cdot (-3) = 1 + 8 - 12 = -3.
\]</span></p>
<p>Substituting into the characteristic polynomial: <span class="math display">\[
\lambda^3 - \text{tr}(A)\lambda^2 + (\text{sum of principal minors})\lambda - \det(A) = 0,
\]</span></p>
<p>we get: <span class="math display">\[
\lambda^3 - 3\lambda^2 - \lambda + 3 = 0.
\]</span></p>
<p>We now solve the cubic equation: <span class="math display">\[\begin{align*}
   \lambda^3 - 3\lambda^2 - \lambda + 3&amp; = 0. \\
   (\lambda-1)(\lambda+1)(\lambda -3)&amp;=0
\end{align*}\]</span></p>
<p><span class="math display">\[\lambda_1 = 1, \quad \lambda_2 = -1, \quad \lambda_3 = 3.\]</span></p>
<p>To find the eigenvector corresponding to <span class="math inline">\(\lambda_1 = 3\)</span>, solve <span class="math inline">\((A - 3I)\mathbf{v} = 0\)</span>: <span class="math display">\[
A - 3I = \begin{pmatrix} 1 &amp; 2 &amp; 4 \\ 0 &amp; 3 &amp; 4 \\ 1 &amp; -1 &amp; -1 \end{pmatrix} - 3\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} -2 &amp; 2 &amp; 4 \\ 0 &amp; 0 &amp; 4 \\ 1 &amp; -1 &amp; -4 \end{pmatrix}.
\]</span></p>
<p>Solving this system gives the eigenvector: <span class="math display">\[
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}.
\]</span></p>
<p>For <span class="math inline">\(\lambda_2 = -1\)</span>, solve <span class="math inline">\((A +I)\mathbf{v} = 0\)</span>: <span class="math display">\[
A +I = \begin{pmatrix} 1 &amp; 2 &amp; 4 \\ 0 &amp; 3 &amp; 4 \\ 1 &amp; -1 &amp; -1 \end{pmatrix} +\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} 2 &amp; 2 &amp; 4 \\ 0 &amp; 4 &amp; 4 \\ 1 &amp; -1 &amp; 0 \end{pmatrix}.
\]</span></p>
<p>Note that the third row is depending on first and second rows. So by finding the cross product of first two rows,</p>
<p><span class="math display">\[
\mathbf{v}_2 = \begin{pmatrix} -1 \\ -1 \\ 1 \end{pmatrix}.
\]</span></p>
<p>For <span class="math inline">\(\lambda_3 = 1\)</span>, solve <span class="math inline">\((A -I)\mathbf{v} = 0\)</span>: <span class="math display">\[
A - I = \begin{pmatrix} 1 &amp; 2 &amp; 4 \\ 0 &amp; 3 &amp; 4 \\ 1 &amp; -1 &amp; -1 \end{pmatrix} -\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} 0 &amp; 2 &amp; 4 \\ 0 &amp; 2 &amp; 4 \\ 1 &amp; -1 &amp; -2\end{pmatrix}.
\]</span></p>
<p>Note that the second row is same as first row. So by finding the cross product of first and third rows, <span class="math display">\[
\mathbf{v}_3 = \begin{pmatrix} 0 \\ -2 \\ 1 \end{pmatrix}.
\]</span></p>
<p>Thus, the eigenvalues of the matrix are: <span class="math display">\[
\lambda_1 = 3, \quad \lambda_2 = -1, \quad \lambda_3 = 1
\]</span></p>
<p>with corresponding eigenvectors <span class="math inline">\(\mathbf{v}_1=\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}\)</span>, <span class="math inline">\(\mathbf{v}_2=\begin{pmatrix} -1 \\ -1 \\ 1 \end{pmatrix}\)</span>, and <span class="math inline">\(\mathbf{v}_3=\begin{pmatrix} 0 \\ -2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Left eigen vectors of the matrix <span class="math inline">\(A\)</span> are eigen vectors of <span class="math inline">\(A^T\)</span>.</p>
<p>Here <span class="math inline">\(A^T=\begin{bmatrix}
    1&amp;0&amp;1\\ 2&amp;3&amp;-1\\ 4&amp;4&amp;-1
\end{bmatrix}\)</span>.</p>
<p>Since <span class="math inline">\(A\)</span> and <span class="math inline">\(A^T\)</span> have same eigen values, it is enough to find corresponding eigen vectors. When <span class="math inline">\(\lambda=3\)</span>, the coefficient matrix of <span class="math inline">\((A-\lambda I)X=0\)</span> reduced into <span class="math inline">\(\begin{bmatrix}
    -2&amp;0&amp;1\\ 2&amp;0&amp;-1\\ 4&amp;4&amp;-4
\end{bmatrix}\)</span></p>
<p>Here the only independent rows are first and last. So the eigen vector can be found as the cross product of these two rows. <span class="math inline">\(\therefore v_1=\begin{bmatrix}
    1\\1\\2
\end{bmatrix}\)</span>.</p>
<p>When <span class="math inline">\(\lambda=-1\)</span>, the coefficient matrix of <span class="math inline">\((A-\lambda I)X=0\)</span> reduced into <span class="math inline">\(\begin{bmatrix}
    2&amp;0&amp;1\\ 2&amp;4&amp;-1\\ 4&amp;4&amp;0
\end{bmatrix}\)</span></p>
<p>Here the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows. <span class="math inline">\(\therefore v_2=\begin{bmatrix}
    -1\\1\\2
\end{bmatrix}\)</span>. When <span class="math inline">\(\lambda=1\)</span>, the coefficient matrix of <span class="math inline">\((A-\lambda I)X=0\)</span> reduced into <span class="math inline">\(\begin{bmatrix}
    0&amp;0&amp;1\\ 2&amp;2&amp;-1\\ 4&amp;4&amp;-2
\end{bmatrix}\)</span></p>
<p>Here the only independent rows are first and second. So the eigen vector can be found as the cross product of these two rows. <span class="math inline">\(\therefore v_2=\begin{bmatrix}
    -1\\1\\0
\end{bmatrix}\)</span>.</p>
</section>
<section id="python-code-to-find-eigen-values-and-eigen-vectors" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="python-code-to-find-eigen-values-and-eigen-vectors"><span class="header-section-number">5.4.4</span> <code>Python</code> code to find eigen values and eigen vectors</h3>
<ol type="1">
<li>Find eigen values and eigen vectors of <span class="math inline">\(A=\begin{bmatrix} 2&amp;1\\ 1&amp;2\end{bmatrix}\)</span>.</li>
</ol>
<div id="2e79fb41" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> null_space</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define matrix A</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Find eigenvalues</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>eigenvalues, _ <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define identity matrix I</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(A.shape[<span class="dv">0</span>])</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over eigenvalues to find corresponding eigenvectors</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, eigenvalue <span class="kw">in</span> <span class="bu">enumerate</span>(eigenvalues):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute A - lambda * I</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    A_lambda_I <span class="op">=</span> A <span class="op">-</span> eigenvalue <span class="op">*</span> I</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find the null space (which gives the eigenvector)</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    eig_vector <span class="op">=</span> null_space(A_lambda_I)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Eigenvalue </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>eigenvalue<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Eigenvector </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:</span><span class="ch">\n</span><span class="sc">{</span>eig_vector<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalue 1: 3.0
Eigenvector 1:
[[0.70710678]
 [0.70710678]]

Eigenvalue 2: 1.0
Eigenvector 2:
[[-0.70710678]
 [ 0.70710678]]
</code></pre>
</div>
</div>
<p>Same can be done using direct approach. Code for this task is given below.</p>
<div id="cec8f0e6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define matrix A</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Find eigenvalues and eigenvectors</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the results</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:"</span>, eigenvalues)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors:</span><span class="ch">\n</span><span class="st">"</span>, eigenvectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalues: [3. 1.]
Eigenvectors:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]</code></pre>
</div>
</div>
</section>
<section id="diagonalization-of-symmetric-matrices" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5" class="anchored" data-anchor-id="diagonalization-of-symmetric-matrices"><span class="header-section-number">5.4.5</span> Diagonalization of Symmetric Matrices</h3>
<p>For a symmetric matrix <span class="math inline">\(A\)</span>, the process of diagonalization can be summarized as follows:</p>
<ol type="1">
<li><p><strong>Compute eigenvalues</strong>: Solve the characteristic equation <span class="math inline">\(\text{det}(A - \lambda I) = 0\)</span> to find the eigenvalues.</p></li>
<li><p><strong>Find eigenvectors</strong>: For each eigenvalue <span class="math inline">\(\lambda_i\)</span>, solve <span class="math inline">\((A - \lambda_i I)v_i = 0\)</span> to find the corresponding eigenvector <span class="math inline">\(v_i\)</span>.</p></li>
<li><p><strong>Form the eigenvector matrix</strong>: Arrange the eigenvectors into a matrix <span class="math inline">\(Q\)</span>, with each eigenvector as a column.</p></li>
<li><p><strong>Form the diagonal matrix of eigenvalues</strong>: Construct <span class="math inline">\(\Lambda\)</span> by placing the eigenvalues along the diagonal of the matrix.</p></li>
</ol>
<p>Thus, the matrix can be expressed as <span class="math inline">\(A = Q \Lambda Q^\top\)</span>.</p>
<ol type="1">
<li>Diagonalize the matrix <span class="math inline">\(A=\begin{bmatrix} 2&amp;1\\ 1&amp;2\end{bmatrix}\)</span>.</li>
</ol>
<p><code>Python</code> code for this task is given below.</p>
<div id="a83d7c92" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define matrix A</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Find eigenvalues and eigenvectors</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Construct the diagonal matrix D (eigenvalues)</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(eigenvalues)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Construct the matrix P (eigenvectors)</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> eigenvectors</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Calculate the inverse of P</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>P_inv <span class="op">=</span> np.linalg.inv(P)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the diagonalization: A = P D P_inv</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>A_reconstructed <span class="op">=</span> P <span class="op">@</span> D <span class="op">@</span> P_inv</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix A:"</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Eigenvalues (Diagonal matrix D):"</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(D)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Eigenvectors (Matrix P):"</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(P)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Inverse of P:"</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(P_inv)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reconstructed matrix A (P D P^(-1)):"</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A_reconstructed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Matrix A:
[[2 1]
 [1 2]]

Eigenvalues (Diagonal matrix D):
[[3. 0.]
 [0. 1.]]

Eigenvectors (Matrix P):
[[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]

Inverse of P:
[[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]

Reconstructed matrix A (P D P^(-1)):
[[2. 1.]
 [1. 2.]]</code></pre>
</div>
</div>
</section>
<section id="matrix-functions-and-spectral-theorem" class="level3" data-number="5.4.6">
<h3 data-number="5.4.6" class="anchored" data-anchor-id="matrix-functions-and-spectral-theorem"><span class="header-section-number">5.4.6</span> Matrix Functions and Spectral Theorem</h3>
<p>Once a matrix is diagonalized, various matrix functions become easier to compute. For a function <span class="math inline">\(f(A)\)</span>, such as the exponential of a matrix or any power, the function can be applied to the diagonal matrix of eigenvalues:</p>
<p><span class="math display">\[
f(A) = Q f(\Lambda) Q^\top
\]</span></p>
<p>where <span class="math inline">\(f(\Lambda)\)</span> is the function applied element-wise to the eigenvalues in the diagonal matrix <span class="math inline">\(\Lambda\)</span>.</p>
</section>
</section>
</section>
<section id="qr-decomposition" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> QR Decomposition</h1>
<p>The <strong>QR decomposition</strong> of a matrix is a factorization technique that expresses a matrix <span class="math inline">\(A\)</span> as the product of an orthogonal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
A = QR
\]</span></p>
<p>where: - <span class="math inline">\(Q\)</span> is an orthogonal matrix (<span class="math inline">\(Q^T Q = I\)</span>), meaning its columns are orthonormal vectors. - <span class="math inline">\(R\)</span> is an upper triangular matrix.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Orthogonality</strong>: The columns of <span class="math inline">\(Q\)</span> are orthonormal, which implies that <span class="math inline">\(Q^T = Q^{-1}\)</span>.</li>
<li><strong>Uniqueness</strong>: The QR decomposition is unique if the columns of <span class="math inline">\(A\)</span> are linearly independent.</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Relation with Other Decompositions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>LU Decomposition</strong>:
<ul>
<li><strong>LU decomposition</strong> factors a matrix <span class="math inline">\(A\)</span> into a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span>: <span class="math display">\[
A = LU
\]</span></li>
<li>Unlike QR, LU decomposition does not require the columns of <span class="math inline">\(A\)</span> to be orthogonal.</li>
<li>QR decomposition is often used when <span class="math inline">\(A\)</span> is not square or when numerical stability is a concern, as it can be computed using Gram-Schmidt or Householder reflections.</li>
</ul></li>
<li><strong>Cholesky Decomposition (CR)</strong>:
<ul>
<li>The <strong>Cholesky decomposition</strong> is a specific case of LU decomposition applicable to symmetric positive-definite matrices: <span class="math display">\[
A = LL^T
\]</span></li>
<li>It is more efficient than LU decomposition for suitable matrices but does not provide orthogonality like QR.</li>
</ul></li>
<li><strong>Spectral Decomposition</strong>:
<ul>
<li>The <strong>spectral decomposition</strong> expresses a symmetric matrix <span class="math inline">\(A\)</span> in terms of its eigenvalues and eigenvectors: <span class="math display">\[
A = Q \Lambda Q^T
\]</span></li>
<li>While QR decomposition provides an orthogonal basis for any matrix, spectral decomposition is specifically used for symmetric matrices, providing insights into the matrix’s properties through its eigenvalues and eigenvectors.</li>
</ul></li>
<li><strong>Singular Value Decomposition (SVD)</strong>:
<ul>
<li>The <strong>SVD</strong> decomposes a matrix <span class="math inline">\(A\)</span> into three matrices: <span class="math display">\[
A = U \Sigma V^T
\]</span></li>
<li><span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matrices, and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix of singular values.</li>
<li>SVD is more general than QR and is particularly useful in applications involving rank-deficient matrices, dimensionality reduction, and noise reduction.</li>
</ul></li>
</ol>
</div>
</div>
<section id="practical-uses-of-qr-decomposition" class="level3" data-number="6.0.1">
<h3 data-number="6.0.1" class="anchored" data-anchor-id="practical-uses-of-qr-decomposition"><span class="header-section-number">6.0.1</span> Practical Uses of QR Decomposition</h3>
<ol type="1">
<li><strong>Solving Linear Systems</strong>:
<ul>
<li>QR decomposition is used to solve linear systems of equations, especially over-determined systems where there are more equations than unknowns. The least squares solution can be efficiently obtained via QR.</li>
</ul></li>
<li><strong>Eigenvalue Problems</strong>:
<ul>
<li>QR algorithms are often used in iterative methods for finding eigenvalues and eigenvectors of matrices, especially for large matrices.</li>
</ul></li>
<li><strong>Numerical Stability</strong>:
<ul>
<li>QR decomposition is numerically stable, making it suitable for computations involving floating-point arithmetic, particularly when dealing with ill-conditioned matrices.</li>
</ul></li>
<li><strong>Computer Graphics</strong>:
<ul>
<li>In computer graphics, QR decomposition can be used in perspective projection, where 3D points are projected onto a 2D plane, often needing orthogonal transformations.</li>
</ul></li>
<li><strong>Signal Processing</strong>:
<ul>
<li>In signal processing, QR decomposition is utilized in adaptive filtering algorithms and for solving problems related to estimation theory.</li>
</ul></li>
</ol>
</section>
<section id="python-method-for-dr-decomposition" class="level3" data-number="6.0.2">
<h3 data-number="6.0.2" class="anchored" data-anchor-id="python-method-for-dr-decomposition"><span class="header-section-number">6.0.2</span> <code>Python</code> method for DR decomposition</h3>
<ol type="1">
<li>Find the QR decomposition of the matrix, <span class="math inline">\(A=\begin{bmatrix}12&amp;-51&amp;4\\ 6&amp;167&amp;-68\\ -4&amp;24&amp;-41\end{bmatrix}\)</span>. Verify the decomposition using the reconstruction.</li>
</ol>
<div id="b66a3eee" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define  A</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">12</span>, <span class="op">-</span><span class="dv">51</span>, <span class="dv">4</span>],</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">6</span>, <span class="dv">167</span>, <span class="op">-</span><span class="dv">68</span>],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">4</span>, <span class="dv">24</span>, <span class="op">-</span><span class="dv">41</span>]])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform QR decomposition</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> np.linalg.qr(A)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the results</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix A:"</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(A)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Orthogonal Matrix Q:"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Q)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Upper Triangular Matrix R:"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(R)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the decomposition</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Verification (Q @ R):"</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.dot(Q, R))</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if Q is orthogonal (Q^T @ Q should be the identity matrix)</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Q^T @ Q (should be identity):"</span>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.dot(Q.T, Q))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Matrix A:
[[ 12 -51   4]
 [  6 167 -68]
 [ -4  24 -41]]

Orthogonal Matrix Q:
[[-0.85714286  0.39428571  0.33142857]
 [-0.42857143 -0.90285714 -0.03428571]
 [ 0.28571429 -0.17142857  0.94285714]]

Upper Triangular Matrix R:
[[ -14.  -21.   14.]
 [   0. -175.   70.]
 [   0.    0.  -35.]]

Verification (Q @ R):
[[ 12. -51.   4.]
 [  6. 167. -68.]
 [ -4.  24. -41.]]

Q^T @ Q (should be identity):
[[ 1.00000000e+00 -5.04131884e-17 -3.39864191e-17]
 [-5.04131884e-17  1.00000000e+00  2.30881074e-17]
 [-3.39864191e-17  2.30881074e-17  1.00000000e+00]]</code></pre>
</div>
</div>
</section>
<section id="overdetermined-systems" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="overdetermined-systems"><span class="header-section-number">6.1</span> Overdetermined Systems</h2>
<p>An <strong>overdetermined system</strong> of linear equations is a system in which there are more equations than unknowns. Mathematically, if we have a matrix <span class="math inline">\(A\)</span> of size <span class="math inline">\(m \times n\)</span> where <span class="math inline">\(m &gt; n\)</span>, the system can be represented as:</p>
<p><span class="math display">\[
A \mathbf{x} = \mathbf{b}
\]</span></p>
<p>where: - <span class="math inline">\(A\)</span> is the coefficient matrix, - <span class="math inline">\(\mathbf{x}\)</span> is the vector of unknowns (with size <span class="math inline">\(n\)</span>), - <span class="math inline">\(\mathbf{b}\)</span> is the vector of constants (with size <span class="math inline">\(m\)</span>).</p>
<section id="example-of-an-overdetermined-system" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="example-of-an-overdetermined-system"><span class="header-section-number">6.1.1</span> Example of an Overdetermined System</h3>
<p>Consider the following system of equations:</p>
<p><span class="math display">\[\begin{align*}
2x_1 + 3x_2 &amp;= 5 \\
4x_1 + 6x_2 &amp;= 10 \\
1x_1 + 2x_2 &amp;= 3 \\
\end{align*}\]</span></p>
<p>Here, we have three equations but only two unknowns (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>). This system is overdetermined.</p>
</section>
<section id="challenges-in-solving-overdetermined-systems" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="challenges-in-solving-overdetermined-systems"><span class="header-section-number">6.1.2</span> Challenges in Solving Overdetermined Systems</h3>
<ol type="1">
<li><p><strong>No Exact Solutions</strong>: In most cases, an overdetermined system does not have an exact solution because the equations may be inconsistent. For example, if one equation contradicts another, no value of <span class="math inline">\(\mathbf{x}\)</span> can satisfy all equations simultaneously.</p></li>
<li><p><strong>Finding Best Approximation</strong>: When the system is consistent, it may still be that no single solution satisfies all equations perfectly. Therefore, the goal is often to find an approximate solution that minimizes the error.</p></li>
</ol>
</section>
<section id="why-we-need-qr-decomposition" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="why-we-need-qr-decomposition"><span class="header-section-number">6.1.3</span> Why We Need QR Decomposition</h3>
<p>QR decomposition is particularly useful for solving overdetermined systems for the following reasons:</p>
<ol type="1">
<li><strong>Least Squares Solution</strong>:
<ul>
<li>The primary goal in solving an overdetermined system is to find the least squares solution, which minimizes the sum of the squared residuals (the differences between the left and right sides of the equations). QR decomposition allows us to efficiently compute this solution.</li>
</ul></li>
<li><strong>Orthogonality</strong>:
<ul>
<li>The QR decomposition expresses the matrix <span class="math inline">\(A\)</span> as the product of an orthogonal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>: <span class="math display">\[
A = QR
\]</span></li>
<li>The orthogonality of <span class="math inline">\(Q\)</span> ensures numerical stability and helps in reducing the problem to solving triangular systems.</li>
</ul></li>
<li><strong>Stability</strong>:
<ul>
<li>QR decomposition is more stable than other methods, such as Gaussian elimination, especially when dealing with ill-conditioned matrices. This is crucial in applications where precision is important.</li>
</ul></li>
<li><strong>Computational Efficiency</strong>:
<ul>
<li>The process of obtaining the QR decomposition can be performed using efficient algorithms, such as Gram-Schmidt orthogonalization or Householder reflections, which makes it suitable for large systems.</li>
</ul></li>
</ol>
</section>
<section id="solving-an-overdetermined-system-using-qr-decomposition" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="solving-an-overdetermined-system-using-qr-decomposition"><span class="header-section-number">6.1.4</span> Solving an Overdetermined System using QR Decomposition</h3>
<p>Given an overdetermined system represented as <span class="math inline">\(A \mathbf{x} = \mathbf{b}\)</span>, the steps to find the least squares solution using QR decomposition are as follows:</p>
<ol type="1">
<li><strong>Compute QR Decomposition</strong>:
<ul>
<li>Decompose the matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>.</li>
</ul></li>
<li><strong>Formulate the Normal Equations</strong>:
<ul>
<li>The least squares solution can be found from the equation: <span class="math display">\[
R \mathbf{x} = Q^T \mathbf{b}
\]</span></li>
</ul></li>
<li><strong>Solve the Triangular System</strong>:
<ul>
<li>Solve for <span class="math inline">\(\mathbf{x}\)</span> using back substitution, as <span class="math inline">\(R\)</span> is an upper triangular matrix.</li>
</ul></li>
</ol>
<p><code>Python</code> code for solving the above system of equations is given below.</p>
<div id="e280944e" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the coefficient matrix A and the constant vector b</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">3</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform QR decomposition</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> np.linalg.qr(A)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the least squares solution</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the equation R * x = Q^T * b</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>Q_b <span class="op">=</span> np.dot(Q.T, b)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(R, Q_b)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The least squares solution is:"</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The least squares solution is:
[1. 1.]</code></pre>
</div>
</div>
</section>
<section id="problems-1" class="level3" data-number="6.1.5">
<h3 data-number="6.1.5" class="anchored" data-anchor-id="problems-1"><span class="header-section-number">6.1.5</span> Problems</h3>
<blockquote class="blockquote">
<p>Problem 1: Simple Overdetermined System</p>
</blockquote>
<p><strong>Problem Statement:</strong><br>
Solve the overdetermined system given by the equations:</p>
<p><span class="math display">\[\begin{align*}
2x + 3y &amp;= 5 \\
4x + 6y &amp;= 10 \\
1x + 2y &amp;= 2
\end{align*}\]</span></p>
<div id="cf7a2c68" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem 1: Simple Overdetermined System</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the coefficient matrix A and the vector b</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>A1 <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">2</span>])</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>Q1, R1 <span class="op">=</span> np.linalg.qr(A1)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>x_qr1 <span class="op">=</span> np.linalg.solve(R1, Q1.T <span class="op">@</span> b1)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinary Least Squares solution using np.linalg.lstsq</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>x_ols1, residuals1, rank1, s1 <span class="op">=</span> np.linalg.lstsq(A1, b1, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 1 - QR Decomposition Solution:"</span>, x_qr1)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 1 - Ordinary Least Squares Solution:"</span>, x_ols1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Problem 1 - QR Decomposition Solution: [ 4. -1.]
Problem 1 - Ordinary Least Squares Solution: [ 4. -1.]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Problem 2: Overdetermined System with No Exact Solution</p>
</blockquote>
<p><strong>Problem Statement:</strong><br>
Solve the following system:</p>
<p><span class="math display">\[\begin{align*}
x + 2y &amp;= 3 \\
2x + 4y &amp;= 6 \\
3x + 1y &amp;= 5
\end{align*}\]</span></p>
<div id="55a8bab8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem 2: Overdetermined System with No Exact Solution</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the coefficient matrix A and the vector b</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>A2 <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">3</span>, <span class="dv">1</span>]])</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.array([<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>])</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>Q2, R2 <span class="op">=</span> np.linalg.qr(A2)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>x_qr2 <span class="op">=</span> np.linalg.solve(R2, Q2.T <span class="op">@</span> b2)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinary Least Squares solution using np.linalg.lstsq</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>x_ols2, residuals2, rank2, s2 <span class="op">=</span> np.linalg.lstsq(A2, b2, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 2 - QR Decomposition Solution:"</span>, x_qr2)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 2 - Ordinary Least Squares Solution:"</span>, x_ols2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Problem 2 - QR Decomposition Solution: [1.4 0.8]
Problem 2 - Ordinary Least Squares Solution: [1.4 0.8]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Problem 3: Overdetermined System with Random Data</p>
</blockquote>
<p><strong>Problem Statement:</strong><br>
Generate a random overdetermined system and solve it:</p>
<p><span class="math display">\[
Ax = b
\]</span></p>
<p>Where <span class="math inline">\(A\)</span> is a random <span class="math inline">\(6 \times 3\)</span> matrix and <span class="math inline">\(b\)</span> is generated accordingly.</p>
<div id="51585417" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem 3: Overdetermined System with Random Data</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a random overdetermined system</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>A3 <span class="op">=</span> np.random.rand(<span class="dv">6</span>, <span class="dv">3</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>x_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])  <span class="co"># True solution</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>b3 <span class="op">=</span> A3 <span class="op">@</span> x_true <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">6</span>)  <span class="co"># Adding some noise</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>Q3, R3 <span class="op">=</span> np.linalg.qr(A3)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>x_qr3 <span class="op">=</span> np.linalg.solve(R3, Q3.T <span class="op">@</span> b3)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinary Least Squares solution using np.linalg.lstsq</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>x_ols3, residuals3, rank3, s3 <span class="op">=</span> np.linalg.lstsq(A3, b3, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 3 - QR Decomposition Solution:"</span>, x_qr3)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 3 - Ordinary Least Squares Solution:"</span>, x_ols3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Problem 3 - QR Decomposition Solution: [0.94791379 2.10331498 2.98999875]
Problem 3 - Ordinary Least Squares Solution: [0.94791379 2.10331498 2.98999875]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Problem 4: Real-World Data Fitting</p>
</blockquote>
<p><strong>Problem Statement:</strong><br>
Fit a linear model to the following data points:</p>
<p><span class="math display">\[\begin{align*}
(1, 2), (2, 3), (3, 5), (4, 7), (5, 11)
\end{align*}\]</span></p>
<div id="bbbf9e2a" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem 4: Real-World Data Fitting</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data points</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>x_data <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>y_data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">11</span>])</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the design matrix A</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>A4 <span class="op">=</span> np.vstack([x_data, np.ones(<span class="bu">len</span>(x_data))]).T  <span class="co"># Add intercept</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>Q4, R4 <span class="op">=</span> np.linalg.qr(A4)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>x_qr4 <span class="op">=</span> np.linalg.solve(R4, Q4.T <span class="op">@</span> y_data)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinary Least Squares solution using np.linalg.lstsq</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>x_ols4, residuals4, rank4, s4 <span class="op">=</span> np.linalg.lstsq(A4, y_data, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 4 - QR Decomposition Solution:"</span>, x_qr4)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 4 - Ordinary Least Squares Solution:"</span>, x_ols4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Problem 4 - QR Decomposition Solution: [ 2.2 -1. ]
Problem 4 - Ordinary Least Squares Solution: [ 2.2 -1. ]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Problem 5: Polynomial Fit (Higher Degree)</p>
</blockquote>
<p><strong>Problem Statement:</strong><br>
Fit a quadratic polynomial to the following data points:</p>
<p><span class="math display">\[\begin{align*}
(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)
\end{align*}\]</span></p>
<div id="b53c7167" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem 5: Polynomial Fit (Higher Degree)</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data points for polynomial fitting</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x_data_poly <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>y_data_poly <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">16</span>, <span class="dv">25</span>])</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the design matrix for a quadratic polynomial</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>A5 <span class="op">=</span> np.vstack([x_data_poly<span class="op">**</span><span class="dv">2</span>, x_data_poly, np.ones(<span class="bu">len</span>(x_data_poly))]).T</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>Q5, R5 <span class="op">=</span> np.linalg.qr(A5)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>x_qr5 <span class="op">=</span> np.linalg.solve(R5, Q5.T <span class="op">@</span> y_data_poly)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinary Least Squares solution using np.linalg.lstsq</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>x_ols5, residuals5, rank5, s5 <span class="op">=</span> np.linalg.lstsq(A5, y_data_poly, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 5 - QR Decomposition Solution:"</span>, x_qr5)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Problem 5 - Ordinary Least Squares Solution:"</span>, x_ols5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Problem 5 - QR Decomposition Solution: [ 1.00000000e+00 -2.73316113e-15  3.80986098e-15]
Problem 5 - Ordinary Least Squares Solution: [ 1.00000000e+00 -6.77505297e-15  1.06049542e-14]</code></pre>
</div>
</div>
</section>
</section>
<section id="module-review" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="module-review"><span class="header-section-number">6.2</span> Module review</h2>
<ol type="1">
<li>Diagonalize the matrix <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 5 &amp; 1 \\ 3 &amp; 1 &amp; 1 \end{bmatrix}\)</span> and write the spectral decomposition of <span class="math inline">\(A\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- To diagonalize a matrix, find the eigenvalues and eigenvectors of the matrix. - Compute the eigenvectors of <span class="math inline">\(A\)</span> to form the matrix <span class="math inline">\(P\)</span>, and diagonalize <span class="math inline">\(A\)</span> using the formula <span class="math inline">\(A = P D P^{-1}\)</span> where <span class="math inline">\(D\)</span> is the diagonal matrix of eigenvalues. - Spectral decomposition expresses the matrix as a sum of eigenvalue-weighted outer products of eigenvectors.</p>
<hr>
<ol start="2" type="1">
<li>Find the eigenvalues and eigenvectors of the matrix <span class="math inline">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span> and write the spectral decomposition.</li>
</ol>
<p><strong>Hint:</strong><br>
- To find the eigenvalues, solve the characteristic equation <span class="math inline">\(det(A - \lambda I) = 0\)</span>. - Once you have the eigenvalues <span class="math inline">\(\lambda\)</span>, find the eigenvectors by solving <span class="math inline">\((A - \lambda I)v = 0\)</span> for each eigenvalue. - Express <span class="math inline">\(A\)</span> as a sum of eigenvalue-weighted outer products of its eigenvectors.</p>
<hr>
<ol start="3" type="1">
<li>Compute the QR decomposition of the matrix <span class="math inline">\(A = \begin{bmatrix} 4 &amp; 3 \\ 3 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- Use Gram-Schmidt process or the <code>numpy.linalg.qr()</code> function to compute the QR decomposition. - The matrix <span class="math inline">\(A\)</span> is factored into an orthogonal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span> such that <span class="math inline">\(A = QR\)</span>.</p>
<hr>
<ol start="4" type="1">
<li>Derive the LU decomposition of the matrix <span class="math inline">\(A = \begin{bmatrix} 4 &amp; 3 &amp; 1 \\ 6 &amp; 3 &amp; 2 \\ 3 &amp; 1 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- Use Gaussian elimination to decompose the matrix into lower and upper triangular matrices. - The matrix <span class="math inline">\(A\)</span> is written as <span class="math inline">\(A = LU\)</span>, where <span class="math inline">\(L\)</span> is a lower triangular matrix with 1s on the diagonal, and <span class="math inline">\(U\)</span> is an upper triangular matrix.</p>
<hr>
<ol start="5" type="1">
<li>Verify the rank-nullity theorem for the matrix <span class="math inline">\(A = \begin{bmatrix} 7 &amp; -3 &amp; 5 \\ 9 &amp; 11 &amp; 2 \\ 16 &amp; 8 &amp; 7 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- The rank-nullity theorem states that the sum of the rank and nullity (dimension of the null space) of a matrix equals the number of its columns. - Compute the rank of the matrix <span class="math inline">\(A\)</span> by finding the number of linearly independent rows or columns, and use this to determine the nullity.</p>
<hr>
<ol start="6" type="1">
<li>What are the five important matrix decompositions in linear algebra? Compare them in terms of application.</li>
</ol>
<p><strong>Hint:</strong><br>
- The five important matrix decompositions include: - <strong>LU Decomposition</strong>: Used for solving systems of linear equations, especially in computational methods. - <strong>QR Decomposition</strong>: Primarily used in solving least squares problems and orthogonalization. - <strong>Eigenvalue Decomposition</strong>: Useful in spectral analysis and principal component analysis (PCA). - <strong>Singular Value Decomposition (SVD)</strong>: Crucial for data compression, noise reduction, and dimensionality reduction. - <strong>Cholesky Decomposition</strong>: Applied in numerical optimization, especially for solving linear systems with symmetric, positive-definite matrices. - Compare their efficiency, use cases, and computational costs in different applications such as image processing, machine learning, etc.</p>
<hr>
<ol start="7" type="1">
<li>If <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 &amp; 4 \\ 0 &amp; 3 &amp; 4 \\ 1 &amp; -1 &amp; -1 \end{bmatrix}\)</span>, find the eigenvalues and eigenvectors of <span class="math inline">\(A\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- To find the eigenvalues of <span class="math inline">\(A\)</span>, solve the characteristic equation <span class="math inline">\(det(A - \lambda I) = 0\)</span>. - After finding the eigenvalues <span class="math inline">\(\lambda\)</span>, substitute them back into the equation <span class="math inline">\((A - \lambda I) v = 0\)</span> to find the corresponding eigenvectors. - Eigenvectors should be normalized if required.</p>
<hr>
<ol start="8" type="1">
<li>Perform the QR decomposition of the matrix <span class="math inline">\(A = \begin{bmatrix} 4 &amp; 1 \\ 3 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- Use Gram-Schmidt process or the <code>numpy.linalg.qr()</code> function to compute the QR decomposition. - The matrix <span class="math inline">\(A\)</span> is decomposed into <span class="math inline">\(A = QR\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix and <span class="math inline">\(R\)</span> is an upper triangular matrix.</p>
<hr>
<ol start="9" type="1">
<li>Compute the LU decomposition of the matrix <span class="math inline">\(A = \begin{bmatrix} 2 &amp; 3 &amp; 1 \\ 3 &amp; 4 &amp; 2 \\ 1 &amp; 2 &amp; 3 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- Use Gaussian elimination or the <code>scipy.linalg.lu()</code> function to find the LU decomposition. - The matrix <span class="math inline">\(A\)</span> is decomposed as <span class="math inline">\(A = LU\)</span>, where <span class="math inline">\(L\)</span> is a lower triangular matrix with 1s on the diagonal and <span class="math inline">\(U\)</span> is an upper triangular matrix.</p>
<hr>
<ol start="10" type="1">
<li>Find the Singular Value Decomposition (SVD) of the matrix <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span>.</li>
</ol>
<p><strong>Hint:</strong><br>
- Use the <code>numpy.linalg.svd()</code> function to compute the SVD of matrix <span class="math inline">\(A\)</span>. - The SVD decomposes <span class="math inline">\(A\)</span> into <span class="math inline">\(A = U \Sigma V^T\)</span>, where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matrices and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix containing the singular values.</p>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./module_3.html" class="pagination-link" aria-label="Python Libraries for Computational Linear Algebra">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python Libraries for Computational Linear Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./module_5.html" class="pagination-link" aria-label="Practical Uses Cases">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Practical Uses Cases</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>